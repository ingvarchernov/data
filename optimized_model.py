import os
import logging
import numpy as np
import tensorflow as tf
from tensorflow import keras
from gpu_config import configure_gpu

@tf.keras.utils.register_keras_serializable()
def mape(y_true, y_pred):
    """MAPE –º–µ—Ç—Ä–∏–∫–∞ –∑ –∫—Ä–∞—â–æ—é —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—é"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    return tf.reduce_mean(tf.abs((y_true - y_pred) / (tf.abs(y_true) + 1e-6))) * 100

@tf.keras.utils.register_keras_serializable()
def directional_accuracy(y_true, y_pred):
    """–¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞–ø—Ä—è–º–∫—É —Ä—É—Ö—É —Ü—ñ–Ω–∏ (–∑—Ä–æ—Å—Ç–∞–Ω–Ω—è/–ø–∞–¥—ñ–Ω–Ω—è)"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    # –†–∞—Ö—É—î–º–æ —Ä—ñ–∑–Ω–∏—Ü—é (–¥–µ–ª—å—Ç—É) –º—ñ–∂ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
    # –Ø–∫—â–æ –±–∞—Ç—á –º–∞—î —Ñ–æ—Ä–º—É (batch_size, 1), –±–µ—Ä–µ–º–µ –∑–Ω–∞–∫ —Å–∞–º–∏—Ö –∑–Ω–∞—á–µ–Ω—å
    # –¶–µ –ø—Ä–∞—Ü—é—î, —è–∫—â–æ y_true –≤–∂–µ —î –∑–º—ñ–Ω–æ—é —Ü—ñ–Ω–∏ (price_diff)
    true_direction = tf.sign(y_true + 1e-8)  # –¥–æ–¥–∞—î–º–æ epsilon —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ 0
    pred_direction = tf.sign(y_pred + 1e-8)
    
    # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –Ω–∞–ø—Ä—è–º–∫–∏
    correct = tf.cast(tf.equal(true_direction, pred_direction), tf.float32)
    return tf.reduce_mean(correct)
from keras import layers, callbacks, optimizers, mixed_precision
from typing import Tuple, Dict, List, Optional, Union
import matplotlib.pyplot as plt
from datetime import datetime
import gc


logger = logging.getLogger(__name__)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è mixed precision
mixed_precision.set_global_policy('mixed_float16')


class DatabaseHistoryCallback(callbacks.Callback):
    """Callback –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö"""
    
    def __init__(self, db_engine, symbol_id: int, interval_id: int, fold: int = 1):
        super().__init__()
        self.db_engine = db_engine
        self.symbol_id = symbol_id
        self.interval_id = interval_id
        self.fold = fold
    
    def on_epoch_end(self, epoch, logs=None):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ –µ–ø–æ—Ö–∏ –≤ –ë–î"""
        if logs is None:
            return
        
        try:
            from sqlalchemy import text
            with self.db_engine.connect() as conn:
                conn.execute(text("""
                    INSERT INTO training_history 
                    (symbol_id, interval_id, fold, epoch, loss, mae, val_loss, val_mae, mape, val_mape, directional_accuracy, real_mae, real_mape)
                    VALUES (:symbol_id, :interval_id, :fold, :epoch, :loss, :mae, :val_loss, :val_mae, :mape, :val_mape, :directional_accuracy, :real_mae, :real_mape)
                    ON CONFLICT (symbol_id, interval_id, fold, epoch)
                    DO UPDATE SET
                        loss = EXCLUDED.loss,
                        mae = EXCLUDED.mae,
                        val_loss = EXCLUDED.val_loss,
                        val_mae = EXCLUDED.val_mae,
                        mape = EXCLUDED.mape,
                        val_mape = EXCLUDED.val_mape,
                        directional_accuracy = EXCLUDED.directional_accuracy,
                        real_mae = EXCLUDED.real_mae,
                        real_mape = EXCLUDED.real_mape
                """), {
                    'symbol_id': self.symbol_id,
                    'interval_id': self.interval_id,
                    'fold': self.fold,
                    'epoch': epoch + 1,  # epoch –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ 0, –∞–ª–µ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –∑ 1
                    'loss': float(logs.get('loss', 0)),
                    'mae': float(logs.get('mae', 0)),
                    'val_loss': float(logs.get('val_loss', 0)),
                    'val_mae': float(logs.get('val_mae', 0)),
                    'mape': float(logs.get('mape', 0)),
                    'val_mape': float(logs.get('val_mape', 0)),
                    'directional_accuracy': float(logs.get('directional_accuracy', 0)),
                    'real_mae': float(logs.get('real_mae', logs.get('mae', 0))),
                    'real_mape': float(logs.get('real_mape', logs.get('mape', 0)))
                })
                conn.commit()
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –ë–î: {e}")


class DenormalizedMetricsCallback(callbacks.Callback):
    """Callback –¥–ª—è –≤–∏–≤–æ–¥—É —Å–ø—Ä–æ—â–µ–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫"""
    
    def __init__(self, scaler=None, feature_index: int = None, X_val: np.ndarray = None, y_val: np.ndarray = None):
        super().__init__()
        self.scaler = scaler
        self.feature_index = feature_index
        self.X_val = X_val
        self.y_val = y_val
    
    def on_epoch_end(self, epoch, logs=None):
        """–í–∏–≤—ñ–¥ –º–µ—Ç—Ä–∏–∫ –ø—ñ—Å–ª—è –µ–ø–æ—Ö–∏"""
        if logs is None:
            return
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            if self.X_val is not None and self.y_val is not None and self.scaler is not None:
                y_pred = self.model.predict(self.X_val, verbose=0).flatten()
                y_true = self.y_val  # –≤—ñ–¥—Å–æ—Ç–∫–æ–≤—ñ –∑–º—ñ–Ω–∏
                
                # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä—ñ–≤–Ω—ñ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∏—Ö –∑–º—ñ–Ω
                mae_norm = np.mean(np.abs(y_true - y_pred))
                mape_norm = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-6))) * 100
                
                # Directional accuracy: –∑–Ω–∞–∫ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∏—Ö –∑–º—ñ–Ω
                dir_acc = np.mean(np.sign(y_true) == np.sign(y_pred)) * 100
                
                logger.info(f"üí∞ –ú–µ—Ç—Ä–∏–∫–∏ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∏—Ö –∑–º—ñ–Ω (Epoch {epoch + 1}): "
                           f"MAE={mae_norm:.4f}, MAPE={mape_norm:.2f}%, –ù–∞–ø—Ä—è–º–æ–∫={dir_acc:.1f}%")
                
                # –†–µ–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏: –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –Ω–∞–∑–∞–¥ –¥–æ –∞–±—Å–æ–ª—é—Ç–Ω–∏—Ö —Ü—ñ–Ω –¥–ª—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—Ç–æ—á–Ω—ñ —Ü—ñ–Ω–∏ –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö (–æ—Å—Ç–∞–Ω–Ω—ñ–π timestep, –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ)
                # –û—Å–∫—ñ–ª—å–∫–∏ –¥–∞–Ω—ñ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ, –±–µ—Ä–µ–º–æ –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ close –∑ X_val
                current_prices = self.X_val[:, -1, self.feature_index].flatten()
                
                # –†–µ–∞–ª—å–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Ü—ñ–Ω–∏ = –ø–æ—Ç–æ—á–Ω—ñ + (–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤—ñ –∑–º—ñ–Ω–∏ * –ø–æ—Ç–æ—á–Ω—ñ / 100)
                y_true_pct = y_true  # –≤–∂–µ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤—ñ –∑–º—ñ–Ω–∏
                y_pred_pct = y_pred  # –≤–∂–µ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤—ñ –∑–º—ñ–Ω–∏
                
                true_next_prices = current_prices * (1 + y_true_pct / 100)
                pred_next_prices = current_prices * (1 + y_pred_pct / 100)
                
                mae_real = np.mean(np.abs(true_next_prices - pred_next_prices))
                mape_real = np.mean(np.abs((true_next_prices - pred_next_prices) / (np.abs(true_next_prices) + 1e-6))) * 100
                
                logger.info(f"üíµ –†–µ–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ —Ü—ñ–Ω (Epoch {epoch + 1}): "
                           f"MAE={mae_real:.2f}, MAPE={mape_real:.2f}%, –ù–∞–ø—Ä—è–º–æ–∫={dir_acc:.1f}%")
                
                # –î–æ–¥–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ logs –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –ë–î
                logs['mape'] = mape_real
                logs['val_mape'] = mape_real  # –¥–ª—è validation –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ –∂ –º–µ—Ç—Ä–∏–∫–∏
                logs['directional_accuracy'] = dir_acc
                logs['real_mae'] = mae_real
                logs['real_mape'] = mape_real
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –º–µ—Ç—Ä–∏–∫: {e}")


class TransformerBlock(layers.Layer):
    """Transformer –±–ª–æ–∫ –∑ Multi-Head Attention"""
    
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.ff_dim = ff_dim
        self.rate = rate
        
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential([
            layers.Dense(ff_dim, activation="relu"),
            layers.Dense(embed_dim),
        ])
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "num_heads": self.num_heads,
            "ff_dim": self.ff_dim,
            "rate": self.rate,
        })
        return config

class PositionalEncoding(layers.Layer):
    """–ü–æ–∑–∏—Ü—ñ–π–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–ª—è Transformer"""
    
    def __init__(self, maxlen, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.maxlen = maxlen
        self.embed_dim = embed_dim
        
    def build(self, input_shape):
        self.pos_encoding = self.add_weight(
            name="pos_encoding",
            shape=(self.maxlen, self.embed_dim),
            initializer="uniform",
            trainable=True,
        )
        super().build(input_shape)

    def call(self, inputs):
        length = tf.shape(inputs)[1]
        positions = tf.range(start=0, limit=length, delta=1)
        return inputs + tf.gather(self.pos_encoding, positions)

    def get_config(self):
        config = super().get_config()
        config.update({
            "maxlen": self.maxlen,
            "embed_dim": self.embed_dim,
        })
        return config

class OptimizedPricePredictionModel:
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ü—ñ–Ω"""
    
    def __init__(self, 
                 input_shape: Tuple[int, int],
                 model_type: str = "transformer_lstm",
                 use_mixed_precision: bool = True,
                 use_xla: bool = True,
                 scaler=None,
                 feature_index: int = None,
                 model_config: Dict = None):
        
        self.input_shape = input_shape
        # –ú–∞–ø—ñ–Ω–≥ —Ç–∏–ø—ñ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        model_type_mapping = {
            "transformer": "transformer_lstm",
            "advanced_lstm": "advanced_lstm", 
            "cnn_lstm": "cnn_lstm"
        }
        self.model_type = model_type_mapping.get(model_type, model_type)
        self.use_mixed_precision = use_mixed_precision
        self.use_xla = use_xla
        self.scaler = scaler
        self.feature_index = feature_index
        
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ (—è–∫—â–æ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–µ—Ñ–æ–ª—Ç–Ω—É)
        if model_config is None:
            from optimized_config import MODEL_CONFIG
            self.model_config = MODEL_CONFIG
        else:
            self.model_config = model_config
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è GPU
        self._configure_gpu()
        
        # –ö–∞—Å—Ç–æ–º–Ω—ñ –æ–±'—î–∫—Ç–∏ –¥–ª—è —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        self.custom_objects = {
            'TransformerBlock': TransformerBlock,
            'PositionalEncoding': PositionalEncoding,
            'mape': mape,
            'directional_accuracy': directional_accuracy,
        }
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        self.model = None
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
        self.create_model()
    
    def _configure_gpu(self):
        """–û–ø—Ç–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU –±–µ–∑ –Ω–∞–¥–º—ñ—Ä–Ω–æ–≥–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        configure_gpu(
            use_mixed_precision=self.use_mixed_precision,
            use_xla=self.use_xla,
            memory_growth=True,
        )
    
    def build_transformer_lstm_model(self) -> keras.Model:
        """Hybrid Transformer-LSTM –º–æ–¥–µ–ª—å"""
        inputs = layers.Input(shape=self.input_shape)

        # –ü–æ–∑–∏—Ü—ñ–π–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è
        x = PositionalEncoding(maxlen=self.input_shape[0], embed_dim=self.input_shape[1])(inputs)

        # Transformer –±–ª–æ–∫–∏
        x = TransformerBlock(embed_dim=self.input_shape[1], num_heads=8, ff_dim=256, rate=0.1)(x)
        x = TransformerBlock(embed_dim=self.input_shape[1], num_heads=8, ff_dim=256, rate=0.1)(x)

        # LSTM –±–ª–æ–∫–∏
        x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2, kernel_regularizer=tf.keras.regularizers.l2(0.001)))(x)
        x = layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)(x)

        # Dense –±–ª–æ–∫–∏ –∑ residual connections
        dense_input = x
        x = layers.Dense(128, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)

        x = layers.Dense(64, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)

        # Residual connection
        if dense_input.shape[-1] == 64:
            x = layers.Add()([x, dense_input])

        x = layers.Dense(32, activation='relu')(x)
        x = layers.Dropout(0.1)(x)

        # –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –∑ float32 –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        outputs = layers.Dense(1, activation='linear', dtype='float32', name='output')(x)

        model = keras.Model(inputs=inputs, outputs=outputs, name='transformer_lstm_model')

        return model
    
    def build_advanced_lstm_model(self) -> keras.Model:
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ LSTM –º–æ–¥–µ–ª—å –∑ attention - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ MODEL_CONFIG"""
        inputs = layers.Input(shape=self.input_shape)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É
        lstm_units_1 = self.model_config.get('lstm_units_1', 320)
        lstm_units_2 = self.model_config.get('lstm_units_2', 160)
        lstm_units_3 = self.model_config.get('lstm_units_3', 80)
        attention_heads = self.model_config.get('attention_heads', 10)
        attention_key_dim = self.model_config.get('attention_key_dim', 80)
        dense_units = self.model_config.get('dense_units', [640, 320, 160, 80])
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Ö–æ–¥—É
        x = layers.LayerNormalization()(inputs)
        
        # –ë–∞–≥–∞—Ç–æ—à–∞—Ä–æ–≤—ñ LSTM –∑ residual connections —Ç–∞ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
        lstm1 = layers.LSTM(lstm_units_1, return_sequences=True, dropout=0.4, recurrent_dropout=0.3,
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
        lstm1_norm = layers.LayerNormalization()(lstm1)
        
        lstm2 = layers.LSTM(lstm_units_2, return_sequences=True, dropout=0.4, recurrent_dropout=0.3,
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))(lstm1_norm)
        lstm2_norm = layers.LayerNormalization()(lstm2)
        
        lstm3 = layers.LSTM(lstm_units_3, return_sequences=True, dropout=0.4, recurrent_dropout=0.3,
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))(lstm2_norm)
        lstm3_norm = layers.LayerNormalization()(lstm3)
        
        # Attention –º–µ—Ö–∞–Ω—ñ–∑–º –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É
        attention = layers.MultiHeadAttention(
            num_heads=attention_heads, 
            key_dim=attention_key_dim
        )(lstm3_norm, lstm3_norm)
        attention = layers.Dropout(0.1)(attention)
        
        # Global pooling
        pooled = layers.GlobalAveragePooling1D()(attention)
        
        # Dense –±–ª–æ–∫–∏ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É —Ç–∞ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
        x = pooled
        for i, units in enumerate(dense_units):
            x = layers.Dense(units, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
            x = layers.BatchNormalization()(x)
            dropout_rate = 0.5 if i == 0 else 0.4 if i == 1 else 0.3
            x = layers.Dropout(dropout_rate)(x)
        
        outputs = layers.Dense(1, activation='linear', dtype='float32', name='output')(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name='advanced_lstm_model')
        
        return model
    
    def build_cnn_lstm_model(self) -> keras.Model:
        """CNN-LSTM –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤"""
        inputs = layers.Input(shape=self.input_shape)
        
        # 1D CNN –±–ª–æ–∫–∏
        x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.1)(x)
        
        x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.1)(x)
        
        x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)
        x = layers.BatchNormalization()(x)
        
        # LSTM –±–ª–æ–∫–∏
        x = layers.LSTM(32, return_sequences=True, dropout=0.2)(x)
        x = layers.LSTM(16, return_sequences=False, dropout=0.2)(x)
        
        # Dense –±–ª–æ–∫–∏
        x = layers.Dense(50, activation='relu')(x)
        x = layers.Dropout(0.2)(x)
        
        outputs = layers.Dense(1, activation='linear', dtype='float32', name='output')(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name='cnn_lstm_model')
        
        return model
    
    def build_dense_model(self) -> keras.Model:
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ Dense –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏"""
        inputs = layers.Input(shape=self.input_shape, name='input')

        # –û—Ç—Ä–∏–º—É—î–º–æ dense_units –∑ –∫–æ–Ω—Ñ—ñ–≥—É –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ defaults
        dense_units = self.model_config.get('dense_units', [256, 128, 64])

        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Ö–æ–¥—É
        x = layers.LayerNormalization()(inputs)

        # –î–µ–∫—ñ–ª—å–∫–∞ Dense —à–∞—Ä—ñ–≤ –∑ dropout —Ç–∞ batch normalization
        for i, units in enumerate(dense_units):
            x = layers.Dense(units, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
            x = layers.BatchNormalization()(x)
            x = layers.Dropout(0.3)(x)

        # –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä
        outputs = layers.Dense(1, activation='sigmoid', dtype='float32', name='output')(x)

        model = keras.Model(inputs=inputs, outputs=outputs, name='enhanced_dense_model')

        return model
    
    def create_model(self) -> keras.Model:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑–≥—ñ–¥–Ω–æ –∑ —Ç–∏–ø–æ–º"""
        if self.model_type == "transformer_lstm":
            model = self.build_transformer_lstm_model()
        elif self.model_type == "advanced_lstm":
            model = self.build_advanced_lstm_model()
        elif self.model_type == "cnn_lstm":
            model = self.build_cnn_lstm_model()
        elif self.model_type == "dense":
            model = self.build_dense_model()
        else:
            raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –º–æ–¥–µ–ª—ñ: {self.model_type}")
        
        logger.info(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–∞ –º–æ–¥–µ–ª—å —Ç–∏–ø—É: {self.model_type}")
        logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –≤ –º–æ–¥–µ–ª—ñ: {model.count_params():,}")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–æ–¥–µ–ª—å
        self.model = model
        return model
    
    def compile_model(self, model: keras.Model, learning_rate: float = 0.001) -> keras.Model:
        """–ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –º–æ–¥–µ–ª—ñ –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –∑ gradient clipping
        optimizer = optimizers.Adam(
            learning_rate=learning_rate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7,
            clipnorm=1.0
        )
        
        # –î–ª—è mixed precision
        if self.use_mixed_precision:
            optimizer = mixed_precision.LossScaleOptimizer(optimizer)
        
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',  # Binary classification
            metrics=[
                'accuracy',  # accuracy for binary classification
                directional_accuracy  # —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞–ø—Ä—è–º–∫—É —Ä—É—Ö—É
            ],
            jit_compile=self.use_xla
        )
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —É custom callbacks
        self.additional_metrics = {
            'mape': mape,
            'directional_accuracy': directional_accuracy
        }
        
        return model
    
    def _create_lr_schedule(self, epochs: int = 200, warmup_epochs: int = 10, base_lr: float = 0.001):
        """–°—Ç–≤–æ—Ä—é—î —Ä–æ–∑–∫–ª–∞–¥ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ warmup —Ç–∞ cosine annealing"""
        def lr_schedule(epoch):
            if epoch < warmup_epochs:
                # Linear warmup
                return base_lr * (epoch + 1) / warmup_epochs
            else:
                # Cosine annealing –∑ restarts
                progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
                cosine_decay = 0.5 * (1 + np.cos(np.pi * progress))
                
                # –î–æ–¥–∞—î–º–æ periodic restarts –∫–æ–∂–Ω—ñ 50 –µ–ø–æ—Ö –ø—ñ—Å–ª—è warmup
                if epoch > warmup_epochs + 50:
                    cycle = (epoch - warmup_epochs) // 50
                    cycle_progress = ((epoch - warmup_epochs) % 50) / 50
                    cosine_decay = cosine_decay * (0.5 + 0.5 * np.cos(2 * np.pi * cycle_progress))
                
                return base_lr * cosine_decay
        
        return lr_schedule
    
    def get_callbacks(self, 
                     model_save_path: str,
                     patience: int = 50,
                     reduce_lr_patience: int = 20) -> List[callbacks.Callback]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏—Ö callback'—ñ–≤"""
        
        callback_list = [
            # –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π Early stopping –∑ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥–æ–º –æ—Å–Ω–æ–≤–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=patience,
                restore_best_weights=True,
                verbose=1,
                mode='min'
            ),
            
            # ReduceLROnPlateau
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=reduce_lr_patience,
                min_lr=1e-7,
                verbose=1,
                mode='min'
            ),
            
            # Model checkpoint
            callbacks.ModelCheckpoint(
                filepath=model_save_path,
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=False,
                verbose=1,
                mode='min'
            ),
            
            # TensorBoard
            callbacks.TensorBoard(
                log_dir=f'./logs/{datetime.now().strftime("%Y%m%d-%H%M%S")}',
                histogram_freq=10,
                write_graph=True,
                update_freq='epoch'
            ),
            
            # –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π Learning rate scheduler –∑ warmup —Ç–∞ cosine annealing
            callbacks.LearningRateScheduler(
                self._create_lr_schedule(epochs=200, warmup_epochs=10),
                verbose=0
            ),
            
            # Memory cleanup
            callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: gc.collect()
            )
        ]
        
        return callback_list
    
    def create_data_generators(self, 
                             X_train: np.ndarray, 
                             y_train: np.ndarray,
                             X_val: np.ndarray, 
                             y_val: np.ndarray,
                             batch_size: int = 64) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏—Ö data generators"""
        
        # –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
        train_dataset = train_dataset.shuffle(buffer_size=1000)
        train_dataset = train_dataset.batch(batch_size)
        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))
        val_dataset = val_dataset.batch(batch_size)
        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)
        
        return train_dataset, val_dataset
    
    def train_model(self,
                   X_train: np.ndarray,
                   y_train: np.ndarray,
                   X_val: np.ndarray,
                   y_val: np.ndarray,
                   model_save_path: str,
                   epochs: int = 200,
                   batch_size: int = 64,
                   learning_rate: float = 0.001,
                   db_callback: callbacks.Callback = None,
                   additional_callbacks: List[callbacks.Callback] = None) -> Tuple[keras.Model, keras.callbacks.History]:
        """–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –∫–æ–º–ø—ñ–ª—è—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        model = self.create_model()
        model = self.compile_model(model, learning_rate)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è data generators
        train_dataset, val_dataset = self.create_data_generators(
            X_train, y_train, X_val, y_val, batch_size
        )
        
        # Callback'–∏
        callback_list = self.get_callbacks(model_save_path)
        
        # –î–æ–¥–∞—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ callback'–∏ –ü–ï–†–ï–î DB callback (—â–æ–± –º–µ—Ç—Ä–∏–∫–∏ –±—É–ª–∏ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω—ñ)
        if additional_callbacks is not None:
            callback_list.extend(additional_callbacks)
        
        # –î–æ–¥–∞—î–º–æ DB callback —è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω–∏–π (–ø—ñ—Å–ª—è additional, —â–æ–± –º–µ—Ç—Ä–∏–∫–∏ –±—É–ª–∏ –≤ logs)
        if db_callback is not None:
            callback_list.append(db_callback)
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        logger.info(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ {self.model_type}")
        
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=epochs,
            callbacks=callback_list,
            verbose=2  # –û–¥–Ω–∞ –ª—ñ–Ω—ñ—è –Ω–∞ –µ–ø–æ—Ö—É
        )
        
        logger.info("‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
        return model, history
    
    def save_model_with_metadata(self, 
                                model: keras.Model, 
                                save_path: str, 
                                metadata: Dict = None):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏"""
        try:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–æ–¥–µ–ª—å —É HDF5 —Ñ–æ—Ä–º–∞—Ç—ñ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
            model.save(save_path, save_format='h5')
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
            if metadata:
                metadata_path = save_path.replace('.h5', '_metadata.json')
                import json
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {save_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            raise
    
    def load_model(self, model_path: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ —Ñ–∞–π–ª—É"""
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å
            self.model = keras.models.load_model(
                model_path, 
                custom_objects=self.custom_objects
            )
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: {model_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            raise
    
    def plot_training_history(self, history: keras.callbacks.History, save_path: str = None):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ—Ü–µ—Å—É —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(history.history['loss'], label='Train Loss')
        axes[0, 0].plot(history.history['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # MAE
        axes[0, 1].plot(history.history['mae'], label='Train MAE')
        axes[0, 1].plot(history.history['val_mae'], label='Val MAE')
        axes[0, 1].set_title('Mean Absolute Error')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('MAE')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # MAPE
        if 'mape' in history.history:
            axes[1, 0].plot(history.history['mape'], label='Train MAPE')
            axes[1, 0].plot(history.history['val_mape'], label='Val MAPE')
            axes[1, 0].set_title('Mean Absolute Percentage Error')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('MAPE (%)')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
        
        # Learning Rate
        if 'lr' in history.history:
            axes[1, 1].plot(history.history['lr'])
            axes[1, 1].set_title('Learning Rate')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].set_yscale('log')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"‚úÖ –ì—Ä–∞—Ñ—ñ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path}")
        
        plt.close()
    
    def predict(self, X: np.ndarray, return_confidence: bool = False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        
        Args:
            X: –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            return_confidence: –ß–∏ –ø–æ–≤–µ—Ä—Ç–∞—Ç–∏ —Ç–∞–∫–æ–∂ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É
            
        Returns:
            –ü—Ä–æ–≥–Ω–æ–∑ –∞–±–æ (–ø—Ä–æ–≥–Ω–æ–∑, –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å) —è–∫—â–æ return_confidence=True
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞. –°–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –∞–±–æ –Ω–∞–≤—á—ñ—Ç—å –º–æ–¥–µ–ª—å.")
        
        try:
            # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ –¥–∞–Ω—ñ –º–∞—é—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É —Ñ–æ—Ä–º—É
            if len(X.shape) == 1:
                X = X.reshape(1, -1)
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–∞–Ω—ñ, —è–∫—â–æ —î scaler
            if self.scaler is not None:
                X_scaled = self.scaler.transform(X)
            else:
                X_scaled = X
            
            # –†–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑
            predictions = self.model.predict(X_scaled, verbose=0)
            
            # –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å
            if return_confidence:
                # –ü—Ä–æ—Å—Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –≤—ñ–¥ 0
                confidence = 1.0 / (1.0 + np.abs(predictions.flatten()))
                confidence = np.clip(confidence, 0.1, 0.9)  # –û–±–º–µ–∂—É—î–º–æ –¥—ñ–∞–ø–∞–∑–æ–Ω
                return predictions.flatten(), confidence
            
            return predictions.flatten()
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è: {e}")
            raise

# –§–∞–±—Ä–∏—á–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó
def create_transformer_lstm_model(input_shape: Tuple[int, ...]) -> OptimizedPricePredictionModel:
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è Transformer-LSTM –º–æ–¥–µ–ª—ñ"""
    return OptimizedPricePredictionModel(input_shape, "transformer_lstm")

def create_advanced_lstm_model(input_shape: Tuple[int, ...]) -> OptimizedPricePredictionModel:
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó LSTM –º–æ–¥–µ–ª—ñ"""
    return OptimizedPricePredictionModel(input_shape, "advanced_lstm")

def create_cnn_lstm_model(input_shape: Tuple[int, ...]) -> OptimizedPricePredictionModel:
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è CNN-LSTM –º–æ–¥–µ–ª—ñ"""
    return OptimizedPricePredictionModel(input_shape, "cnn_lstm")

def create_dense_model(input_shape: Tuple[int, ...]) -> OptimizedPricePredictionModel:
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è Dense –º–æ–¥–µ–ª—ñ"""
    return OptimizedPricePredictionModel(input_shape, "dense")