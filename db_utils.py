# -*- coding: utf-8 -*-
import logging
import numpy as np
import pandas as pd
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv

logger = logging.getLogger(__name__)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
load_dotenv()

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –æ–±–æ–≤‚Äô—è–∑–∫–æ–≤–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
required_env_vars = ['DB_NAME', 'DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT']
missing_vars = [var for var in required_env_vars if not os.getenv(var)]
if missing_vars:
    logger.error(f"–í—ñ–¥—Å—É—Ç–Ω—ñ –æ–±–æ–≤‚Äô—è–∑–∫–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞: {', '.join(missing_vars)}")
    raise ValueError(f"–ù–µ–æ–±—Ö—ñ–¥–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞: {', '.join(missing_vars)}")

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è SQLAlchemy engine –∑ –ø—É–ª–æ–º –∑‚Äô—î–¥–Ω–∞–Ω—å
db_string = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
engine = create_engine(db_string, pool_size=5, max_overflow=10)

def insert_symbol(symbol):
    """–í—Å—Ç–∞–≤–∫–∞ –∞–±–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è ID —Å–∏–º–≤–æ–ª—É."""
    try:
        with engine.connect() as conn:
            result = conn.execute(
                text("INSERT INTO symbols (symbol) VALUES (:symbol) ON CONFLICT (symbol) DO NOTHING RETURNING symbol_id"),
                {"symbol": symbol}
            )
            symbol_id = result.scalar()
            if symbol_id is None:
                result = conn.execute(
                    text("SELECT symbol_id FROM symbols WHERE symbol = :symbol"),
                    {"symbol": symbol}
                )
                symbol_id = result.scalar()
            conn.commit()
        logger.debug(f"–°–∏–º–≤–æ–ª {symbol} –º–∞—î symbol_id={symbol_id}")
        return symbol_id
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤—Ü—ñ —Å–∏–º–≤–æ–ª—É {symbol}: {e}")
        raise

def insert_interval(interval):
    """–í—Å—Ç–∞–≤–∫–∞ –∞–±–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è ID —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É."""
    try:
        with engine.connect() as conn:
            result = conn.execute(
                text("INSERT INTO intervals (interval) VALUES (:interval) ON CONFLICT (interval) DO NOTHING RETURNING interval_id"),
                {"interval": interval}
            )
            interval_id = result.scalar()
            if interval_id is None:
                result = conn.execute(
                    text("SELECT interval_id FROM intervals WHERE interval = :interval"),
                    {"interval": interval}
                )
                interval_id = result.scalar()
            conn.commit()
        logger.debug(f"–Ü–Ω—Ç–µ—Ä–≤–∞–ª {interval} –º–∞—î interval_id={interval_id}")
        return interval_id
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤—Ü—ñ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É {interval}: {e}")
        raise

def insert_historical_data(data, symbol_id, interval_id):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö —É PostgreSQL."""
    try:
        data = data.copy()
        data['symbol_id'] = symbol_id
        data['interval_id'] = interval_id

        columns = [
            'symbol_id', 'interval_id', 'timestamp', 'open', 'high', 'low', 'close',
            'volume', 'quote_av', 'trades', 'tb_base_av', 'tb_quote_av'
        ]

        with engine.connect() as conn:
            # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–∏—Ö
            data[columns].to_sql('historical_data', conn, if_exists='append', index=False, method='multi')
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è data_id –¥–ª—è –≤—Å—Ç–∞–≤–ª–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤
            result = conn.execute(
                text("""
                    SELECT data_id
                    FROM historical_data
                    WHERE symbol_id = :symbol_id
                    AND interval_id = :interval_id
                    AND timestamp IN :timestamps
                    ORDER BY timestamp
                """),
                {'symbol_id': symbol_id, 'interval_id': interval_id, 'timestamps': tuple(data['timestamp'])}
            )
            data_ids = [row[0] for row in result.fetchall()]
            conn.commit()
        logger.info(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(data_ids)} –∑–∞–ø–∏—Å—ñ–≤ —É PostgreSQL –¥–ª—è symbol_id={symbol_id} ({interval_id}).")
        return data_ids
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
        raise

def check_and_append_historical_data(symbol, interval, days_back, api_key, api_secret):
    from data_extraction import get_historical_data

    if not api_key or not api_secret:
        logger.error("API –∫–ª—é—á—ñ (api_key –∞–±–æ api_secret) –Ω–µ –Ω–∞–¥–∞–Ω—ñ. –ù–µ–º–æ–∂–ª–∏–≤–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ –∑ Binance.")
        raise ValueError("API –∫–ª—é—á—ñ (api_key –∞–±–æ api_secret) –Ω–µ –Ω–∞–¥–∞–Ω—ñ.")

    symbol_id = insert_symbol(symbol)
    interval_id = insert_interval(interval)
    current_time = datetime.now()

    try:
        with engine.connect() as conn:
            result = conn.execute(
                text("""
                    SELECT MAX(timestamp)
                    FROM historical_data
                    WHERE symbol_id = :symbol_id AND interval_id = :interval_id
                """),
                {"symbol_id": symbol_id, "interval_id": interval_id}
            )
            last_timestamp = result.scalar()
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} ({interval}): {e}")
        last_timestamp = None

    if last_timestamp is None:
        logger.info(f"–î–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –≤—ñ–¥—Å—É—Ç–Ω—ñ. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –∑ Binance –∑–∞ {days_back} –¥–Ω—ñ–≤.")
        start_time = current_time - timedelta(days=days_back)
        data = get_historical_data(symbol, interval=interval, days_back=days_back, api_key=api_key, api_secret=api_secret)
        if data is None or data.empty:
            logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –∑ Binance. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ API-–∫–ª—é—á—ñ, –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –∞–±–æ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —Å–∏–º–≤–æ–ª—É.")
            raise ValueError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –∑ Binance.")
        data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')
        insert_historical_data(data, symbol_id, interval_id)
        logger.info(f"–î–æ–¥–∞–Ω–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ –∑ –Ω—É–ª—è: {len(data)} –∑–∞–ø–∏—Å—ñ–≤.")
        return True

    # –î–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
    logger.debug(f"–û—Å—Ç–∞–Ω–Ω—ñ–π timestamp —É –±–∞–∑—ñ: {last_timestamp}")
    time_diff = current_time - last_timestamp
    hours_diff = time_diff.total_seconds() / 3600
    logger.debug(f"–ß–∞—Å–æ–≤–∞ —Ä—ñ–∑–Ω–∏—Ü—è: {hours_diff} –≥–æ–¥–∏–Ω")

    if hours_diff <= 1:
        logger.info(f"–î–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –∞–∫—Ç—É–∞–ª—å–Ω—ñ (–æ—Å—Ç–∞–Ω–Ω—è –¥–∞—Ç–∞: {last_timestamp}).")
        return True

    logger.info(f"–î–æ–ø–æ–≤–Ω—é—é –¥–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –∑ {last_timestamp} –¥–æ {current_time}.")
    days_to_fetch = hours_diff / 24 + 1
    new_data = get_historical_data(symbol, interval=interval, days_back=days_to_fetch, api_key=api_key, api_secret=api_secret)
    if new_data is None or new_data.empty:
        logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –∑ Binance. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ API-–∫–ª—é—á—ñ —Ç–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è.")
        raise ValueError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ –¥–ª—è {symbol} ({interval}) –∑ Binance.")

    new_data['timestamp'] = pd.to_datetime(new_data['timestamp'], unit='ms')
    new_data = new_data[new_data['timestamp'] > last_timestamp]

    if new_data.empty:
        logger.info(f"–ù–µ–º–∞—î –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –¥–ª—è {symbol} ({interval}).")
        return True

    insert_historical_data(new_data, symbol_id, interval_id)
    logger.info(f"–î–æ–¥–∞–Ω–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ: {len(new_data)} –∑–∞–ø–∏—Å—ñ–≤ –∑ {new_data['timestamp'].min()} –¥–æ {new_data['timestamp'].max()}.")
    return True

def get_historical_data_from_db(symbol, interval, days_back, api_key, api_secret):
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö —ñ–∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö —ñ–∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é —Ç–∞ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è–º."""
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∞ –¥–æ–ø–æ–≤–Ω—é—î–º–æ –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è–º
    if not check_and_append_historical_data(symbol, interval, days_back, api_key, api_secret):
        logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} ({interval}).")
        return pd.DataFrame()

    try:
        start_time = datetime.now() - timedelta(days=days_back)
        symbol_id = insert_symbol(symbol)
        interval_id = insert_interval(interval)

        query = """
            SELECT
                h.data_id, h.timestamp, h.open, h.high, h.low, h.close, h.volume,
                h.quote_av, h.trades, h.tb_base_av, h.tb_quote_av
            FROM historical_data h
            WHERE h.symbol_id = :symbol_id AND h.interval_id = :interval_id AND h.timestamp >= :start_time
            ORDER BY h.timestamp
        """
        df = pd.read_sql_query(
            text(query),
            engine,
            params={"symbol_id": symbol_id, "interval_id": interval_id, "start_time": start_time}
        )

        logger.debug(f"–û—Ç—Ä–∏–º–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤ —ñ–∑ –±–∞–∑–∏ –¥–ª—è {symbol} ({interval}).")
        return df
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –¥–∞–Ω–∏—Ö –∑ –±–∞–∑–∏ –¥–ª—è {symbol}: {e}")
        return pd.DataFrame()

def check_technical_indicators_exists(data_id):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —ñ—Å–Ω—É—î –∑–∞–ø–∏—Å —É technical_indicators –¥–ª—è –¥–∞–Ω–æ–≥–æ data_id."""
    try:
        with engine.connect() as conn:
            result = conn.execute(
                text("SELECT 1 FROM technical_indicators WHERE data_id = :data_id"),
                {"data_id": data_id}
            )
            exists = result.scalar() is not None
        logger.debug(f"–ó–∞–ø–∏—Å –¥–ª—è data_id={data_id} —É technical_indicators —ñ—Å–Ω—É—î: {exists}")
        return exists
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Å—É –¥–ª—è data_id={data_id}: {e}")
        raise

def insert_technical_indicators(data_id, indicators):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ —É PostgreSQL –∑ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º –ø—Ä–∏ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ."""
    try:
        with engine.connect() as conn:
            conn.execute(
                text("""
                    INSERT INTO technical_indicators (
                        data_id, rsi, macd, macd_signal, upper_band, lower_band,
                        stoch, stoch_signal, ema, atr, cci, obv, volatility,
                        volume_pct, close_lag1, close_lag2, close_diff, log_return,
                        hour_norm, day_norm, adx, vwap
                    ) VALUES (
                        :data_id, :rsi, :macd, :macd_signal, :upper_band, :lower_band,
                        :stoch, :stoch_signal, :ema, :atr, :cci, :obv, :volatility,
                        :volume_pct, :close_lag1, :close_lag2, :close_diff, :log_return,
                        :hour_norm, :day_norm, :adx, :vwap
                    )
                    ON CONFLICT (data_id) DO UPDATE SET
                        rsi = EXCLUDED.rsi,
                        macd = EXCLUDED.macd,
                        macd_signal = EXCLUDED.macd_signal,
                        upper_band = EXCLUDED.upper_band,
                        lower_band = EXCLUDED.lower_band,
                        stoch = EXCLUDED.stoch,
                        stoch_signal = EXCLUDED.stoch_signal,
                        ema = EXCLUDED.ema,
                        atr = EXCLUDED.atr,
                        cci = EXCLUDED.cci,
                        obv = EXCLUDED.obv,
                        volatility = EXCLUDED.volatility,
                        volume_pct = EXCLUDED.volume_pct,
                        close_lag1 = EXCLUDED.close_lag1,
                        close_lag2 = EXCLUDED.close_lag2,
                        close_diff = EXCLUDED.close_diff,
                        log_return = EXCLUDED.log_return,
                        hour_norm = EXCLUDED.hour_norm,
                        day_norm = EXCLUDED.day_norm,
                        adx = EXCLUDED.adx,
                        vwap = EXCLUDED.vwap
                """),
                indicators
            )
            conn.commit()
        logger.debug(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ –∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –¥–ª—è data_id={data_id}")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –¥–ª—è data_id={data_id}: {e}")
        raise

def insert_normalized_data(data_id, normalized_values):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö —É PostgreSQL –∑ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º –ø—Ä–∏ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ."""
    try:
        data_id = int(data_id)
        records = [{"data_id": data_id, "feature": k, "normalized_value": float(v)} for k, v in normalized_values.items()]
        with engine.connect() as conn:
            conn.execute(
                text("""
                    INSERT INTO normalized_data (data_id, feature, normalized_value)
                    VALUES (:data_id, :feature, :normalized_value)
                    ON CONFLICT (data_id, feature) DO UPDATE SET
                        normalized_value = EXCLUDED.normalized_value
                """),
                records
            )
            conn.commit()
        logger.debug(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ –∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è data_id={data_id}")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è data_id={data_id}: {e}")
        raise

def insert_training_history(history_data):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —É PostgreSQL."""
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –∑–Ω–∞—á–µ–Ω—å NumPy —É –ø—Ä–æ—Å—Ç—ñ —Ç–∏–ø–∏ float
        for key in ['loss', 'mae', 'mape', 'val_loss', 'val_mae', 'val_mape', 'real_mae', 'real_mape']:
            if isinstance(history_data[key], np.floating):
                history_data[key] = float(history_data[key])
            elif isinstance(history_data[key], np.ndarray):
                history_data[key] = float(history_data[key].item())

        with engine.connect() as conn:
            conn.execute(
                text("""
                    INSERT INTO training_history (
                        symbol_id, interval_id, fold, epoch, loss, mae, mape,
                        val_loss, val_mae, val_mape, real_mae, real_mape
                    ) VALUES (
                        :symbol_id, :interval_id, :fold, :epoch, :loss, :mae, :mape,
                        :val_loss, :val_mae, :val_mape, :real_mae, :real_mape
                    )
                    ON CONFLICT ON CONSTRAINT training_history_symbol_id_interval_id_fold_epoch_key
                    DO UPDATE SET
                        loss = EXCLUDED.loss,
                        mae = EXCLUDED.mae,
                        mape = EXCLUDED.mape,
                        val_loss = EXCLUDED.val_loss,
                        val_mae = EXCLUDED.val_mae,
                        val_mape = EXCLUDED.val_mape,
                        real_mae = EXCLUDED.real_mae,
                        real_mape = EXCLUDED.real_mape
                """),
                history_data
            )
            conn.commit()
        logger.debug(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ –∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω–æ —ñ—Å—Ç–æ—Ä—ñ—é —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: fold={history_data['fold']}, epoch={history_data['epoch']}")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —ñ—Å—Ç–æ—Ä—ñ—ó —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {e}")
        raise

def insert_prediction(prediction_data):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ —É PostgreSQL."""
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –∑–Ω–∞—á–µ–Ω—å NumPy —É –ø—Ä–æ—Å—Ç—ñ —Ç–∏–ø–∏ float
        for key in ['last_price', 'predicted_price', 'fold_1_prediction', 'fold_2_prediction',
                    'fold_3_prediction', 'fold_4_prediction', 'fold_5_prediction']:
            if key in prediction_data and prediction_data[key] is not None:
                if isinstance(prediction_data[key], np.floating):
                    prediction_data[key] = float(prediction_data[key])
                elif isinstance(prediction_data[key], np.ndarray):
                    prediction_data[key] = float(prediction_data[key].item())

        with engine.connect() as conn:
            conn.execute(
                text("""
                    INSERT INTO predictions (
                        symbol_id, interval_id, timestamp, last_price, predicted_price,
                        fold_1_prediction, fold_2_prediction, fold_3_prediction,
                        fold_4_prediction, fold_5_prediction
                    ) VALUES (
                        :symbol_id, :interval_id, :timestamp, :last_price, :predicted_price,
                        :fold_1_prediction, :fold_2_prediction, :fold_3_prediction,
                        :fold_4_prediction, :fold_5_prediction
                    )
                    ON CONFLICT ON CONSTRAINT predictions_symbol_id_interval_id_timestamp_key
                    DO UPDATE SET
                        last_price = EXCLUDED.last_price,
                        predicted_price = EXCLUDED.predicted_price,
                        fold_1_prediction = EXCLUDED.fold_1_prediction,
                        fold_2_prediction = EXCLUDED.fold_2_prediction,
                        fold_3_prediction = EXCLUDED.fold_3_prediction,
                        fold_4_prediction = EXCLUDED.fold_4_prediction,
                        fold_5_prediction = EXCLUDED.fold_5_prediction
                """),
                prediction_data
            )
            conn.commit()
        logger.debug(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ –∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è timestamp={prediction_data['timestamp']}")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –ø—Ä–æ–≥–Ω–æ–∑—É –¥–ª—è timestamp={prediction_data['timestamp']}: {e}")
        raise

def insert_scaler_stats(symbol_id, interval_id, target_mean, target_std):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è."""
    try:
        with engine.connect() as conn:
            conn.execute(
                text("""
                    INSERT INTO scaler_stats (symbol_id, interval_id, target_mean, target_std)
                    VALUES (:symbol_id, :interval_id, :target_mean, :target_std)
                    ON CONFLICT (symbol_id, interval_id) DO UPDATE
                    SET target_mean = EXCLUDED.target_mean, target_std = EXCLUDED.target_std
                """),
                {
                    "symbol_id": symbol_id,
                    "interval_id": interval_id,
                    "target_mean": float(target_mean),
                    "target_std": float(target_std)
                }
            )
            conn.commit()
        logger.debug(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è symbol_id={symbol_id}, interval_id={interval_id}")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è symbol_id={symbol_id}: {e}")
        raise

def get_scaler_stats(symbol_id, interval_id):
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è."""
    try:
        with engine.connect() as conn:
            result = conn.execute(
                text("""
                    SELECT target_mean, target_std
                    FROM scaler_stats
                    WHERE symbol_id = :symbol_id AND interval_id = :interval_id
                """),
                {"symbol_id": symbol_id, "interval_id": interval_id}
            )
            row = result.fetchone()
        if row:
            logger.debug(f"–û—Ç—Ä–∏–º–∞–Ω–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è: mean={row[0]}, std={row[1]}")
            return row[0], row[1]
        logger.warning(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è symbol_id={symbol_id}, interval_id={interval_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞.")
        return None, None
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è symbol_id={symbol_id}: {e}")
        return None, None

# ===============================
# –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–Ü BATCH –û–ü–ï–†–ê–¶–Ü–á –î–õ–Ø –®–í–ò–î–ö–û–î–Ü–á
# ===============================

def batch_insert_historical_data(data_list, symbol_id, interval_id):
    """–®–í–ò–î–ö–ê batch –≤—Å—Ç–∞–≤–∫–∞ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö (–∑–∞–º—ñ—Å—Ç—å –ø–æ–≤—ñ–ª—å–Ω–∏—Ö –æ–¥–∏–Ω–∞—Ä–Ω–∏—Ö)."""
    import time
    start_time = time.time()
    
    try:
        if not data_list:
            return []
            
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è batch insert
        records = []
        for data_point in data_list:
            records.append({
                'symbol_id': symbol_id,
                'interval_id': interval_id,
                'timestamp': data_point['timestamp'],
                'open': float(data_point['open']),
                'high': float(data_point['high']),
                'low': float(data_point['low']),
                'close': float(data_point['close']),
                'volume': float(data_point['volume']),
                'quote_av': float(data_point.get('quote_av', data_point['volume'] * data_point['close'])),
                'trades': int(data_point.get('trades', 0)),
                'tb_base_av': float(data_point.get('tb_base_av', data_point['volume'] * 0.5)),
                'tb_quote_av': float(data_point.get('tb_quote_av', data_point['volume'] * data_point['close'] * 0.5)),
            })
        
        with engine.connect() as conn:
            # –í–∏–∫–æ–Ω—É—î–º–æ batch insert –∑ upsert
            data_ids = []
            stmt = text("""
                INSERT INTO historical_data 
                (symbol_id, interval_id, timestamp, open, high, low, close, volume, quote_av, trades, tb_base_av, tb_quote_av)
                VALUES 
                (:symbol_id, :interval_id, :timestamp, :open, :high, :low, :close, :volume, :quote_av, :trades, :tb_base_av, :tb_quote_av)
                ON CONFLICT (symbol_id, interval_id, timestamp)
                DO UPDATE SET
                    open = EXCLUDED.open,
                    high = EXCLUDED.high,
                    low = EXCLUDED.low,
                    close = EXCLUDED.close,
                    volume = EXCLUDED.volume,
                    quote_av = EXCLUDED.quote_av,
                    trades = EXCLUDED.trades,
                    tb_base_av = EXCLUDED.tb_base_av,
                    tb_quote_av = EXCLUDED.tb_quote_av
                RETURNING data_id
            """)
            
            # –í–∏–∫–æ–Ω—É—î–º–æ batch –≤—Å—Ç–∞–≤–∫—É
            for record in records:
                result = conn.execute(stmt, record)
                data_id = result.scalar()
                data_ids.append(data_id)
            
            conn.commit()
        
        duration = time.time() - start_time
        speed = len(records) / duration if duration > 0 else 0
        logger.info(f"‚úÖ BATCH INSERT: {len(records)} –∑–∞–ø–∏—Å—ñ–≤ –∑–∞ {duration:.3f}s ({speed:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫)")
        
        return data_ids
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ batch insert —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
        raise


def batch_insert_technical_indicators(indicators_list):
    """–®–í–ò–î–ö–ê batch –≤—Å—Ç–∞–≤–∫–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ (–∑–∞–º—ñ—Å—Ç—å –ø–æ–≤—ñ–ª—å–Ω–∏—Ö –æ–¥–∏–Ω–∞—Ä–Ω–∏—Ö)."""
    import time
    start_time = time.time()
    
    try:
        if not indicators_list:
            return 0
        
        with engine.connect() as conn:
            # Batch upsert —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
            stmt = text("""
                INSERT INTO technical_indicators 
                (data_id, rsi, macd, macd_signal, upper_band, lower_band, stoch, stoch_signal, 
                 ema, atr, cci, obv, volatility, volume_pct, close_lag1, close_lag2, 
                 close_diff, log_return, hour_norm, day_norm, adx, vwap)
                VALUES 
                (:data_id, :rsi, :macd, :macd_signal, :upper_band, :lower_band, :stoch, :stoch_signal,
                 :ema, :atr, :cci, :obv, :volatility, :volume_pct, :close_lag1, :close_lag2,
                 :close_diff, :log_return, :hour_norm, :day_norm, :adx, :vwap)
                ON CONFLICT (data_id)
                DO UPDATE SET
                    rsi = EXCLUDED.rsi,
                    macd = EXCLUDED.macd,
                    macd_signal = EXCLUDED.macd_signal,
                    upper_band = EXCLUDED.upper_band,
                    lower_band = EXCLUDED.lower_band,
                    stoch = EXCLUDED.stoch,
                    stoch_signal = EXCLUDED.stoch_signal,
                    ema = EXCLUDED.ema,
                    atr = EXCLUDED.atr,
                    cci = EXCLUDED.cci,
                    obv = EXCLUDED.obv,
                    volatility = EXCLUDED.volatility,
                    volume_pct = EXCLUDED.volume_pct,
                    close_lag1 = EXCLUDED.close_lag1,
                    close_lag2 = EXCLUDED.close_lag2,
                    close_diff = EXCLUDED.close_diff,
                    log_return = EXCLUDED.log_return,
                    hour_norm = EXCLUDED.hour_norm,
                    day_norm = EXCLUDED.day_norm,
                    adx = EXCLUDED.adx,
                    vwap = EXCLUDED.vwap
            """)
            
            # –í–∏–∫–æ–Ω—É—î–º–æ batch –≤—Å—Ç–∞–≤–∫—É
            conn.execute(stmt, indicators_list)
            conn.commit()
        
        duration = time.time() - start_time
        speed = len(indicators_list) / duration if duration > 0 else 0
        logger.info(f"‚úÖ BATCH INDICATORS: {len(indicators_list)} –∑–∞–ø–∏—Å—ñ–≤ –∑–∞ {duration:.3f}s ({speed:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫)")
        
        return len(indicators_list)
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ batch insert —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤: {e}")
        raise


def optimize_database_performance():
    """–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ë–î –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ–¥—ñ—ó."""
    try:
        with engine.connect() as conn:
            optimization_queries = [
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ–¥—ñ—ó
                "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_historical_data_symbol_interval_timestamp ON historical_data (symbol_id, interval_id, timestamp DESC);",
                "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_technical_indicators_data_id ON technical_indicators (data_id);",
                "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_symbols_symbol ON symbols (symbol);",
                "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_intervals_interval ON intervals (interval);",
                # –ê–Ω–∞–ª—ñ–∑ —Ç–∞–±–ª–∏—Ü—å –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫–∞
                "ANALYZE historical_data;",
                "ANALYZE technical_indicators;",
                "ANALYZE symbols;",
                "ANALYZE intervals;",
            ]
            
            for query in optimization_queries:
                try:
                    conn.execute(text(query))
                    conn.commit()
                    logger.info(f"‚úÖ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è: {query[:50]}...")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–º–æ–∂–ª–∏–≤–æ, –≤–∂–µ —ñ—Å–Ω—É—î): {e}")
                    
        logger.info("üöÄ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ë–î –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                    
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ë–î: {e}")
