# -*- coding: utf-8 -*-
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –≥–æ–ª–æ–≤–Ω–∏–π –º–æ–¥—É–ª—å –∑ –Ω–æ–≤–æ—é –∞—Ä—Ö—ñ        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–µ—à—É
        cache_stats = get_cache_info()
        logger.info(f"üíæ –ö–µ—à —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ: {cache_stats['memory_cache_size']} MB –ø–∞–º'—è—Ç—ñ, {cache_stats['redis_keys']} –∫–ª—é—á—ñ–≤")—Ç—É—Ä–æ—é
–Ü–Ω—Ç–µ–≥—Ä—É—î –≤—Å—ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó: –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ—Å—Ç—å, –∫–µ—à—É–≤–∞–Ω–Ω—è, GPU, Rust —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
"""
import numpy as np
import pandas as pd
import asyncio
import logging
import sys
import os
import time
import argparse
from binance_loader import save_ohlcv_to_db
from datetime import datetime, timedelta
from pathlib import Path

# –°–∏—Å—Ç–µ–º–Ω—ñ –º–æ–¥—É–ª—ñ
from dotenv import load_dotenv

# –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–¥—É–ª—ñ
from optimized_db import db_manager, save_technical_indicators_batch, save_normalized_data_batch, save_predictions
from optimized_indicators import global_calculator
from optimized_model import OptimizedPricePredictionModel, DatabaseHistoryCallback, DenormalizedMetricsCallback
from cache_system import cache_manager, get_cache_info
from async_architecture import ml_pipeline, init_async_system, shutdown_async_system
from gpu_config import configure_gpu, get_gpu_info
from monitoring_system import monitoring_system
from fundamental_integrator import fundamental_integrator
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ optimized_config –∑–∞–º—ñ—Å—Ç—å config
from optimized_config import SYMBOL, INTERVAL, DAYS_BACK, LOOK_BACK, STEPS, MODEL_CONFIG

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimized_app.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class OptimizedCryptoMLSystem:
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self):
        self.initialized = False
        self.gpu_available = False
        
    async def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏"""
        if self.initialized:
            return
            
        logger.info("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
        load_dotenv()
        self._validate_environment()
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è GPU
        self.gpu_available = configure_gpu()
        if self.gpu_available:
            gpu_info = get_gpu_info()
            logger.info(f"‚úÖ GPU –¥–æ—Å—Ç—É–ø–Ω–∏–π: {len(gpu_info['details'])} –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤")

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏
        await init_async_system()

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–µ—à—É
        cache_stats = get_cache_info()
        logger.info(f"ÔøΩ –ö–µ—à —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ: {cache_stats['memory_cache_size']} MB –ø–∞–º'—è—Ç—ñ, {cache_stats['redis_keys']} –∫–ª—é—á—ñ–≤")

        # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ –ë–î
        try:
            await db_manager.execute_query_cached("SELECT 1 as test", use_cache=False)
            logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –ø—ñ–¥–∫–ª—é—á–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ –ë–î: {e}")
            raise

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É —Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        monitoring_system.db_manager = db_manager
        fundamental_integrator.db_manager = db_manager
        fundamental_integrator.cache_manager = cache_manager
        await fundamental_integrator.initialize()

        self.initialized = True
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
    
    def _validate_environment(self):
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞"""
        required_vars = ['API_KEY', 'API_SECRET', 'DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASSWORD']
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            logger.error(f"‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞: {', '.join(missing_vars)}")
            raise ValueError(f"–ù–µ–æ–±—Ö—ñ–¥–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∑–º—ñ–Ω–Ω—ñ: {', '.join(missing_vars)}")
    
    async def process_symbol_optimized(self, 
                                     symbol: str, 
                                     interval: str, 
                                     days_back: int,
                                     look_back: int,
                                     steps: int,
                                     force_retrain: bool = False,
                                     use_cv: bool = False,
                                     model_type: str = "advanced_lstm"):
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å–∏–º–≤–æ–ª—É"""
        logger.info(f"üìà –ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏ {symbol} ({interval})")
        start_time = datetime.now()
        
        try:
            # –ö–µ—à—É–≤–∞–Ω–Ω—è symbol_id —Ç–∞ interval_id
            cache_key = f"{symbol}_{interval}_ids"
            cached_ids = cache_manager.get(cache_key)
            
            if cached_ids:
                symbol_id, interval_id = cached_ids
            else:
                symbol_id = await db_manager.get_or_create_symbol_id(symbol)
                interval_id = await db_manager.get_or_create_interval_id(interval)
                cache_manager.set(cache_key, (symbol_id, interval_id), ttl=86400)  # 24 –≥–æ–¥–∏–Ω–∏
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º
            data = await db_manager.get_historical_data_optimized(
                symbol_id, interval_id, days_back, use_cache=True
            )
            
            if data.empty:
                logger.error(f"‚ùå –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è {symbol}")
                return None
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
            indicators = await global_calculator.calculate_all_indicators_batch(data)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ OHLCV –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ join
            original_ohlcv = data[['open', 'high', 'low', 'close', 'volume']].copy()
            
            # –î–æ–¥–∞–≤–∞–Ω–Ω—è —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –¥–æ –¥–∞–Ω–∏—Ö
            for name, indicator in indicators.items():
                if len(indicator) > 0:
                    data = data.join(indicator, how='inner', lsuffix='_orig', rsuffix=f'_{name}')
            
            # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ OHLCV –∫–æ–ª–æ–Ω–∫–∏ (join –º—ñ–≥ —ó—Ö –ø–µ—Ä–µ–ø–∏—Å–∞—Ç–∏)
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col in original_ohlcv.columns:
                    data[col] = original_ohlcv[col]
            
            # –û—á–∏—â–µ–Ω–Ω—è –≤—ñ–¥ NaN
            initial_count = len(data)
            data = data.dropna()
            if len(data) < initial_count:
                logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ NaN: {initial_count} ‚Üí {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
            else:
                logger.info(f"‚úì –î–∞–Ω—ñ –±–µ–∑ NaN: {len(data)} –∑–∞–ø–∏—Å—ñ–≤")

            # –î–û–î–ê–í–ê–ù–ù–Ø –î–û–î–ê–¢–ö–û–í–ò–• –°–¢–ê–¢–ò–°–¢–ò–ß–ù–ò–• –§–Ü–ß–ï–ô
            # ...existing code...
            
            # –í–ò–î–ê–õ–ï–ù–ù–Ø OUTLIERS - –∫—Ä–∏—Ç–∏—á–Ω–∏–π –∫—Ä–æ–∫ –¥–ª—è —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
            outliers_start = len(data)
            logger.info(f"üîç –í–∏–¥–∞–ª–µ–Ω–Ω—è outliers –∑ {outliers_start} –∑–∞–ø–∏—Å—ñ–≤...")

            # –í–∏–¥–∞–ª—è—î–º–æ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ü—ñ–Ω (–±—ñ–ª—å—à–µ 10 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –≤—ñ–¥—Ö–∏–ª–µ–Ω—å)
            price_cols = ['close', 'high', 'low', 'open']
            for col in price_cols:
                if col in data.columns:
                    mean_price = data[col].mean()
                    std_price = data[col].std()
                    # –í–∏–¥–∞–ª—è—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è, —è–∫—ñ –≤—ñ–¥—Ö–∏–ª—è—é—Ç—å—Å—è –±—ñ–ª—å—à–µ –Ω—ñ–∂ –Ω–∞ 5 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –≤—ñ–¥—Ö–∏–ª–µ–Ω—å
                    data = data[abs(data[col] - mean_price) <= 5 * std_price]

            # –í–∏–¥–∞–ª—è—î–º–æ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ –æ–±'—î–º–∏ (–±—ñ–ª—å—à–µ 10 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –≤—ñ–¥—Ö–∏–ª–µ–Ω—å)
            if 'volume' in data.columns:
                vol_mean = data['volume'].mean()
                vol_std = data['volume'].std()
                data = data[abs(data['volume'] - vol_mean) <= 10 * vol_std]

            # –í–∏–¥–∞–ª—è—î–º–æ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
            indicator_cols = ['RSI', 'MACD', 'ATR', 'Stoch_K', 'Stoch_D', 'Williams_R', 'CCI', 'ADX']
            for col in indicator_cols:
                if col in data.columns:
                    # RSI –º–∞—î –±—É—Ç–∏ –º—ñ–∂ 0-100, —ñ–Ω—à—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –º–∞—é—Ç—å —Ä–æ–∑—É–º–Ω—ñ –º–µ–∂—ñ
                    if col == 'RSI':
                        data = data[(data[col] >= 0) & (data[col] <= 100)]
                    elif col in ['Stoch_K', 'Stoch_D']:
                        data = data[(data[col] >= -20) & (data[col] <= 120)]  # Stochastic –º–æ–∂–µ –≤–∏—Ö–æ–¥–∏—Ç–∏ –∑–∞ 0-100
                    elif col == 'Williams_R':
                        data = data[(data[col] >= -100) & (data[col] <= 0)]  # Williams %R –≤—ñ–¥ -100 –¥–æ 0
                    else:
                        # –î–ª—è —ñ–Ω—à–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –≤–∏–¥–∞–ª—è—î–º–æ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
                        col_mean = data[col].mean()
                        col_std = data[col].std()
                        data = data[abs(data[col] - col_mean) <= 5 * col_std]

            outliers_removed = outliers_start - len(data)
            if outliers_removed > 0:
                logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ outliers: {outliers_start} ‚Üí {len(data)} –∑–∞–ø–∏—Å—ñ–≤ (-{outliers_removed})")
            else:
                logger.info(f"‚úì Outliers –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
            
            if len(data) < look_back:
                logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏: {len(data)} < {look_back}")
                return None
            
            # –î–û–î–ê–¢–ö–û–í–Ü –°–¢–ê–¢–ò–°–¢–ò–ß–ù–Ü –§–Ü–ß–Ü
            # Rolling statistics –¥–ª—è —Ü—ñ–Ω–∏
            data['close_rolling_mean_10'] = data['close'].rolling(10).mean()
            data['close_rolling_std_10'] = data['close'].rolling(10).std()
            data['close_rolling_skew_20'] = data['close'].rolling(20).skew()
            data['close_rolling_kurt_20'] = data['close'].rolling(20).kurt()
            
            # Volume-based features
            if 'volume' in data.columns:
                data['volume_rolling_mean_10'] = data['volume'].rolling(10).mean()
                data['volume_rolling_std_10'] = data['volume'].rolling(10).std()
                data['volume_to_price_ratio'] = data['volume'] / (data['close'] + 1e-6)
                data['volume_change'] = data['volume'].pct_change().fillna(0)
            
            # RSI-based features
            if 'RSI' in data.columns:
                data['rsi_overbought'] = (data['RSI'] > 70).astype(int)
                data['rsi_oversold'] = (data['RSI'] < 30).astype(int)
                data['rsi_divergence'] = data['RSI'].diff(5)  # 5-period RSI change
            
            # MACD-based features
            if 'MACD' in data.columns and 'MACD_Signal' in data.columns:
                data['macd_histogram'] = data['MACD'] - data['MACD_Signal']
                data['macd_crossover'] = np.where(data['MACD'] > data['MACD_Signal'], 1, -1)
                data['macd_trend'] = data['macd_histogram'].rolling(5).mean()
            
            # Bollinger Bands advanced features
            if 'BB_Upper' in data.columns and 'BB_Lower' in data.columns:
                data['bb_squeeze'] = (data['BB_Upper'] - data['BB_Lower']) / data['close']
                data['bb_breakout_up'] = (data['close'] > data['BB_Upper']).astype(int)
                data['bb_breakout_down'] = (data['close'] < data['BB_Lower']).astype(int)
            
            # Stochastic features
            if 'Stoch_K' in data.columns and 'Stoch_D' in data.columns:
                data['stoch_divergence'] = data['Stoch_K'] - data['Stoch_D']
                data['stoch_overbought'] = (data['Stoch_K'] > 80).astype(int)
                data['stoch_oversold'] = (data['Stoch_K'] < 20).astype(int)
            
            # ATR-based volatility features
            if 'ATR' in data.columns:
                data['atr_ratio'] = data['ATR'] / data['close']
                data['atr_change'] = data['ATR'].pct_change().fillna(0)
            
            # Price action patterns
            data['doji'] = abs(data['close'] - data['open']) / (data['high'] - data['low'] + 1e-6) < 0.1
            data['hammer'] = ((data['high'] - data['low'] > 0) & 
                            (abs(data['open'] - data['close']) < 0.3 * (data['high'] - data['low'])) & 
                            ((data['low'] - data['close']) > 0.6 * (data['high'] - data['low']))).astype(int)
            
            # Time-based features (—è–∫—â–æ —î timestamp)
            if 'timestamp' in data.columns:
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ timestamp –ø—Ä–∞–≤–∏–ª—å–Ω–æ (Unix timestamp –≤ –º—ñ–ª—ñ—Å–µ–∫—É–Ω–¥–∞—Ö)
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Ü–µ —á–∏—Å–ª–∞
                if pd.api.types.is_numeric_dtype(data['timestamp']):
                    if data['timestamp'].max() > 1e10:  # –ú—ñ–ª—ñ—Å–µ–∫—É–Ω–¥–∏
                        data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')
                    else:  # –°–µ–∫—É–Ω–¥–∏ –∞–±–æ –≤–∂–µ datetime
                        data['timestamp'] = pd.to_datetime(data['timestamp'])
                
                data['hour'] = data['timestamp'].dt.hour
                data['day_of_week'] = data['timestamp'].dt.dayofweek
                data['month'] = data['timestamp'].dt.month
                # –¶–∏–∫–ª—ñ—á–Ω—ñ features –¥–ª—è —á–∞—Å—É
                data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
                data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
                data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
                data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
            
            # –†–æ–∑—à–∏—Ä–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Ñ—ñ—á–µ–π –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
            strategic_features = [
                # Basic OHLCV
                'close', 'volume', 'high', 'low', 'open',
                
                # Technical indicators
                'RSI', 'MACD', 'MACD_Signal', 'ATR', 'EMA_20', 'EMA_10', 'EMA_50',
                'Stoch_K', 'Stoch_D', 'Williams_R', 'CCI',
                
                # Price-based features
                'trend', 'volatility', 'return', 'momentum', 'momentum_10', 'momentum_20',
                'return_5', 'return_10', 'close_lag1', 'close_lag2', 'close_diff', 'log_return',
                'close_rolling_mean_10', 'close_rolling_std_10', 'close_rolling_skew_20', 'close_rolling_kurt_20',
                
                # Volume features
                'volume_pct', 'volume_ma5', 'volume_ma20', 'volume_std',
                'volume_rolling_mean_10', 'volume_rolling_std_10', 'volume_to_price_ratio', 'volume_change',
                
                # Bollinger Bands
                'bb_dist_upper', 'bb_dist_lower', 'bb_width', 'bb_position', 'bb_squeeze', 'bb_breakout_up', 'bb_breakout_down',
                
                # RSI features
                'rsi_overbought', 'rsi_oversold', 'rsi_divergence',
                
                # MACD features
                'macd_histogram', 'macd_crossover', 'macd_trend',
                
                # Stochastic features
                'stoch_divergence', 'stoch_overbought', 'stoch_oversold',
                
                # ATR features
                'atr_ratio', 'atr_change',
                
                # Price action patterns
                'doji', 'hammer', 'high_low_ratio', 'close_open_ratio',
                
                # Time features (if available)
                'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos'
            ]
            feature_columns = [f for f in strategic_features if f in data.columns]
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø—ñ—Å–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—ñ—á–µ–π
            logger.info(f"üìä –§—ñ–Ω–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(data)} –∑–∞–ø–∏—Å—ñ–≤, {len(feature_columns)} —Ñ—ñ—á–µ–π")
            
            # –í–∏–¥–∞–ª—è—î–º–æ NaN –ø—ñ—Å–ª—è lag-—Ñ—ñ—á–µ–π —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞ Inf
            data = data.replace([np.inf, -np.inf], np.nan)
            data = data.dropna()
            
            if len(data) < look_back:
                logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏: {len(data)} < {look_back}")
                return None
            
            logger.info(f"üìä –§—ñ–Ω–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(data)} –∑–∞–ø–∏—Å—ñ–≤, {len(feature_columns)} —Ñ—ñ—á–µ–π")
            
            # –ó–ë–Ü–† –§–£–ù–î–ê–ú–ï–ù–¢–ê–õ–¨–ù–ò–• –î–ê–ù–ò–•
            fundamental_start = len(feature_columns)
            logger.info(f"üì∞ –ó–±—ñ—Ä —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è {symbol}...")
            try:
                # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ timestamp —è–∫ —ñ–Ω–¥–µ–∫—Å –¥–ª—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                if 'timestamp' in data.columns:
                    data.set_index('timestamp', inplace=True)
                    logger.info(f"üìÖ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ timestamp —è–∫ —ñ–Ω–¥–µ–∫—Å –¥–ª—è {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
                
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–µ—Ä—ñ–æ–¥ –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–æ—Ä—É —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                data_start_time = data.index.min()
                data_end_time = data.index.max()
                hours_back = int((data_end_time - data_start_time).total_seconds() / 3600)
                
                # –ó–±–∏—Ä–∞—î–º–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–∞ –ø–µ—Ä—ñ–æ–¥ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                fundamental_data = await fundamental_integrator.collect_fundamental_data_for_period(
                    symbol, data_start_time, data_end_time
                )
                
                if fundamental_data:
                    logger.info(f"üìä –ó—ñ–±—Ä–∞–Ω–æ {len(fundamental_data)} —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
                    
                    # –û—Ç—Ä–∏–º—É—î–º–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏ –¥–ª—è –ø–µ—Ä—ñ–æ–¥—É –¥–∞–Ω–∏—Ö
                    start_time = data.index.min()
                    end_time = data.index.max()
                    
                    fundamental_df = await fundamental_integrator.get_fundamental_features(
                        symbol, start_time, end_time
                    )
                    
                    if not fundamental_df.empty:
                        # –ö–æ–º–±—ñ–Ω—É—î–º–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
                        data = fundamental_integrator.combine_with_technical_data(data, fundamental_df)
                        logger.info(f"üîó –ü–æ—î–¥–Ω–∞–Ω–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ: {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
                        
                        # –û–Ω–æ–≤–ª—é—î–º–æ feature_columns –∑ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏
                        fundamental_features = [
                            'aggregate_sentiment', 'news_sentiment_score', 'social_sentiment_score',
                            'active_addresses', 'transaction_count', 'whale_activity'
                        ]
                        feature_columns.extend([f for f in fundamental_features if f in data.columns])
                        
                        features_added = len(feature_columns) - fundamental_start
                        logger.info(f"üìä –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ —Ñ—ñ—á—ñ: {fundamental_start} ‚Üí {len(feature_columns)} (+{features_added})")
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ–º–∞—î —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} –≤ –ø–µ—Ä—ñ–æ–¥—ñ {start_time} - {end_time}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑—ñ–±—Ä–∞—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è {symbol}")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–±–æ—Ä—É —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
                # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            
            # –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –¢–ï–•–ù–Ü–ß–ù–ò–• –Ü–ù–î–ò–ö–ê–¢–û–†–Ü–í –í –ë–î
            try:
                await save_technical_indicators_batch(db_manager, symbol, interval, data)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏: {e}")
            
            # Time-series validation: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –°–ï–†–ï–î–ù–Ü 20% —è–∫ –≤–∞–ª—ñ–¥–∞—Ü—ñ—é (–Ω–µ –æ—Å—Ç–∞–Ω–Ω—ñ!)
            # –î–ª—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –∫—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ —Å–µ—Ä–µ–¥–∏–Ω–∏ –ø–µ—Ä—ñ–æ–¥—É
            # —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø—Ä–æ–±–ª–µ–º –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ä–∏–Ω–∫–æ–≤–∏–º–∏ —É–º–æ–≤–∞–º–∏

            # –†–æ–∑–¥—ñ–ª—è—î–º–æ –Ω–∞ train/val/test: 60% / 20% / 20%
            n_total = len(data)
            train_end = int(n_total * 0.6)
            val_end = int(n_total * 0.8)

            train_data = data.iloc[:train_end]
            val_data = data.iloc[train_end:val_end]
            # test_data = data.iloc[val_end:]  # –∑–∞—Ä–µ–∑–µ—Ä–≤–æ–≤–∞–Ω–æ –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è

            logger.info(f"üìä –•—Ä–æ–Ω–æ–ª–æ–≥—ñ—á–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª: Train={len(train_data)}, Val={len(val_data)} (60%/20%)")
            
            X_train_raw = train_data[feature_columns].values
            X_val_raw = val_data[feature_columns].values

            # –í–ê–ñ–õ–ò–í–û: RobustScaler –∫—Ä–∞—â–µ –¥–ª—è trending —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–µ–¥—ñ–∞–Ω—É —Ç–∞ IQR –∑–∞–º—ñ—Å—Ç—å min/max, —Å—Ç—ñ–π–∫–∏–π –¥–æ outliers
            from sklearn.preprocessing import RobustScaler
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train_raw)
            X_val_scaled = scaler.transform(X_val_raw)  # transform, –ù–ï fit_transform!
            
            # RobustScaler –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î center_ —Ç–∞ scale_ –∑–∞–º—ñ—Å—Ç—å min/max
            close_idx_feat = feature_columns.index('close')
            logger.info(f"‚úì Scaler: train close center={scaler.center_[close_idx_feat]:.2f}, scale={scaler.scale_[close_idx_feat]:.2f}")

            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–ª—è train
            X_train_sequences = []
            for i in range(len(X_train_scaled) - look_back):
                X_train_sequences.append(X_train_scaled[i:i + look_back])
            X_train_sequences = np.array(X_train_sequences)
            
            # Data Augmentation –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó
            def augment_training_data(X_sequences, y_targets, augmentation_factor=2):
                """–î–æ–¥–∞—î–º–æ —à—É–º —Ç–∞ –Ω–µ–≤–µ–ª–∏–∫—ñ perturbation –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
                augmented_sequences = [X_sequences]
                augmented_targets = [y_targets]
                
                for _ in range(augmentation_factor - 1):
                    # –î–æ–¥–∞—î–º–æ –≥–∞—É—Å—ñ–≤—Å—å–∫–∏–π —à—É–º (0.5% –≤—ñ–¥ std –∫–æ–∂–Ω–æ–≥–æ feature)
                    noise = np.random.normal(0, 0.005, X_sequences.shape)
                    noisy_sequences = X_sequences + noise * np.std(X_sequences, axis=(0, 1), keepdims=True)
                    
                    # –ù–µ–≤–µ–ª–∏–∫—ñ —á–∞—Å–æ–≤—ñ –∑—Å—É–≤–∏ (1-2 –∫—Ä–æ–∫–∏)
                    shift_amount = np.random.randint(-1, 2, size=X_sequences.shape[0])  # -1, 0, –∞–±–æ 1
                    
                    shifted_sequences = np.zeros_like(X_sequences)
                    for i, shift in enumerate(shift_amount):
                        if shift > 0:
                            shifted_sequences[i, shift:] = noisy_sequences[i, :-shift]
                            shifted_sequences[i, :shift] = noisy_sequences[i, 0]  # –ø–æ–≤—Ç–æ—Ä—é—î–º–æ –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç
                        elif shift < 0:
                            shifted_sequences[i, :shift] = noisy_sequences[i, -shift:]
                            shifted_sequences[i, shift:] = noisy_sequences[i, -1]  # –ø–æ–≤—Ç–æ—Ä—é—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –µ–ª–µ–º–µ–Ω—Ç
                        else:
                            shifted_sequences[i] = noisy_sequences[i]
                    
                    augmented_sequences.append(shifted_sequences)
                    augmented_targets.append(y_targets)  # —Ü—ñ–ª—ñ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è —Ç—ñ –∂
                
                # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ augmented –¥–∞–Ω—ñ
                X_augmented = np.concatenate(augmented_sequences, axis=0)
                y_augmented = np.concatenate(augmented_targets, axis=0)
                
                # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ
                indices = np.random.permutation(len(X_augmented))
                return X_augmented[indices], y_augmented[indices]
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–ª—è val (–±–µ–∑ augmentation)
            X_val_sequences = []
            for i in range(len(X_val_scaled) - look_back):
                X_val_sequences.append(X_val_scaled[i:i + look_back])
            X_val_sequences = np.array(X_val_sequences)
            
            # –¶—ñ–ª—å–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ - –∞–±—Å–æ–ª—é—Ç–Ω—ñ —Ü—ñ–Ω–∏ (close), –∞ –Ω–µ —Ä—ñ–∑–Ω–∏—Ü—ñ
            close_idx = feature_columns.index('close')
            y_train = X_train_scaled[look_back:, close_idx]  # –Ω–∞—Å—Ç—É–ø–Ω—ñ –∞–±—Å–æ–ª—é—Ç–Ω—ñ —Ü—ñ–Ω–∏
            y_val = X_val_scaled[look_back:, close_idx]

            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ augmentation —Ç—ñ–ª—å–∫–∏ –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            X_train_sequences, y_train = augment_training_data(X_train_sequences, y_train, augmentation_factor=3)
            logger.info(f"üîÑ Data augmentation: {len(X_train_sequences)} —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö —Å–µ–º–ø–ª—ñ–≤ (–±—É–ª–æ {len(X_train_scaled) - look_back})")

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–ø—ñ–≤–ø–∞–¥–∞—é—Ç—å —Ä–æ–∑–º—ñ—Ä–∏ –ø—ñ—Å–ª—è augmentation
            expected_train_len = len(X_train_sequences)
            expected_val_len = len(X_val_scaled) - look_back
            
            if len(y_train) != expected_train_len or len(y_val) != expected_val_len:
                logger.error(f"‚ùå –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å y –Ω–µ —Å–ø—ñ–≤–ø–∞–¥–∞—î: y_train={len(y_train)}, expected={expected_train_len}")
                return None
            
            X_sequences = X_train_sequences  # –î–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –∫–æ–¥–æ–º            X_sequences = X_train_sequences  # –î–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –∫–æ–¥–æ–º
            
            # –î–ª—è prediction –ø–æ—Ç—Ä—ñ–±–Ω—ñ –≤—Å—ñ –¥–∞–Ω—ñ —Ä–∞–∑–æ–º
            X_data = data[feature_columns].values
            X_all_scaled = scaler.transform(X_data)  # transform –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ train scaler
            
            # –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ù–û–†–ú–ê–õ–Ü–ó–û–í–ê–ù–ò–• –î–ê–ù–ò–• –í –ë–î
            try:
                # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
                normalized_data_df = data.reset_index().copy()  # reset_index —â–æ–± timestamp —Å—Ç–∞–≤ –∫–æ–ª–æ–Ω–∫–æ—é
                
                # –î–æ–¥–∞—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —è–∫ –Ω–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
                for i, feature in enumerate(feature_columns):
                    normalized_data_df[f'{feature}_normalized'] = X_all_scaled[:, i]
                
                await save_normalized_data_batch(db_manager, symbol, interval, normalized_data_df)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ: {e}")
            
            if len(X_sequences) == 0:
                logger.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ")
                return None
            
            # 4. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è/–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ

            model_path = f"models/optimized_{symbol}_{interval}.keras"

            retrain = force_retrain or not Path(model_path).exists()
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ input_shape —É –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
            metadata_path = model_path.replace('.keras', '_metadata.json')
            if not retrain and Path(metadata_path).exists():
                import json
                with open(metadata_path, 'r') as f:
                    meta = json.load(f)
                old_shape = tuple(meta.get('data_shape', [0, 0]))
                new_shape = X_sequences.shape
                if old_shape != new_shape:
                    logger.info("‚ö†Ô∏è Input shape –∑–º—ñ–Ω–∏–≤—Å—è, –ø–µ—Ä–µ—Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
                    retrain = True

            if retrain:
                logger.info("ü§ñ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ...")
                close_index = feature_columns.index('close')
                
                model_builder = OptimizedPricePredictionModel(
                    input_shape=(look_back, len(feature_columns)),
                    model_type=model_type,
                    scaler=scaler,
                    feature_index=close_index
                )
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–∂–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ X_train_sequences, X_val_sequences, y_train, y_val
                X_train = X_train_sequences.astype(np.float32)
                X_val = X_val_sequences.astype(np.float32)
                y_train = y_train.astype(np.float32)
                y_val = y_val.astype(np.float32)
                
                # –õ–æ–≥—É–≤–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
                logger.info(f"y_train: min={y_train.min():.4f}, max={y_train.max():.4f}, mean={y_train.mean():.4f}, std={y_train.std():.4f}")
                logger.info(f"y_val: min={y_val.min():.4f}, max={y_val.max():.4f}, mean={y_val.mean():.4f}, std={y_val.std():.4f}")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ NaN/Inf
                if np.isnan(y_train).any() or np.isnan(y_val).any():
                    logger.error("‚ùå y_train –∞–±–æ y_val –º—ñ—Å—Ç–∏—Ç—å NaN!")
                    return None
                if np.isinf(y_train).any() or np.isinf(y_val).any():
                    logger.error("‚ùå y_train –∞–±–æ y_val –º—ñ—Å—Ç–∏—Ç—å Inf!")
                    return None
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ callback –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –ë–î
                db_callback = DatabaseHistoryCallback(
                    db_engine=db_manager.sync_engine,
                    symbol_id=symbol_id,
                    interval_id=interval_id,
                    fold=1
                )
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ callback –¥–ª—è –≤–∏–≤–æ–¥—É –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
                denorm_callback = DenormalizedMetricsCallback(
                    scaler=scaler,
                    feature_index=feature_columns.index('close'),
                    X_val=X_val,
                    y_val=y_val
                )
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ optimized_config
                start_training_time = time.time()
                model, history = model_builder.train_model(
                    X_train, y_train, X_val, y_val,
                    model_save_path=model_path,
                    epochs=MODEL_CONFIG['epochs'],
                    batch_size=MODEL_CONFIG['batch_size'],
                    learning_rate=MODEL_CONFIG['learning_rate'],
                    db_callback=db_callback,
                    additional_callbacks=[denorm_callback]
                )
                training_time = time.time() - start_training_time
                
                # –ó–∞–ø–∏—Å—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª—ñ –≤ —Å–∏—Å—Ç–µ–º—É –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
                monitoring_system.record_model_metrics(
                    symbol=symbol,
                    interval=interval,
                    model_type=model_type,
                    training_time=training_time,
                    history=history
                )
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ scaler
                scaler_params = {}
                if hasattr(scaler, 'data_min_'):
                    # MinMaxScaler
                    scaler_params = {
                        'type': 'MinMaxScaler',
                        'min': scaler.data_min_.tolist(),
                        'max': scaler.data_max_.tolist()
                    }
                elif hasattr(scaler, 'center_'):
                    # RobustScaler –∞–±–æ StandardScaler
                    scaler_type = type(scaler).__name__
                    scaler_params = {
                        'type': scaler_type,
                        'center': scaler.center_.tolist(),
                        'scale': scaler.scale_.tolist()
                    }
                
                metadata = {
                    'symbol': symbol,
                    'interval': interval,
                    'features': feature_columns,
                    'scaler_params': scaler_params,
                    'trained_at': datetime.now().isoformat(),
                    'data_shape': X_sequences.shape,
                    'model_type': model_type
                }
                model_builder.save_model_with_metadata(model, model_path, metadata)
            else:
                logger.info("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó –º–æ–¥–µ–ª—ñ...")
                model_builder = OptimizedPricePredictionModel(
                    input_shape=(look_back, len(feature_columns)),
                    model_type=model_type,
                    scaler=scaler,
                    feature_index=feature_columns.index('close')
                )
                model = model_builder.load_model_with_metadata(model_path)
                
                # –Ø–∫—â–æ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –±–µ–∑ –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó, –ø–µ—Ä–µ–∫–æ–º–ø—ñ–ª—é—î–º–æ —ó—ó
                if not model.compiled:
                    logger.info("üîß –ú–æ–¥–µ–ª—å –Ω–µ —Å–∫–æ–º–ø—ñ–ª—å–æ–≤–∞–Ω–∞, –∫–æ–º–ø—ñ–ª—é—î–º–æ...")
                    model = model_builder.recompile_loaded_model(model)
            
            # 5. –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            logger.info("üîÆ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤...")
            
            # –ó–ë–Ü–† –°–í–Ü–ñ–ò–• –§–£–ù–î–ê–ú–ï–ù–¢–ê–õ–¨–ù–ò–• –î–ê–ù–ò–• –î–õ–Ø –ü–†–û–ì–ù–û–ó–£–í–ê–ù–ù–Ø
            logger.info(f"üì∞ –ó–±—ñ—Ä —Å–≤—ñ–∂–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É {symbol}...")
            try:
                # –ó–±–∏—Ä–∞—î–º–æ —Å–≤—ñ–∂—ñ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ (–æ—Å—Ç–∞–Ω–Ω—ñ 24 –≥–æ–¥–∏–Ω–∏)
                fresh_fundamental_list = await fundamental_integrator.collect_fundamental_data_for_period(
                    symbol, 
                    datetime.now() - timedelta(hours=24), 
                    datetime.now()
                )
                
                if fresh_fundamental_list:
                    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å–ø–∏—Å–æ–∫ –≤ DataFrame –¥–ª—è –æ–±—Ä–æ–±–∫–∏
                    fresh_fundamental_data = []
                    for feature in fresh_fundamental_list:
                        fresh_fundamental_data.append({
                            'timestamp': feature.timestamp,
                            'aggregate_sentiment': feature.aggregate_sentiment,
                            'news_sentiment_score': feature.news_sentiment_score,
                            'social_sentiment_score': feature.social_sentiment_score,
                            'active_addresses': feature.active_addresses,
                            'transaction_count': feature.transaction_count,
                            'whale_activity': feature.whale_activity
                        })
                    
                    latest_fundamental = pd.DataFrame(fresh_fundamental_data)
                    
                    if not latest_fundamental.empty:
                        # –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑ —á–∞—Å–æ–≤–æ—é —ñ–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—î—é
                        fundamental_features = ['aggregate_sentiment', 'news_sentiment_score', 'social_sentiment_score',
                                              'active_addresses', 'transaction_count', 'whale_activity']
                        
                        # –°—Ç–≤–æ—Ä—é—î–º–æ —á–∞—Å–æ–≤—É —Å—ñ—Ç–∫—É –¥–ª—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö look_back –ø–µ—Ä—ñ–æ–¥—ñ–≤
                        last_timestamps = data.index[-look_back:]
                        
                        # –Ü–Ω—Ç–µ—Ä–ø–æ–ª—é—î–º–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –ø–æ —á–∞—Å—É
                        for feature in fundamental_features:
                            if feature in data.columns and feature in latest_fundamental.columns:
                                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ resample —Ç–∞ interpolate –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥—É
                                feature_series = latest_fundamental.set_index('timestamp')[feature]
                                
                                # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Ä—ñ—é –∑ —Ç–∏–º–∏ –∂ timestamp —â–æ –π –æ—Å—Ç–∞–Ω–Ω—ñ –¥–∞–Ω—ñ
                                interpolated_feature = feature_series.reindex(last_timestamps, method='ffill').fillna(method='bfill').fillna(0.0)
                                
                                # –î–æ–¥–∞—î–º–æ –Ω–µ–≤–µ–ª–∏–∫–∏–π —à—É–º –¥–ª—è —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–æ—Å—Ç—ñ (–∞–ª–µ –º–µ–Ω—à–∏–π –Ω—ñ–∂ –≤ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—ñ)
                                noise_level = 0.001  # 0.1% —à—É–º
                                noise = np.random.normal(0, noise_level * interpolated_feature.std(), len(interpolated_feature))
                                interpolated_feature = interpolated_feature + noise
                                
                                # –û–Ω–æ–≤–ª—é—î–º–æ –¥–∞–Ω—ñ
                                data.loc[last_timestamps, feature] = interpolated_feature.values
                        
                        # –ü–µ—Ä–µ—Ä–∞—Ö–æ–≤—É—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ –∑ –æ–Ω–æ–≤–ª–µ–Ω–∏–º–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏
                        X_data_updated = data[feature_columns].values
                        X_all_scaled = scaler.transform(X_data_updated)
                        
                        logger.info(f"üîÑ –û–Ω–æ–≤–ª–µ–Ω–æ –¥–∞–Ω—ñ —ñ–Ω—Ç–µ—Ä–ø–æ–ª—å–æ–≤–∞–Ω–∏–º–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏ –∑ —á–∞—Å–æ–≤–æ—é —Å—ñ—Ç–∫–æ—é")
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ–º–∞—î —Å–≤—ñ–∂–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑—ñ–±—Ä–∞—Ç–∏ —Å–≤—ñ–∂—ñ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–±–æ—Ä—É —Å–≤—ñ–∂–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
                # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ –¥–∞–Ω–∏–º–∏
            
            # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            last_sequence = X_all_scaled[-look_back:].reshape(1, look_back, len(feature_columns))
            
            predictions = []
            current_sequence = last_sequence.copy()

            for step in range(steps):
                # –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥–±–∞—á–∞—î –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—É –∞–±—Å–æ–ª—é—Ç–Ω—É —Ü—ñ–Ω—É
                pred_scaled = model.predict(current_sequence, verbose=0)[0, 0]

                # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—É —Ü—ñ–Ω—É
                dummy = np.zeros((1, len(feature_columns)))
                dummy[0, close_idx] = pred_scaled
                predicted_price = scaler.inverse_transform(dummy)[0, close_idx]

                predictions.append(float(predicted_price))

                # –û–Ω–æ–≤–ª—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É
                new_row = current_sequence[0, -1, :].copy()
                new_row[close_idx] = pred_scaled  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—É –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—É —Ü—ñ–Ω—É
                current_sequence = np.roll(current_sequence, -1, axis=1)
                current_sequence[0, -1, :] = new_row
            
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ - –ø—Ä–æ—Å—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–∂–µ –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ü—ñ–Ω–∏
            predictions_denorm = predictions
            
            # 6. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é —Ü—ñ–Ω—É (inverse scaler)
            last_scaled = scaler.inverse_transform([X_all_scaled[-1]])[0]
            last_price_denorm = last_scaled[feature_columns.index('close')]
            
            results = {
                'symbol': symbol,
                'interval': interval,
                'timestamp': datetime.now().isoformat(),
                'last_price': last_price_denorm,
                'predictions': predictions_denorm,
                'steps': steps,
                'processing_time': (datetime.now() - start_time).total_seconds()
            }
            
            # –ö–µ—à—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            cache_key = f"predictions:{symbol}:{interval}:{steps}"
            cache_manager.set(cache_key, results, ttl=1800)
            
            # –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ü–†–û–ì–ù–û–ó–Ü–í –í –ë–î
            try:
                await save_predictions(db_manager, symbol, interval, predictions_denorm, last_price_denorm)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏: {e}")
            
            # –ó–∞–ø–∏—Å—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ –≤ —Å–∏—Å—Ç–µ–º—É –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
            for i, predicted_price in enumerate(predictions_denorm):
                monitoring_system.record_prediction_metrics(
                    symbol=symbol,
                    interval=interval,
                    predicted_price=predicted_price,
                    actual_price=last_price_denorm,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é –≤—ñ–¥–æ–º—É —Ü—ñ–Ω—É —è–∫ –±–∞–∑–æ–≤—É
                    confidence_score=None
                )
            
            logger.info(f"üîÆ –ü—Ä–æ–≥–Ω–æ–∑–∏: {[f'{p:.2f}' for p in predictions_denorm]}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {symbol}: {e}", exc_info=True)
            return None
    
    async def batch_process_symbols(self, symbols: list, model_type: str = "advanced_lstm", **kwargs):
        """–ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å–∏–º–≤–æ–ª—ñ–≤"""
        logger.info(f"üîÑ –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ {len(symbols)} —Å–∏–º–≤–æ–ª—ñ–≤ –∑ –º–æ–¥–µ–ª–ª—é {model_type}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–¥–∞—á—ñ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
        tasks = []
        for symbol in symbols:
            task = self.process_symbol_optimized(symbol, model_type=model_type, **kwargs)
            tasks.append(task)
        
        # –í–∏–∫–æ–Ω—É—î–º–æ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º
        semaphore = asyncio.Semaphore(3)  # –ú–∞–∫—Å–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∏ –æ–¥–Ω–æ—á–∞—Å–Ω–æ
        
        async def limited_process(task):
            async with semaphore:
                return await task
        
        limited_tasks = [limited_process(task) for task in tasks]
        results = await asyncio.gather(*limited_tasks, return_exceptions=True)
        
        # –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        successful = sum(1 for r in results if r is not None and not isinstance(r, Exception))
        failed = len(results) - successful
        
        logger.info(f"‚úÖ –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful} —É—Å–ø—ñ—à–Ω–æ, {failed} –∑ –ø–æ–º–∏–ª–∫–∞–º–∏")
        
        return results
    
    async def get_system_status(self):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É —Å–∏—Å—Ç–µ–º–∏"""
        return {
            'initialized': self.initialized,
            'gpu_available': self.gpu_available,
            'gpu_info': get_gpu_info() if self.gpu_available else None,
            'cache_stats': get_cache_info(),
            'worker_stats': ml_pipeline.worker_pool.get_stats() if ml_pipeline.worker_pool else None,
            'monitoring_status': monitoring_system.get_system_status(),
            'performance_summary': monitoring_system.get_performance_summary(),
            'timestamp': datetime.now().isoformat()
        }
    
    async def cleanup(self):
        """–û—á–∏—â–µ–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤"""
        logger.info("üßπ –û—á–∏—â–µ–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤...")
        await shutdown_async_system()
        logger.info("‚úÖ –û—á–∏—â–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º–∏
crypto_system = OptimizedCryptoMLSystem()

async def main():
    """–ì–æ–ª–æ–≤–Ω–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    parser = argparse.ArgumentParser(description="–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç")
    parser.add_argument("--symbol", type=str, default=SYMBOL, help="–¢–æ—Ä–≥–æ–≤–∞ –ø–∞—Ä–∞")
    parser.add_argument("--interval", type=str, default=INTERVAL, help="–Ü–Ω—Ç–µ—Ä–≤–∞–ª —á–∞—Å—É")
    parser.add_argument("--days_back", type=int, default=DAYS_BACK, help="–î–Ω—ñ–≤ —ñ—Å—Ç–æ—Ä—ñ—ó")
    parser.add_argument("--look_back", type=int, default=LOOK_BACK, help="–†–æ–∑–º—ñ—Ä –≤—ñ–∫–Ω–∞ –∑ optimized_config")
    parser.add_argument("--steps", type=int, default=STEPS, help="–ö—Ä–æ–∫—ñ–≤ –ø—Ä–æ–≥–Ω–æ–∑—É")
    parser.add_argument("--force_retrain", action="store_true", help="–ü—Ä–∏–º—É—Å–æ–≤–µ –ø–µ—Ä–µ—Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è")
    parser.add_argument("--use_cv", action="store_true", help="–í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ TimeSeriesSplit cross-validation")
    parser.add_argument("--batch", nargs="+", help="–ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å–∏–º–≤–æ–ª—ñ–≤")
    parser.add_argument("--symbols", nargs="+", default=[SYMBOL], help="–°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏")
    parser.add_argument("--model_type", type=str, default="advanced_lstm", choices=["advanced_lstm", "transformer", "cnn_lstm"], help="–¢–∏–ø –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--status", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º–∏")
    
    args = parser.parse_args()
    
    try:
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
        await crypto_system.initialize()

        # –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–∏ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
        monitoring_task = asyncio.create_task(monitoring_system.start_monitoring(interval_seconds=60))
        logger.info("üìä –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –∑–∞–ø—É—â–µ–Ω–∞")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑ Binance –¥–ª—è –≤—Å—ñ—Ö —Å–∏–º–≤–æ–ª—ñ–≤
        logger.info("‚è≥ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑ Binance...")
        for symbol in args.symbols:
            await save_ohlcv_to_db(db_manager, symbol, args.interval, days_back=args.days_back)
        logger.info(f"‚úÖ –î–∞–Ω—ñ –∑ Binance –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É historical_data –¥–ª—è {len(args.symbols)} —Å–∏–º–≤–æ–ª—ñ–≤")

        if args.status:
            # –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å
            status = await crypto_system.get_system_status()
            logger.info(f"üìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º–∏: {status}")
            return
        
        if args.batch:
            # –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞
            results = await crypto_system.batch_process_symbols(
                symbols=args.batch,
                interval=args.interval,
                days_back=args.days_back,
                look_back=args.look_back,
                steps=args.steps,
                force_retrain=args.force_retrain,
                use_cv=args.use_cv,
                model_type=args.model_type
            )
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {args.batch[i]}: {result}")
                elif result:
                    logger.info(f"‚úÖ {args.batch[i]}: {result['predictions']}")
        else:
            # –û–±—Ä–æ–±–∫–∞ –≤—Å—ñ—Ö —Å–∏–º–≤–æ–ª—ñ–≤ –∑ --symbols
            all_results = []
            for symbol in args.symbols:
                logger.info(f"üìä –û–±—Ä–æ–±–∫–∞ —Å–∏–º–≤–æ–ª—É: {symbol}")
                result = await crypto_system.process_symbol_optimized(
                    symbol=symbol,
                    interval=args.interval,
                    days_back=args.days_back,
                    look_back=args.look_back,
                    steps=args.steps,
                    force_retrain=args.force_retrain,
                    use_cv=args.use_cv,
                    model_type=args.model_type
                )
                
                if result:
                    all_results.append(result)
                    predictions = result['predictions']
                    if predictions:
                        last_price = result.get('last_price', 0)
                        first_pred = predictions[0]
                        if last_price > 0:
                            price_error_pct = ((first_pred - last_price) / last_price) * 100
                            error_sign = "+" if price_error_pct >= 0 else ""
                            logger.info(f"‚úÖ {symbol}: –ü—Ä–æ–≥–Ω–æ–∑ {first_pred:.2f} ({error_sign}{price_error_pct:.2f}%) –≤—ñ–¥ {last_price:.2f}")
                        else:
                            logger.info(f"‚úÖ {symbol}: –ü—Ä–æ–≥–Ω–æ–∑–∏: {[f'{p:.2f}' for p in predictions]}")
            
            # –ü—ñ–¥—Å—É–º–æ–∫ –¥–ª—è –≤—Å—ñ—Ö —Å–∏–º–≤–æ–ª—ñ–≤
            if all_results:
                logger.info("üéØ –ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è:")
                for result in all_results:
                    symbol = result['symbol']
                    last_price = result['last_price']
                    predictions = result['predictions']
                    if predictions:
                        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≤—ñ–¥—Å–æ—Ç–∫–æ–≤—É –ø–æ—Ö–∏–±–∫—É –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É
                        first_pred = predictions[0]
                        price_error_pct = ((first_pred - last_price) / last_price) * 100
                        error_sign = "+" if price_error_pct >= 0 else ""
                        logger.info(f"   {symbol}: –û—Å—Ç–∞–Ω–Ω—è —Ü—ñ–Ω–∞ {last_price:.2f}, –ü—Ä–æ–≥–Ω–æ–∑ {first_pred:.2f} ({error_sign}{price_error_pct:.2f}%)")
    
    except KeyboardInterrupt:
        logger.info("‚è∏Ô∏è –û—Ç—Ä–∏–º–∞–Ω–æ —Å–∏–≥–Ω–∞–ª –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}", exc_info=True)
    finally:
        # –ó—É–ø–∏–Ω–∫–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
        monitoring_system.stop_monitoring()
        if 'monitoring_task' in locals():
            monitoring_task.cancel()
            try:
                await monitoring_task
            except asyncio.CancelledError:
                pass
        logger.info("üìä –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –∑—É–ø–∏–Ω–µ–Ω–æ")
        
        await crypto_system.cleanup()

if __name__ == "__main__":
    # –Ü–º–ø–æ—Ä—Ç numpy –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ —Ñ—É–Ω–∫—Ü—ñ—ó
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    
    # –ó–∞–ø—É—Å–∫ –≥–æ–ª–æ–≤–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó
    asyncio.run(main())