#!/usr/bin/env python3
"""
üöÄ ENHANCED TRAINING - –ü–æ–∫—Ä–∞—â–µ–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è 60-70% accuracy
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ –∑ —É—Å—ñ–º–∞ –Ω–∞–π–∫—Ä–∞—â–∏–º–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º–∏ ML –¥–ª—è –∫—Ä–∏–ø—Ç–æ-–ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
"""

import os
import sys
import asyncio
import logging
import numpy as np
import pandas as pd
from typing import Tuple, List, Dict
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import joblib
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from gpu_config import configure_gpu
from optimized_model import OptimizedPricePredictionModel
from train_model_advanced import AdvancedModelTrainer
from intelligent_sys import UnifiedBinanceLoader

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

configure_gpu()

# ============================================================================
# üéØ –ü–û–ö–†–ê–©–ï–ù–ê –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –î–õ–Ø 60-70% ACCURACY
# ============================================================================

ENHANCED_CONFIG = {
    # === –ê–†–•–Ü–¢–ï–ö–¢–£–†–ê ===
    'model_type': 'advanced_lstm',
    'sequence_length': 120,  # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 60 –¥–æ 120 (5 –¥–Ω—ñ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)
    'batch_size': 32,        # ‚¨áÔ∏è –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    'epochs': 500,           # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 200 –¥–æ 500
    'learning_rate': 0.0001, # ‚¨áÔ∏è –ú–µ–Ω—à–∞ LR –¥–ª—è —Ç–æ—á–Ω—ñ—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    'early_stopping_patience': 50,  # ‚¨ÜÔ∏è –ë—ñ–ª—å—à–µ —Ç–µ—Ä–ø—ñ–Ω–Ω—è
    'reduce_lr_patience': 15,
    
    # === LSTM LAYERS ===
    'lstm_units_1': 512,     # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 320
    'lstm_units_2': 256,     # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 160
    'lstm_units_3': 128,     # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 80
    'lstm_units_4': 64,      # ‚ûï –î–æ–¥–∞—Ç–∫–æ–≤–∏–π —à–∞—Ä
    
    # === ATTENTION ===
    'attention_heads': 16,   # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 10
    'attention_key_dim': 128, # ‚¨ÜÔ∏è –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 80
    
    # === DENSE LAYERS ===
    'dense_units': [1024, 512, 256, 128, 64],  # ‚¨ÜÔ∏è –ì–ª–∏–±—à–∞ –º–µ—Ä–µ–∂–∞
    
    # === REGULARIZATION ===
    'dropout_rate': 0.35,    # ‚¨áÔ∏è –¢—Ä–æ—Ö–∏ –º–µ–Ω—à–µ –¥–ª—è —Å–∫–ª–∞–¥–Ω—ñ—à–æ—ó –º–æ–¥–µ–ª—ñ
    'l2_regularization': 0.005,  # ‚¨áÔ∏è –ú–µ–Ω—à–µ –¥–ª—è –∑–±—ñ–ª—å—à–µ–Ω–æ—ó –º–µ—Ä–µ–∂—ñ
    
    # === DATA ===
    'days_back': 730,        # ‚¨ÜÔ∏è 2 —Ä–æ–∫–∏ —ñ—Å—Ç–æ—Ä—ñ—ó –∑–∞–º—ñ—Å—Ç—å 1
    'validation_split': 0.15,
    'test_split': 0.10,
}

# ============================================================================
# üîß –†–û–ó–®–ò–†–ï–ù–ò–ô FEATURE ENGINEERING
# ============================================================================

ENHANCED_FEATURES = {
    # === –ë–ê–ó–û–í–Ü –¢–ï–•–ù–Ü–ß–ù–Ü –Ü–ù–î–ò–ö–ê–¢–û–†–ò ===
    'basic': [
        'open', 'high', 'low', 'close', 'volume',
        'returns', 'log_returns',
    ],
    
    # === –¢–†–ï–ù–î ===
    'trend': [
        'ema_9', 'ema_21', 'ema_50', 'ema_100', 'ema_200',
        'sma_9', 'sma_21', 'sma_50', 'sma_100', 'sma_200',
        'tema_9', 'tema_21',  # Triple EMA
        'dema_9', 'dema_21',  # Double EMA
        'kama_10', 'kama_30',  # Kaufman Adaptive MA
    ],
    
    # === MOMENTUM ===
    'momentum': [
        'rsi_7', 'rsi_14', 'rsi_21',
        'roc_5', 'roc_10', 'roc_20',
        'mom_5', 'mom_10', 'mom_20',
        'tsi',  # True Strength Index
        'uo',   # Ultimate Oscillator
        'ppo',  # Percentage Price Oscillator
        'williams_r_14',
        'cci_20',
        'stoch_k', 'stoch_d',
        'stochrsi_k', 'stochrsi_d',
    ],
    
    # === VOLATILITY ===
    'volatility': [
        'atr_7', 'atr_14', 'atr_21',
        'bb_upper', 'bb_middle', 'bb_lower', 'bb_width', 'bb_percent',
        'kc_upper', 'kc_middle', 'kc_lower',  # Keltner Channels
        'dc_upper', 'dc_middle', 'dc_lower',  # Donchian Channels
        'hvol_10', 'hvol_20', 'hvol_30',  # Historical Volatility
        'natr',  # Normalized ATR
        'true_range',
    ],
    
    # === VOLUME ===
    'volume': [
        'obv',  # On-Balance Volume
        'ad',   # Accumulation/Distribution
        'adosc',  # Chaikin A/D Oscillator
        'cmf',  # Chaikin Money Flow
        'mfi',  # Money Flow Index
        'eom',  # Ease of Movement
        'vwap',
        'volume_sma_5', 'volume_sma_10', 'volume_sma_20',
        'volume_ratio',
        'volume_momentum',
    ],
    
    # === MACD ===
    'macd': [
        'macd', 'macd_signal', 'macd_histogram',
        'macd_diff',
    ],
    
    # === ICHIMOKU ===
    'ichimoku': [
        'ichimoku_a', 'ichimoku_b',
        'ichimoku_base', 'ichimoku_conv',
    ],
    
    # === PRICE ACTION ===
    'price_action': [
        'body', 'upper_wick', 'lower_wick',
        'candle_size',
        'high_low_range',
        'close_position',  # –ü–æ–∑–∏—Ü—ñ—è close –≤ high-low range
    ],
    
    # === –°–¢–ê–¢–ò–°–¢–ò–ß–ù–Ü –§–Ü–ß–Ü ===
    'stats': [
        'close_std_5', 'close_std_10', 'close_std_20', 'close_std_30',
        'close_skew_20', 'close_kurt_20',
        'returns_std_20',
        'rolling_sharpe_20',
    ],
    
    # === –î–û–î–ê–¢–ö–û–í–Ü –†–û–ó–†–ê–•–û–í–ê–ù–Ü –§–Ü–ß–Ü ===
    'derived': [
        'price_momentum',
        'acceleration',
        'dist_from_ema_21', 'dist_from_ema_50', 'dist_from_ema_200',
        'trend_strength',
        'volatility_trend',
    ],
}


class EnhancedFeatureEngineer:
    """–†–æ–∑—à–∏—Ä–µ–Ω–∏–π feature engineering –∑ —É—Å—ñ–º–∞ –º–æ–∂–ª–∏–≤–∏–º–∏ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏"""
    
    @staticmethod
    def calculate_all_features(df: pd.DataFrame) -> pd.DataFrame:
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Å—ñ—Ö —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤"""
        logger.info("üîß –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö —Ñ—ñ—á–µ–π...")
        
        data = df.copy()
        
        # === –ë–ê–ó–û–í–Ü ===
        data['returns'] = data['close'].pct_change() * 100
        data['log_returns'] = np.log(data['close'] / data['close'].shift(1)) * 100
        
        # === –¢–†–ï–ù–î - EMA ===
        for period in [9, 21, 50, 100, 200]:
            data[f'ema_{period}'] = data['close'].ewm(span=period, adjust=False).mean()
        
        # === –¢–†–ï–ù–î - SMA ===
        for period in [9, 21, 50, 100, 200]:
            data[f'sma_{period}'] = data['close'].rolling(window=period).mean()
        
        # === MOMENTUM - RSI ===
        for period in [7, 14, 21]:
            delta = data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / (loss + 1e-10)
            data[f'rsi_{period}'] = 100 - (100 / (1 + rs))
        
        # === MOMENTUM - ROC ===
        for period in [5, 10, 20]:
            data[f'roc_{period}'] = ((data['close'] - data['close'].shift(period)) / 
                                     data['close'].shift(period)) * 100
        
        # === MOMENTUM - MOM ===
        for period in [5, 10, 20]:
            data[f'mom_{period}'] = data['close'].diff(period)
        
        # === MOMENTUM - Williams %R ===
        period = 14
        high_roll = data['high'].rolling(window=period).max()
        low_roll = data['low'].rolling(window=period).min()
        data['williams_r_14'] = -100 * ((high_roll - data['close']) / (high_roll - low_roll + 1e-10))
        
        # === MOMENTUM - CCI ===
        period = 20
        tp = (data['high'] + data['low'] + data['close']) / 3
        sma_tp = tp.rolling(window=period).mean()
        mad = tp.rolling(window=period).apply(lambda x: np.abs(x - x.mean()).mean())
        data['cci_20'] = (tp - sma_tp) / (0.015 * mad + 1e-10)
        
        # === MOMENTUM - Stochastic ===
        period = 14
        low_min = data['low'].rolling(window=period).min()
        high_max = data['high'].rolling(window=period).max()
        data['stoch_k'] = 100 * ((data['close'] - low_min) / (high_max - low_min + 1e-10))
        data['stoch_d'] = data['stoch_k'].rolling(window=3).mean()
        
        # === VOLATILITY - ATR ===
        for period in [7, 14, 21]:
            high_low = data['high'] - data['low']
            high_close = np.abs(data['high'] - data['close'].shift())
            low_close = np.abs(data['low'] - data['close'].shift())
            tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
            data[f'atr_{period}'] = tr.rolling(window=period).mean()
        
        # === VOLATILITY - Bollinger Bands ===
        period = 20
        sma = data['close'].rolling(window=period).mean()
        std = data['close'].rolling(window=period).std()
        data['bb_upper'] = sma + (2 * std)
        data['bb_middle'] = sma
        data['bb_lower'] = sma - (2 * std)
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        data['bb_percent'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'] + 1e-10)
        
        # === VOLATILITY - Historical Volatility ===
        for period in [10, 20, 30]:
            data[f'hvol_{period}'] = data['returns'].rolling(window=period).std() * np.sqrt(24)
        
        # === VOLUME ===
        data['obv'] = (np.sign(data['close'].diff()) * data['volume']).fillna(0).cumsum()
        data['vwap'] = (data['close'] * data['volume']).cumsum() / data['volume'].cumsum()
        
        for period in [5, 10, 20]:
            data[f'volume_sma_{period}'] = data['volume'].rolling(window=period).mean()
        
        data['volume_ratio'] = data['volume'] / (data['volume_sma_20'] + 1e-10)
        data['volume_momentum'] = data['volume'].pct_change(5) * 100
        
        # === VOLUME - MFI ===
        period = 14
        tp = (data['high'] + data['low'] + data['close']) / 3
        mf = tp * data['volume']
        mf_pos = mf.where(tp > tp.shift(1), 0).rolling(window=period).sum()
        mf_neg = mf.where(tp < tp.shift(1), 0).rolling(window=period).sum()
        data['mfi'] = 100 - (100 / (1 + mf_pos / (mf_neg + 1e-10)))
        
        # === MACD ===
        ema_12 = data['close'].ewm(span=12, adjust=False).mean()
        ema_26 = data['close'].ewm(span=26, adjust=False).mean()
        data['macd'] = ema_12 - ema_26
        data['macd_signal'] = data['macd'].ewm(span=9, adjust=False).mean()
        data['macd_histogram'] = data['macd'] - data['macd_signal']
        data['macd_diff'] = data['macd'].diff()
        
        # === PRICE ACTION ===
        data['body'] = np.abs(data['close'] - data['open'])
        data['upper_wick'] = data['high'] - np.maximum(data['close'], data['open'])
        data['lower_wick'] = np.minimum(data['close'], data['open']) - data['low']
        data['candle_size'] = data['high'] - data['low']
        data['high_low_range'] = data['high'] - data['low']
        data['close_position'] = (data['close'] - data['low']) / (data['high_low_range'] + 1e-10)
        
        # === –°–¢–ê–¢–ò–°–¢–ò–ß–ù–Ü ===
        for period in [5, 10, 20, 30]:
            data[f'close_std_{period}'] = data['close'].rolling(window=period).std()
        
        data['close_skew_20'] = data['close'].rolling(window=20).skew()
        data['close_kurt_20'] = data['close'].rolling(window=20).kurt()
        data['returns_std_20'] = data['returns'].rolling(window=20).std()
        
        # === –î–û–î–ê–¢–ö–û–í–Ü ===
        data['price_momentum'] = data['close'].diff(5)
        data['acceleration'] = data['price_momentum'].diff()
        
        for period in [21, 50, 200]:
            data[f'dist_from_ema_{period}'] = (data['close'] - data[f'ema_{period}']) / data[f'ema_{period}'] * 100
        
        logger.info(f"‚úÖ –†–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–æ {len(data.columns)} —Ñ—ñ—á–µ–π")
        
        return data


class EnhancedModelTrainer:
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π trainer –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è 60-70% accuracy"""
    
    def __init__(self, symbol: str, interval: str = '1h'):
        self.symbol = symbol
        self.interval = interval
        self.config = ENHANCED_CONFIG.copy()
        self.scaler = RobustScaler()
        self.model = None
        self.history = None
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üöÄ ENHANCED TRAINING - {symbol}")
        logger.info(f"{'='*80}")
        logger.info(f"üìä –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:")
        logger.info(f"   Model: {self.config['model_type']}")
        logger.info(f"   Sequence: {self.config['sequence_length']} (5 –¥–Ω—ñ–≤)")
        logger.info(f"   Batch: {self.config['batch_size']}")
        logger.info(f"   Epochs: {self.config['epochs']}")
        logger.info(f"   Learning Rate: {self.config['learning_rate']}")
        logger.info(f"   Data: {self.config['days_back']} –¥–Ω—ñ–≤ (2 —Ä–æ–∫–∏)")
        logger.info(f"   LSTM: {self.config['lstm_units_1']}-{self.config['lstm_units_2']}-{self.config['lstm_units_3']}-{self.config['lstm_units_4']}")
        logger.info(f"   Attention: {self.config['attention_heads']} heads, {self.config['attention_key_dim']} key_dim")
        logger.info(f"   Dense: {self.config['dense_units']}")
    
    async def load_data(self) -> pd.DataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö"""
        days = self.config['days_back']
        logger.info(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {days} –¥–Ω—ñ–≤ –¥–∞–Ω–∏—Ö...")
        
        from dotenv import load_dotenv
        load_dotenv()
        
        loader = UnifiedBinanceLoader(
            api_key=os.getenv('FUTURES_API_KEY'),
            api_secret=os.getenv('FUTURES_API_SECRET'),
            testnet=False
        )
        
        try:
            data = await loader.get_historical_data(
                symbol=self.symbol,
                interval=self.interval,
                days_back=days
            )
            
            if data is None or len(data) < 500:
                logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö")
                return None
            
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
            return data
            
        finally:
            await loader.close()
    
    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö —Ñ—ñ—á–µ–π"""
        logger.info(f"\nüîß Feature Engineering...")
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Å—ñ—Ö —Ñ—ñ—á–µ–π
        df = EnhancedFeatureEngineer.calculate_all_features(data)
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –∑ –∑–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç—å–º–∞ NaN
        threshold = 0.3  # –º–∞–∫—Å–∏–º—É–º 30% NaN
        for col in df.columns:
            nan_ratio = df[col].isna().sum() / len(df)
            if nan_ratio > threshold:
                logger.warning(f"‚ö†Ô∏è –í–∏–¥–∞–ª—è—î–º–æ {col}: {nan_ratio*100:.1f}% NaN")
                df = df.drop(columns=[col])
        
        # –ó–∞–ø–æ–≤–Ω—é—î–º–æ –∑–∞–ª–∏—à–∫–æ–≤—ñ NaN
        initial_len = len(df)
        df = df.fillna(method='ffill').fillna(method='bfill')
        df = df.dropna()
        dropped = initial_len - len(df)
        
        logger.info(f"‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤ –∑ {len(df.columns)} —Ñ—ñ—á–∞–º–∏")
        logger.info(f"   –í–∏–¥–∞–ª–µ–Ω–æ {dropped} –∑–∞–ø–∏—Å—ñ–≤ –∑ NaN")
        
        return df
    
    def create_sequences(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –∑ target = % –∑–º—ñ–Ω–∞ —Ü—ñ–Ω–∏"""
        logger.info(f"\nüîÑ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π...")
        
        sequence_length = self.config['sequence_length']
        
        # –í—Å—ñ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä—ñ–º —Ç–∏—Ö, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–ª—è target
        feature_cols = [col for col in data.columns if col not in ['close', 'target']]
        
        # Target: –≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ –∑–º—ñ–Ω–∞ —Ü—ñ–Ω–∏ (–Ω–∞—Å—Ç—É–ø–Ω–∞ –≥–æ–¥–∏–Ω–∞)
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ close –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É target
        close_prices = data['close'].values
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—ñ—á–µ–π
        scaled_data = self.scaler.fit_transform(data[feature_cols].values)
        
        X, y = [], []
        
        for i in range(len(scaled_data) - sequence_length):
            # –ü–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å —Ñ—ñ—á–µ–π
            X.append(scaled_data[i:i + sequence_length])
            
            # Target: % –∑–º—ñ–Ω–∞ —Ü—ñ–Ω–∏ –≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ—ó –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó
            current_price = close_prices[i + sequence_length - 1]
            next_price = close_prices[i + sequence_length]
            pct_change = ((next_price - current_price) / current_price) * 100
            
            y.append(pct_change)
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        
        logger.info(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ:")
        logger.info(f"   X shape: {X.shape}")
        logger.info(f"   y shape: {y.shape}")
        logger.info(f"   Features: {len(feature_cols)}")
        logger.info(f"   Target: –≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ –∑–º—ñ–Ω–∞ —Ü—ñ–Ω–∏")
        
        return X, y
    
    def train(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        logger.info(f"\nüéì –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/val/test
        val_size = int(len(X) * self.config['validation_split'])
        test_size = int(len(X) * self.config['test_split'])
        train_size = len(X) - val_size - test_size
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]
        
        logger.info(f"üìä –†–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö:")
        logger.info(f"   Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
        logger.info(f"   Val:   {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)")
        logger.info(f"   Test:  {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        n_features = X.shape[2]
        
        model = OptimizedPricePredictionModel(
            n_features=n_features,
            **self.config
        )
        
        self.model = model.build_model()
        
        # Callbacks
        model_dir = f'models/enhanced_{self.symbol.replace("USDT", "")}'
        os.makedirs(model_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        checkpoint_path = f'{model_dir}/model_{timestamp}.keras'
        
        callbacks_list = [
            tf.keras.callbacks.ModelCheckpoint(
                checkpoint_path,
                monitor='val_directional_accuracy',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            tf.keras.callbacks.EarlyStopping(
                monitor='val_directional_accuracy',
                patience=self.config['early_stopping_patience'],
                restore_best_weights=True,
                mode='max',
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=self.config['reduce_lr_patience'],
                min_lr=1e-7,
                verbose=1
            ),
            tf.keras.callbacks.CSVLogger(f'{model_dir}/training_{timestamp}.csv'),
            tf.keras.callbacks.TensorBoard(
                log_dir=f'logs/tensorboard_{timestamp}',
                histogram_freq=1
            ),
        ]
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        logger.info(f"\nüèãÔ∏è –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=self.config['epochs'],
            batch_size=self.config['batch_size'],
            callbacks=callbacks_list,
            verbose=1
        )
        
        self.history = history.history
        
        # –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ
        logger.info(f"\nüìà –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ...")
        test_results = self.model.evaluate(X_test, y_test, verbose=0)
        test_metrics = dict(zip(self.model.metrics_names, test_results))
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è scaler
        scaler_path = f'{model_dir}/scaler_{timestamp}.pkl'
        joblib.dump(self.scaler, scaler_path)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
        best_val_loss = min(history.history['val_loss'])
        best_val_dir_acc = max(history.history.get('val_directional_accuracy', [0]))
        
        logger.info(f"\n{'='*80}")
        logger.info(f"‚úÖ –¢–†–ï–ù–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info(f"{'='*80}")
        logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")
        logger.info(f"   Best val_loss: {best_val_loss:.6f}")
        logger.info(f"   Best val_directional_accuracy: {best_val_dir_acc:.4f} ({best_val_dir_acc*100:.2f}%)")
        logger.info(f"\nüß™ –¢–ï–°–¢–û–í–ò–ô –ù–ê–ë–Ü–†:")
        for metric, value in test_metrics.items():
            if 'accuracy' in metric:
                logger.info(f"   {metric}: {value:.4f} ({value*100:.2f}%)")
            else:
                logger.info(f"   {metric}: {value:.6f}")
        logger.info(f"\nüíæ –ó–ë–ï–†–ï–ñ–ï–ù–û:")
        logger.info(f"   Model: {checkpoint_path}")
        logger.info(f"   Scaler: {scaler_path}")
        
        return {
            'symbol': self.symbol,
            'val_loss': best_val_loss,
            'val_dir_acc': best_val_dir_acc,
            'test_metrics': test_metrics,
            'model_path': checkpoint_path,
            'scaler_path': scaler_path,
        }


async def train_enhanced_btc():
    """–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è enhanced –º–æ–¥–µ–ª—ñ –¥–ª—è BTC"""
    try:
        trainer = EnhancedModelTrainer('BTCUSDT')
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
        data = await trainer.load_data()
        if data is None:
            return None
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ—á–µ–π
        df = trainer.prepare_features(data)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
        X, y = trainer.create_sequences(df)
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        result = trainer.train(X, y)
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ü–û–ú–ò–õ–ö–ê: {e}", exc_info=True)
        return None


if __name__ == "__main__":
    logger.info("\n" + "="*80)
    logger.info("üöÄ ENHANCED TRAINING FOR BTC - –í–µ—Ä—Å—ñ—è –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è 60-70% accuracy")
    logger.info("="*80 + "\n")
    
    result = asyncio.run(train_enhanced_btc())
    
    if result:
        logger.info("\nüéâ –£–°–ü–Ü–•!")
        sys.exit(0)
    else:
        logger.error("\n‚ùå –ù–ï–í–î–ê–ß–ê")
        sys.exit(1)
