#!/usr/bin/env python3
"""
üîÑ –ü–†–û–î–û–í–ñ–ï–ù–ù–Ø –¢–†–ï–ù–£–í–ê–ù–ù–Ø - Resume training from checkpoint
"""
import os
import sys
import asyncio
import logging
import numpy as np
import tensorflow as tf
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from train_classification import ClassificationTrainer, CLASSIFICATION_CONFIG
from gpu_config import configure_gpu

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –í–∏–º–∏–∫–∞—î–º–æ XLA –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ–≤ –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ batch_size
configure_gpu(use_xla=False)


async def resume_training():
    """–ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ checkpoint"""
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å—Ç–∞–Ω–Ω—é –º–æ–¥–µ–ª—å
    model_dir = 'models/classification_BTC'
    
    # –ó–Ω–∞–π—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π checkpoint
    checkpoints = [f for f in os.listdir(model_dir) if f.startswith('model_') and f.endswith('.keras')]
    if not checkpoints:
        logger.error("‚ùå Checkpoints –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        return None
    
    latest_checkpoint = max(checkpoints, key=lambda f: os.path.getctime(os.path.join(model_dir, f)))
    checkpoint_path = os.path.join(model_dir, latest_checkpoint)
    
    # –ó–Ω–∞–π—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π CSV —Ñ–∞–π–ª
    timestamp = latest_checkpoint.replace('model_', '').replace('.keras', '')
    csv_path = os.path.join(model_dir, f'training_{timestamp}.csv')
    
    logger.info("="*80)
    logger.info("üîÑ –ü–†–û–î–û–í–ñ–ï–ù–ù–Ø –¢–†–ï–ù–£–í–ê–ù–ù–Ø")
    logger.info("="*80)
    logger.info(f"\nüìÅ Checkpoint: {checkpoint_path}")
    logger.info(f"üìä History: {csv_path}")
    
    # –ü—Ä–æ—á–∏—Ç–∞—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å epochs –∑ CSV
    import pandas as pd
    history_df = pd.read_csv(csv_path)
    completed_epochs = len(history_df)
    last_val_acc = history_df['val_accuracy'].iloc[-1]
    best_val_acc = history_df['val_accuracy'].max()
    best_epoch = history_df['val_accuracy'].idxmax() + 1
    
    logger.info(f"\nüìà –ü—Ä–æ–≥—Ä–µ—Å:")
    logger.info(f"   –ó–∞–≤–µ—Ä—à–µ–Ω–æ epochs: {completed_epochs}")
    logger.info(f"   –û—Å—Ç–∞–Ω–Ω—è val_accuracy: {last_val_acc:.4f} ({last_val_acc*100:.2f}%)")
    logger.info(f"   –ù–∞–π–∫—Ä–∞—â–∞ val_accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) –Ω–∞ epoch {best_epoch}")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ trainer
    trainer = ClassificationTrainer('BTCUSDT')
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    logger.info(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
    data = await trainer.load_data()
    if data is None:
        return None
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ñ—ñ—á–µ–π
    logger.info(f"üîß –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ñ—ñ—á–µ–π...")
    df = trainer.calculate_features(data)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º—ñ—Ç–æ–∫
    logger.info(f"üè∑Ô∏è –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º—ñ—Ç–æ–∫...")
    df, labels = trainer.create_labels(df)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
    logger.info(f"üîÑ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π...")
    X, y = trainer.create_sequences(df, labels)
    
    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö (—Ç–æ–π —Å–∞–º–∏–π —Å–ø–æ—Å—ñ–±)
    val_size = int(len(X) * trainer.config['validation_split'])
    test_size = int(len(X) * trainer.config['test_split'])
    train_size = len(X) - val_size - test_size
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size + val_size]
    y_val = y[train_size:train_size + val_size]
    X_test = X[train_size + val_size:]
    y_test = y[train_size + val_size:]
    
    logger.info(f"\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö:")
    logger.info(f"   Train: {len(X_train)}")
    logger.info(f"   Val:   {len(X_val)}")
    logger.info(f"   Test:  {len(X_test)}")
    
    # Class weights
    if trainer.config.get('use_class_weights', True):
        from sklearn.utils.class_weight import compute_class_weight
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(y_train),
            y=y_train
        )
        class_weights = np.clip(class_weights, 0.5, 3.0)
        class_weight_dict = dict(enumerate(class_weights))
    else:
        class_weight_dict = None
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    logger.info(f"\nüîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ checkpoint...")
    model = tf.keras.models.load_model(checkpoint_path)
    trainer.model = model
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—ñ callbacks
    new_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    new_checkpoint_path = f'{model_dir}/model_resumed_{new_timestamp}.keras'
    new_csv_path = f'{model_dir}/training_resumed_{new_timestamp}.csv'
    
    callbacks_list = [
        tf.keras.callbacks.ModelCheckpoint(
            new_checkpoint_path,
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=trainer.config['early_stopping_patience'],
            restore_best_weights=True,
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=trainer.config['reduce_lr_patience'],
            min_lr=1e-7,
            verbose=1
        ),
        tf.keras.callbacks.CSVLogger(new_csv_path),
        tf.keras.callbacks.TensorBoard(
            log_dir=f'logs/tensorboard_resumed_{new_timestamp}',
            histogram_freq=0
        ),
    ]
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ª–∏—à–∫—É epochs
    remaining_epochs = trainer.config['epochs'] - completed_epochs
    
    logger.info(f"\nüèãÔ∏è –ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
    logger.info(f"   –ó–∞–ª–∏—à–∏–ª–æ—Å—å epochs: {remaining_epochs}")
    logger.info(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∏–π epoch: {completed_epochs + 1}")
    logger.info(f"   –ö—ñ–Ω—Ü–µ–≤–∏–π epoch: {trainer.config['epochs']}")
    logger.info(f"")
    
    # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        initial_epoch=completed_epochs,  # ‚≠ê –ö–õ–Æ–ß–û–í–ò–ô –ü–ê–†–ê–ú–ï–¢–†
        epochs=trainer.config['epochs'],
        batch_size=trainer.config['batch_size'],
        class_weight=class_weight_dict,
        callbacks=callbacks_list,
        verbose=1
    )
    
    # –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ
    logger.info(f"\nüìà –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ...")
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
    y_pred = model.predict(X_test, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    # Confusion matrix
    from sklearn.metrics import confusion_matrix, classification_report
    cm = confusion_matrix(y_test, y_pred_classes)
    
    logger.info(f"\nüìä Confusion Matrix:")
    logger.info(f"           Predicted")
    logger.info(f"           DOWN  NEUTRAL  UP")
    logger.info(f"Actual DOWN    {cm[0][0]:4d}    {cm[0][1]:4d}  {cm[0][2]:4d}")
    logger.info(f"    NEUTRAL    {cm[1][0]:4d}    {cm[1][1]:4d}  {cm[1][2]:4d}")
    logger.info(f"         UP    {cm[2][0]:4d}    {cm[2][1]:4d}  {cm[2][2]:4d}")
    
    logger.info(f"\nüìã Classification Report:")
    logger.info(f"\n{classification_report(y_test, y_pred_classes, target_names=['DOWN', 'NEUTRAL', 'UP'])}")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
    best_val_acc = max(history.history['val_accuracy'])
    
    logger.info(f"\n{'='*80}")
    logger.info(f"‚úÖ –¢–†–ï–ù–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
    logger.info(f"{'='*80}")
    logger.info(f"\nüìä –§–Ü–ù–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")
    logger.info(f"   Best val_accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
    logger.info(f"   Test accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    logger.info(f"   Test loss: {test_loss:.6f}")
    logger.info(f"\nüíæ –ó–ë–ï–†–ï–ñ–ï–ù–û:")
    logger.info(f"   Model: {new_checkpoint_path}")
    logger.info(f"   History: {new_csv_path}")
    
    return {
        'symbol': 'BTCUSDT',
        'val_accuracy': best_val_acc,
        'test_accuracy': test_accuracy,
        'test_loss': test_loss,
        'confusion_matrix': cm.tolist(),
        'model_path': new_checkpoint_path,
        'resumed_from': checkpoint_path,
        'completed_epochs': completed_epochs,
    }


if __name__ == "__main__":
    logger.info("\n" + "="*80)
    logger.info("üîÑ RESUME TRAINING - –ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ checkpoint")
    logger.info("="*80 + "\n")
    
    result = asyncio.run(resume_training())
    
    if result:
        logger.info("\nüéâ –£–°–ü–Ü–•!")
        sys.exit(0)
    else:
        logger.error("\n‚ùå –ù–ï–í–î–ê–ß–ê")
        sys.exit(1)
