"""
Base Model Trainer - –ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –≤—Å—ñ—Ö trainers

–ù–∞–¥–∞—î —Å–ø—ñ–ª—å–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª:
- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö (load_data)
- –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features (prepare_features)
- –°—Ç–≤–æ—Ä–µ–Ω–Ω—è sequences (create_sequences)
- Train/test split
- Callbacks —Ç–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è
"""

import logging
import asyncio
from abc import ABC, abstractmethod
from typing import Optional, Tuple, Dict, List
import numpy as np
import pandas as pd
import tensorflow as tf
from pathlib import Path

from .data_loader import DataLoader
from .utils import create_sequences, normalize_data, temporal_split, calculate_class_weights

logger = logging.getLogger(__name__)


class BaseModelTrainer(ABC):
    """
    –ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
    
    –ü—ñ–¥–∫–ª–∞—Å–∏ –º–∞—é—Ç—å —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏:
    - prepare_features(): —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∞ –æ–±—Ä–æ–±–∫–∞ features
    - build_model(): –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ
    - train(): –ø—Ä–æ—Ü–µ—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    """
    
    def __init__(
        self,
        symbol: str = 'BTCUSDT',
        interval: str = '1h',
        sequence_length: int = 60,
        config: Optional[Dict] = None
    ):
        """
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∏–π —Å–∏–º–≤–æ–ª
            interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
            sequence_length: –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            config: –î–æ–¥–∞—Ç–∫–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        """
        self.symbol = symbol
        self.interval = interval
        self.sequence_length = sequence_length
        self.config = config or {}
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        self.data_loader = None
        self.scaler = None
        self.model = None
        
        # –î–∞–Ω—ñ
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        
        logger.info(f"‚úÖ {self.__class__.__name__} —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ: {symbol} {interval}")
    
    async def load_data(self, days: int = 365, force_reload: bool = False) -> pd.DataFrame:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Binance
        
        Args:
            days: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–Ω—ñ–≤ —ñ—Å—Ç–æ—Ä—ñ—ó
            force_reload: –ü—Ä–∏–º—É—Å–æ–≤–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        
        Returns:
            DataFrame –∑ OHLCV –¥–∞–Ω–∏–º–∏
        """
        logger.info(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {self.symbol} {self.interval}, {days} –¥–Ω—ñ–≤")
        
        if self.data_loader is None:
            self.data_loader = DataLoader(
                cache_dir=self.config.get('cache_dir'),
                use_cache=self.config.get('use_cache', False)
            )
        
        df = await self.data_loader.load(
            symbol=self.symbol,
            interval=self.interval,
            days=days,
            force_reload=force_reload
        )
        
        logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
        return df
    
    @abstractmethod
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features (–º–∞—î –±—É—Ç–∏ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –≤ –ø—ñ–¥–∫–ª–∞—Å–∞—Ö)
        
        Args:
            df: OHLCV DataFrame
        
        Returns:
            DataFrame –∑ features
        """
        pass
    
    def create_target(
        self,
        df: pd.DataFrame,
        target_column: str = 'close',
        prediction_horizon: int = 1
    ) -> pd.Series:
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è target variable
        
        Args:
            df: DataFrame –∑ –¥–∞–Ω–∏–º–∏
            target_column: –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É
            prediction_horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É (–∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–µ—Ä—ñ–æ–¥—ñ–≤ –≤–ø–µ—Ä–µ–¥)
        
        Returns:
            Series –∑ target values
        """
        # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - returns
        target = df[target_column].shift(-prediction_horizon) / df[target_column] - 1
        return target
    
    def prepare_sequences(
        self,
        X: np.ndarray,
        y: np.ndarray,
        normalize: bool = True
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è sequences —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        
        Args:
            X: Features array
            y: Target array
            normalize: –ß–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ features
        
        Returns:
            X_sequences, y_sequences
        """
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        if normalize:
            X, self.scaler = normalize_data(
                X,
                scaler_type=self.config.get('scaler_type', 'robust'),
                fit=True
            )
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è sequences
        X_seq, y_seq = create_sequences(
            X, y,
            sequence_length=self.sequence_length,
            step=self.config.get('sequence_step', 1)
        )
        
        logger.info(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ sequences: X{X_seq.shape}, y{y_seq.shape}")
        return X_seq, y_seq
    
    def split_data(
        self,
        X: np.ndarray,
        y: np.ndarray,
        train_ratio: float = 0.7,
        val_ratio: float = 0.15
    ):
        """
        –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/val/test sets
        
        Args:
            X: Features
            y: Targets
            train_ratio: –†–æ–∑–º—ñ—Ä train set
            val_ratio: –†–æ–∑–º—ñ—Ä validation set
        """
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —ñ–Ω–¥–µ–∫—Å—ñ–≤
        n = len(X)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        # Temporal split (–±–µ–∑ shuffle –¥–ª—è time-series)
        self.X_train = X[:train_end]
        self.X_val = X[train_end:val_end]
        self.X_test = X[val_end:]
        
        self.y_train = y[:train_end]
        self.y_val = y[train_end:val_end]
        self.y_test = y[val_end:]
        
        logger.info(f"‚úÖ –î–∞–Ω—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω–æ:")
        logger.info(f"   Train: {len(self.X_train)} samples")
        logger.info(f"   Val:   {len(self.X_val)} samples")
        logger.info(f"   Test:  {len(self.X_test)} samples")
        
        return self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test
    
    @abstractmethod
    def build_model(self) -> tf.keras.Model:
        """
        –ü–æ–±—É–¥–æ–≤–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ (–º–∞—î –±—É—Ç–∏ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –≤ –ø—ñ–¥–∫–ª–∞—Å–∞—Ö)
        
        Returns:
            Compiled Keras model
        """
        pass
    
    def get_callbacks(self, model_dir: Path) -> List[tf.keras.callbacks.Callback]:
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è callbacks –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        
        Args:
            model_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        
        Returns:
            List of callbacks
        """
        callbacks = []
        
        # Model checkpoint
        checkpoint_path = model_dir / f'model_{self.symbol}_{self.interval}.keras'
        callbacks.append(
            tf.keras.callbacks.ModelCheckpoint(
                checkpoint_path,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        )
        
        # Early stopping
        if self.config.get('early_stopping', True):
            callbacks.append(
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=self.config.get('early_stopping_patience', 20),
                    restore_best_weights=True,
                    verbose=1
                )
            )
        
        # Reduce LR on plateau
        if self.config.get('reduce_lr', True):
            callbacks.append(
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=self.config.get('reduce_lr_patience', 10),
                    min_lr=1e-7,
                    verbose=1
                )
            )
        
        # CSV Logger
        csv_path = model_dir / f'training_{self.symbol}_{self.interval}.csv'
        callbacks.append(
            tf.keras.callbacks.CSVLogger(csv_path)
        )
        
        return callbacks
    
    @abstractmethod
    async def train(self, **kwargs):
        """
        –ü—Ä–æ—Ü–µ—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–º–∞—î –±—É—Ç–∏ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –≤ –ø—ñ–¥–∫–ª–∞—Å–∞—Ö)
        """
        pass
    
    def evaluate(self) -> Dict:
        """
        –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ test set
        
        Returns:
            Dict –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞")
        
        if self.X_test is None or self.y_test is None:
            raise ValueError("Test set –Ω–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ")
        
        logger.info("\nüìä –û—Ü—ñ–Ω–∫–∞ –Ω–∞ test set...")
        results = self.model.evaluate(self.X_test, self.y_test, verbose=0)
        
        metrics = {}
        if isinstance(results, list):
            metrics['test_loss'] = results[0]
            for i, metric_name in enumerate(self.model.metrics_names[1:], 1):
                metrics[f'test_{metric_name}'] = results[i]
        else:
            metrics['test_loss'] = results
        
        for key, value in metrics.items():
            logger.info(f"   {key}: {value:.4f}")
        
        return metrics
    
    def save_model(self, path: str):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ —ñ—Å–Ω—É—î")
        
        self.model.save(path)
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {path}")
    
    def load_model(self, path: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        self.model = tf.keras.models.load_model(path)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {path}")
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å—ñ–≤"""
        if self.data_loader:
            await self.data_loader.close()
        logger.info("üîí Cleanup –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
