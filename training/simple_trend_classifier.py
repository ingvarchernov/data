#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞ –º–æ–¥–µ–ª—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç—Ä–µ–Ω–¥—É –∑ accuracy 70%+
–°—Ç—Ä–∞—Ç–µ–≥—ñ—è: Daily timeframe + trend indicators + Random Forest
"""

import os
import sys
import asyncio
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from typing import Tuple, Dict, List
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import joblib

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from binance.client import Client
from training.rust_features import RustFeatureEngineer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleTrendClassifier:
    """
    –ü—Ä–æ—Å—Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç—Ä–µ–Ω–¥—É –∑ –≤–∏—Å–æ–∫–æ—é —Ç–æ—á–Ω—ñ—Å—Ç—é
    
    –ü—ñ–¥—Ö—ñ–¥:
    - Daily timeframe (–º–µ–Ω—à–µ —à—É–º—É)
    - Trend labels: Strong Up (2), Weak Up (1), Weak Down (-1), Strong Down (-2)
    - Random Forest (–Ω–µ overfitting —è–∫ GRU)
    - Top 20 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö features
    """
    
    def __init__(self, symbol: str, timeframe: str = '1d'):
        self.symbol = symbol
        self.timeframe = timeframe
        self.client = Client()  # Public API, no keys needed
        self.feature_engineer = RustFeatureEngineer()
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names = []
        
        # Model configs (–º–µ–Ω—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ overfitting)
        self.n_estimators = 50  # –ú–µ–Ω—à–µ –¥–µ—Ä–µ–≤
        self.max_depth = 5      # –ú–µ–Ω—à–∞ –≥–ª–∏–±–∏–Ω–∞
        self.min_samples_split = 50  # –ë—ñ–ª—å—à–µ samples –Ω–∞ split
        
    def _create_trend_labels(self, df: pd.DataFrame, lookback: int = 3) -> pd.Series:
        """
        –°—Ç–≤–æ—Ä—é—î–º–æ BINARY trend labels (–ø—Ä–æ—Å—Ç—ñ—à–µ = –∫—Ä–∞—â–µ)
        
        Binary classification:
        - UP (1): +1.5%+ –∑–∞ 3 –ø–µ—Ä—ñ–æ–¥–∏ (4h: 12h, 1d: 3 –¥–Ω—ñ)
        - DOWN (0): —ñ–Ω—à–µ
        """
        # –í—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ –∑–º—ñ–Ω–∞ –∑–∞ lookback –ø–µ—Ä—ñ–æ–¥
        pct_change = (df['close'].shift(-lookback) / df['close'] - 1) * 100
        
        # –ü—Ä–æ—Å—Ç–æ UP/DOWN –∑ –ø–æ—Ä–æ–≥–æ–º 1.5%
        labels = (pct_change >= 1.5).astype(int)
        
        return labels
    
    def _create_simple_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°—Ç–≤–æ—Ä—é—î–º–æ –ø—Ä–æ—Å—Ç—ñ, –∞–ª–µ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ features
        
        Focus –Ω–∞ trend indicators, –Ω–µ –Ω–∞ noise
        """
        df = df.copy()
        
        # Trend indicators
        df = self.feature_engineer.calculate_all(
            df,
            sma_periods=[5, 10, 20, 50, 100, 200],
            ema_periods=[9, 12, 21, 26, 50],
            rsi_periods=[7, 14, 21, 28],
            atr_periods=[14, 21],
        )
        
        # Price relative to MAs (trend strength)
        for period in [5, 10, 20, 50, 100, 200]:
            ma_col = f'sma_{period}'
            if ma_col in df.columns:
                df[f'price_vs_sma{period}'] = (df['close'] / df[ma_col] - 1) * 100
        
        # MA crossovers (trend changes)
        if 'sma_50' in df.columns and 'sma_200' in df.columns:
            df['golden_cross'] = (df['sma_50'] > df['sma_200']).astype(int)
            
        if 'ema_12' in df.columns and 'ema_26' in df.columns:
            df['macd_cross'] = (df['ema_12'] > df['ema_26']).astype(int)
        
        # Volume trend
        if 'volume' in df.columns:
            df['volume_sma20'] = df['volume'].rolling(20).mean()
            df['volume_trend'] = df['volume'] / df['volume_sma20']
        
        # RSI levels (overbought/oversold)
        if 'rsi_14' in df.columns:
            df['rsi_overbought'] = (df['rsi_14'] > 70).astype(int)
            df['rsi_oversold'] = (df['rsi_14'] < 30).astype(int)
        
        # Momentum
        df['momentum_5'] = df['close'].pct_change(5) * 100
        df['momentum_10'] = df['close'].pct_change(10) * 100
        df['momentum_20'] = df['close'].pct_change(20) * 100
        
        # Volatility
        df['volatility_20'] = df['close'].rolling(20).std() / df['close'].rolling(20).mean() * 100
        
        return df
    
    async def prepare_data(self, days: int = 730) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        
        Args:
            days: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–Ω—ñ–≤ —ñ—Å—Ç–æ—Ä—ñ—ó (2 —Ä–æ–∫–∏ –¥–ª—è daily)
        """
        logger.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {self.timeframe} –¥–∞–Ω–∏—Ö –¥–ª—è {self.symbol}...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —á–µ—Ä–µ–∑ python-binance
        start_str = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        klines = self.client.get_historical_klines(
            symbol=self.symbol,
            interval=self.timeframe,
            start_str=start_str
        )
        
        df = pd.DataFrame(
            klines,
            columns=['timestamp', 'open', 'high', 'low', 'close', 'volume',
                    'close_time', 'quote_volume', 'trades', 'taker_buy_base',
                    'taker_buy_quote', 'ignore']
        )
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ç–∏–ø–∏
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)
        
        # –ó–∞–ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        
        if df is None or len(df) < 100:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: {len(df) if df is not None else 0} —Ä—è–¥–∫—ñ–≤")
        
        logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ features
        logger.info("üîß –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ features...")
        df = self._create_simple_features(df)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ labels
        logger.info("üè∑Ô∏è –°—Ç–≤–æ—Ä–µ–Ω–Ω—è trend labels...")
        df['trend_label'] = self._create_trend_labels(df, lookback=5)
        
        # –í–∏–¥–∞–ª—è—î–º–æ NaN
        df = df.dropna()
        
        logger.info(f"‚úÖ –ü—ñ—Å–ª—è –æ—á–∏—Å—Ç–∫–∏: {len(df)} —Ä—è–¥–∫—ñ–≤")
        
        # Label distribution
        label_counts = df['trend_label'].value_counts().sort_index()
        logger.info(f"üìä Label distribution:")
        for label, count in label_counts.items():
            pct = count / len(df) * 100
            label_name = {0: 'DOWN', 1: 'UP'}[label]
            logger.info(f"   {label_name:15s}: {count:4d} ({pct:5.1f}%)")
        
        # –í—ñ–¥–±—ñ—Ä features
        feature_cols = [col for col in df.columns 
                       if col not in ['open', 'high', 'low', 'close', 'volume', 
                                     'timestamp', 'trend_label']]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ infinity/NaN
        df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)
        df = df.dropna(subset=feature_cols)
        
        X = df[feature_cols].values
        y = df['trend_label'].values
        
        logger.info(f"‚úÖ Features: {len(feature_cols)}, Samples: {len(X)}")
        
        # Feature importance –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (—è–∫—â–æ —î)
        self.feature_names = feature_cols
        
        return X, y, feature_cols
    
    async def train(self, days: int = 730) -> Dict:
        """
        –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        
        Returns:
            Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üöÄ SIMPLE TREND CLASSIFIER: {self.symbol}")
        logger.info(f"{'='*80}\n")
        
        start_time = datetime.now()
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        X, y, feature_names = await self.prepare_data(days=days)
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False  # Time series - no shuffle!
        )
        
        logger.info(f"üìä Train: {len(X_train)}, Test: {len(X_test)}")
        
        # Scaling
        logger.info("‚öñÔ∏è Scaling features...")
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Random Forest (–Ω–∞–π–∫—Ä–∞—â–∞ –æ–¥–∏–Ω–æ—á–Ω–∞ –º–æ–¥–µ–ª—å)
        logger.info(f"üå≤ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è Random Forest (n_estimators={self.n_estimators})...")
        self.model = RandomForestClassifier(
            n_estimators=self.n_estimators,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'  # Handle imbalanced classes
        )
        
        self.model.fit(X_train_scaled, y_train)
        
        # –û—Ü—ñ–Ω–∫–∞
        y_pred_train = self.model.predict(X_train_scaled)
        y_pred_test = self.model.predict(X_test_scaled)
        
        train_acc = accuracy_score(y_train, y_pred_train)
        test_acc = accuracy_score(y_test, y_pred_test)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò")
        logger.info(f"{'='*80}")
        logger.info(f"Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)")
        logger.info(f"Test Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)")
        logger.info(f"\nTest Classification Report:")
        print(classification_report(y_test, y_pred_test, 
                                   target_names=['DOWN', 'UP'],
                                   zero_division=0))
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        logger.info(f"\nüèÜ Top 15 Most Important Features:")
        for idx, row in feature_importance.head(15).iterrows():
            logger.info(f"   {row['feature']:30s}: {row['importance']:.4f}")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        model_dir = Path(f'models/simple_trend_{self.symbol}')
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = model_dir / f'model_{self.symbol}_{self.timeframe}.pkl'
        scaler_path = model_dir / f'scaler_{self.symbol}_{self.timeframe}.pkl'
        features_path = model_dir / f'features_{self.symbol}_{self.timeframe}.pkl'
        
        joblib.dump(self.model, model_path)
        joblib.dump(self.scaler, scaler_path)
        joblib.dump(feature_names, features_path)
        
        logger.info(f"\nüíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {model_path}")
        
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚è±Ô∏è –ß–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {duration:.1f}s ({duration/60:.1f}m)")
        
        return {
            'train_accuracy': train_acc,
            'test_accuracy': test_acc,
            'n_features': len(feature_names),
            'duration': duration,
            'model_path': str(model_path),
            'feature_importance': feature_importance.to_dict('records')
        }


async def main():
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ BTCUSDT"""
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--symbol', default='BTCUSDT', help='Trading symbol')
    parser.add_argument('--timeframe', default='1d', help='Timeframe (4h, 1d)')
    parser.add_argument('--days', type=int, default=730, help='Days of history')
    args = parser.parse_args()
    
    trainer = SimpleTrendClassifier(
        symbol=args.symbol,
        timeframe=args.timeframe
    )
    
    results = await trainer.train(days=args.days)
    
    if results['test_accuracy'] >= 0.70:
        logger.info(f"\nüéâ SUCCESS! Accuracy {results['test_accuracy']*100:.2f}% >= 70%")
    else:
        logger.warning(f"\n‚ö†Ô∏è Accuracy {results['test_accuracy']*100:.2f}% < 70% - –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏")


if __name__ == '__main__':
    asyncio.run(main())
