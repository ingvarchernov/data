#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –≤—ñ–¥—ñ–±—Ä–∞–Ω–∏–º–∏ —Ñ—ñ—á–∞–º–∏
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç–æ–ø-35 —Ñ—ñ—á–µ–π + —Ä–æ–±–æ—á—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
"""

import os
import sys
import asyncio
import logging
import numpy as np
import pandas as pd
from typing import Tuple
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import joblib

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from gpu_config import configure_gpu
from optimized_model import OptimizedPricePredictionModel
from train_model_advanced import AdvancedModelTrainer
from selected_features import SELECTED_FEATURES

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

configure_gpu()

# –û–ü–¢–ò–ú–ê–õ–¨–ù–ê –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø (–ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –¥–æ —Ä–æ–±–æ—á–æ—ó)
OPTIMAL_CONFIG = {
    'model_type': 'advanced_lstm',  # –ù–ï deep_lstm_xl!
    'sequence_length': 60,           # –ù–ï 96!
    'batch_size': 64,                # –ù–ï 32!
    'epochs': 200,
    'learning_rate': 0.0005,
    'early_stopping_patience': 25,
    'reduce_lr_patience': 10,
    
    # LSTM (—Ä–æ–±–æ—á–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è)
    'lstm_units_1': 320,
    'lstm_units_2': 160,
    'lstm_units_3': 80,
    
    # Attention
    'attention_heads': 10,
    'attention_key_dim': 80,
    
    # Dense
    'dense_units': [640, 320, 160, 80],
    
    # Regularization
    'dropout_rate': 0.4,
    'l2_regularization': 0.01,
}


class OptimizedModelTrainer:
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π trainer –∑ –≤—ñ–¥—ñ–±—Ä–∞–Ω–∏–º–∏ —Ñ—ñ—á–∞–º–∏"""
    
    def __init__(self, symbol: str, interval: str = '1h'):
        self.symbol = symbol
        self.interval = interval
        self.config = OPTIMAL_CONFIG.copy()
        self.scaler = RobustScaler()
        self.model = None
        self.history = None
        
        logger.info(f"üìä {symbol}: {self.config['model_type']}, "
                   f"seq_len={self.config['sequence_length']}, "
                   f"batch={self.config['batch_size']}, "
                   f"features={len(SELECTED_FEATURES)}")
    
    async def load_data(self, days: int = 365) -> pd.DataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö"""
        logger.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {days} –¥–Ω—ñ–≤ –¥–∞–Ω–∏—Ö –¥–ª—è {self.symbol}...")
        
        from dotenv import load_dotenv
        load_dotenv()
        
        from intelligent_sys import UnifiedBinanceLoader
        
        loader = UnifiedBinanceLoader(
            api_key=os.getenv('FUTURES_API_KEY'),
            api_secret=os.getenv('FUTURES_API_SECRET'),
            testnet=False
        )
        
        try:
            data = await loader.get_historical_data(
                symbol=self.symbol,
                interval=self.interval,
                days_back=days
            )
            
            if data is None or len(data) < 500:
                logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è {self.symbol}")
                return None
            
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
            return data
            
        finally:
            await loader.close()
    
    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¢–Ü–õ–¨–ö–ò –≤—ñ–¥—ñ–±—Ä–∞–Ω–∏—Ö —Ñ—ñ—á–µ–π"""
        logger.info(f"üîß –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—ñ–¥—ñ–±—Ä–∞–Ω–∏—Ö —Ñ—ñ—á–µ–π –¥–ª—è {self.symbol}...")
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π AdvancedModelTrainer –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –≤—Å—ñ—Ö —Ñ—ñ—á–µ–π
        temp_trainer = AdvancedModelTrainer(self.symbol, testnet=False)
        df_all = temp_trainer.prepare_features(data)
        
        # –í—ñ–¥–±–∏—Ä–∞—î–º–æ –¢–Ü–õ–¨–ö–ò —Ç–æ–ø-35 —Ñ—ñ—á–µ–π
        missing_features = [f for f in SELECTED_FEATURES if f not in df_all.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ —Ñ—ñ—á—ñ: {missing_features}")
        
        available_features = [f for f in SELECTED_FEATURES if f in df_all.columns]
        df = df_all[available_features].copy()
        
        # –í–∏–¥–∞–ª—è—î–º–æ NaN
        initial_len = len(df)
        df = df.dropna()
        dropped = initial_len - len(df)
        
        logger.info(f"‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤ –∑ {len(df.columns)} –≤—ñ–¥—ñ–±—Ä–∞–Ω–∏–º–∏ —Ñ—ñ—á–∞–º–∏ (dropped {dropped} NaN)")
        logger.info(f"üìä –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —Ñ—ñ—á–µ–π: {len(available_features)}/{len(SELECTED_FEATURES)}")
        
        return df
    
    def create_sequences(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π"""
        sequence_length = self.config['sequence_length']
        
        # –í—Å—ñ –∫–æ–ª–æ–Ω–∫–∏ - —Ü–µ –≤–∂–µ –≤—ñ–¥—ñ–±—Ä–∞–Ω—ñ —Ñ—ñ—á—ñ
        feature_cols = list(data.columns)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        scaled_data = self.scaler.fit_transform(data[feature_cols].values)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        X, y = [], []
        
        # –î–æ–¥–∞—î–º–æ –∫–æ–ª–æ–Ω–∫—É close –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É target
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ close –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ index
        close_prices = data.index.to_series().apply(
            lambda idx: data.loc[idx, 'open'] if 'open' in data.columns else 0
        )
        
        # –Ø–∫—â–æ —î returns, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è close
        if 'returns' in data.columns:
            # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ close —á–µ—Ä–µ–∑ returns
            logger.info("üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é returns –¥–ª—è target")
        
        for i in range(sequence_length, len(scaled_data) - 1):
            X.append(scaled_data[i-sequence_length:i])
            
            # Target - returns –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É –∫—Ä–æ—Ü—ñ (–≤–∂–µ —î –≤ –¥–∞–Ω–∏—Ö)
            if i + 1 < len(data) and 'returns' in data.columns:
                y.append(data['returns'].iloc[i + 1])
            else:
                # Fallback: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ returns –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫—Ä–æ–∫—É
                y.append(data['returns'].iloc[i] if 'returns' in data.columns else 0)
        
        X = np.array(X)
        y = np.array(y)
        
        logger.info(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(X)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π, —Ä–æ–∑–º—ñ—Ä: {X.shape}")
        return X, y
    
    def build_and_compile_model(self, input_shape: Tuple) -> tf.keras.Model:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        logger.info(f"ü§ñ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {self.config['model_type']}")
        
        model_builder = OptimizedPricePredictionModel(
            input_shape=input_shape,
            model_type=self.config['model_type'],
            model_config=self.config
        )
        
        model = model_builder.create_model()
        model = model_builder.compile_model(
            model, 
            learning_rate=self.config['learning_rate']
        )
        
        self.model = model
        return model
    
    def train(self, X: np.ndarray, y: np.ndarray) -> dict:
        """–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"""
        logger.info(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è {self.symbol}...")
        
        # Cross-validation
        tscv = TimeSeriesSplit(n_splits=3)
        
        from tensorflow.keras.callbacks import (
            EarlyStopping, 
            ReduceLROnPlateau, 
            ModelCheckpoint
        )
        
        model_dir = f'models/optimized_{self.symbol.replace("USDT", "")}'
        os.makedirs(model_dir, exist_ok=True)
        checkpoint_path = f'{model_dir}/best_model.h5'
        
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=self.config['early_stopping_patience'],
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=self.config['reduce_lr_patience'],
                min_lr=1e-7,
                verbose=1
            ),
            ModelCheckpoint(
                checkpoint_path,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # –û—Å—Ç–∞–Ω–Ω—ñ–π fold –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        train_idx, val_idx = list(tscv.split(X))[-1]
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        logger.info(f"üìä Train: {len(X_train)}, Val: {len(X_val)}")
        
        # –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª—ñ
        input_shape = (X_train.shape[1], X_train.shape[2])
        self.build_and_compile_model(input_shape)
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=self.config['epochs'],
            batch_size=self.config['batch_size'],
            callbacks=callbacks,
            verbose=1
        )
        
        self.history = history.history
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è scaler
        scaler_path = f'{model_dir}/scaler.pkl'
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"‚úÖ Scaler –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {scaler_path}")
        
        val_loss = min(history.history['val_loss'])
        val_dir_acc = max(history.history.get('val_directional_accuracy', [0]))
        
        logger.info(f"‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"   Best val_loss: {val_loss:.6f}")
        logger.info(f"   Best val_directional_accuracy: {val_dir_acc:.4f}")
        
        return {
            'symbol': self.symbol,
            'val_loss': val_loss,
            'val_dir_acc': val_dir_acc,
            'model_path': checkpoint_path,
            'scaler_path': scaler_path
        }


async def quick_test_optimized():
    """–®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
    symbol = 'BTCUSDT'
    
    logger.info("=" * 80)
    logger.info(f"üöÄ –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ï –¢–†–ï–ù–£–í–ê–ù–ù–Ø - {symbol}")
    logger.info("=" * 80)
    logger.info(f"\nüìä –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:")
    logger.info(f"   Model: {OPTIMAL_CONFIG['model_type']}")
    logger.info(f"   Sequence: {OPTIMAL_CONFIG['sequence_length']}")
    logger.info(f"   Batch: {OPTIMAL_CONFIG['batch_size']}")
    logger.info(f"   Features: {len(SELECTED_FEATURES)} (–≤—ñ–¥—ñ–±—Ä–∞–Ω—ñ)")
    logger.info(f"   Data: 365 days")
    logger.info("")
    
    try:
        trainer = OptimizedModelTrainer(symbol)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
        data = await trainer.load_data(days=365)
        if data is None:
            return None
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ—á–µ–π
        df = trainer.prepare_features(data)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
        X, y = trainer.create_sequences(df)
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        result = trainer.train(X, y)
        
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ –¢–†–ï–ù–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("=" * 80)
        logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")
        logger.info(f"   –°–∏–º–≤–æ–ª: {result['symbol']}")
        logger.info(f"   Best val_directional_accuracy: {result['val_dir_acc']:.4f} ({result['val_dir_acc']*100:.2f}%)")
        logger.info(f"   Best val_loss: {result['val_loss']:.6f}")
        logger.info(f"   –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {result['model_path']}")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ü–û–ú–ò–õ–ö–ê: {e}", exc_info=True)
        return None


if __name__ == "__main__":
    result = asyncio.run(quick_test_optimized())
    
    if result:
        logger.info("\nüéâ –£–°–ü–Ü–•!")
        sys.exit(0)
    else:
        logger.error("\n‚ùå –ù–ï–í–î–ê–ß–ê")
        sys.exit(1)
