# -*- coding: utf-8 -*-
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö
–í–∫–ª—é—á–∞—î –ø—É–ª –∑'—î–¥–Ω–∞–Ω—å, –∫–µ—à—É–≤–∞–Ω–Ω—è, batch –æ–ø–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ—Å—Ç—å
"""
import asyncio
import logging
import os
import pickle
import hashlib
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text, MetaData, Table, select, insert, update
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.pool import QueuePool
import redis.asyncio as redis
from cachetools import TTLCache
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)


class OptimizedDatabaseManager:
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º —Ç–∞ –ø—É–ª–æ–º –∑'—î–¥–Ω–∞–Ω—å"""
    
    def __init__(
        self, 
        db_url: str = None,
        redis_url: str = "redis://localhost:6379",
        use_redis: bool = True,
        pool_size: int = 20,
        max_overflow: int = 30,
        cache_ttl: int = 3600
    ):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ë–î
        
        Args:
            db_url: URL –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ PostgreSQL
            redis_url: URL –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Redis
            use_redis: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Redis –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
            pool_size: –†–æ–∑–º—ñ—Ä –ø—É–ª—É –∑'—î–¥–Ω–∞–Ω—å
            max_overflow: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∑'—î–¥–Ω–∞–Ω—å
            cache_ttl: –ß–∞—Å –∂–∏—Ç—Ç—è –∫–µ—à—É (—Å–µ–∫—É–Ω–¥–∏)
        """
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        if db_url is None:
            env_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT', 'DB_NAME']
            missing = [var for var in env_vars if not os.getenv(var)]
            if missing:
                raise ValueError(
                    f"‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –¥–ª—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î: {missing}. "
                    f"–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–¥–∞–π—Ç–µ —ó—Ö —É .env –∞–±–æ —á–µ—Ä–µ–∑ export."
                )
            db_url = (
                f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
                f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
            )
        
        self.db_url = db_url
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π engine
        self.sync_engine = create_engine(
            db_url,
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π engine
        async_db_url = db_url.replace("postgresql://", "postgresql+asyncpg://")
        self.async_engine = create_async_engine(
            async_db_url,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        self.async_session_factory = async_sessionmaker(
            self.async_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        # –ö–µ—à—É–≤–∞–Ω–Ω—è
        self.use_redis = use_redis
        self.memory_cache = TTLCache(maxsize=1000, ttl=cache_ttl)
        self.redis_client = None
        self.cache_ttl = cache_ttl
        
        if use_redis:
            try:
                self.redis_client = redis.from_url(
                    redis_url, 
                    max_connections=20, 
                    decode_responses=False
                )
                logger.info("‚úÖ Redis –∫–ª—ñ—î–Ω—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ memory cache: {e}")
                self.use_redis = False
        
        # –ú–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ —Ç–∞–±–ª–∏—Ü—å
        self.metadata = MetaData()
        self._metadata_loaded = False
        
        logger.info("‚úÖ OptimizedDatabaseManager —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")
    
    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è (—Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑'—î–¥–Ω–∞–Ω—å)"""
        try:
            # –¢–µ—Å—Ç PostgreSQL
            async with self.async_session_factory() as session:
                await session.execute(text("SELECT 1"))
            logger.info("‚úÖ PostgreSQL –∑'—î–¥–Ω–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–µ")
            
            # –¢–µ—Å—Ç Redis
            if self.use_redis and self.redis_client:
                try:
                    await self.redis_client.ping()
                    logger.info("‚úÖ Redis –∑'—î–¥–Ω–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–µ")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Redis ping failed: {e}")
                    self.use_redis = False
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
            self._load_table_metadata()
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ë–î: {e}")
            raise
    
    def _load_table_metadata(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —Ç–∞–±–ª–∏—Ü—å (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó)"""
        if self._metadata_loaded:
            return
        
        try:
            with self.sync_engine.connect() as conn:
                self.metadata.reflect(bind=conn)
            self._metadata_loaded = True
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω—ñ {len(self.metadata.tables)} —Ç–∞–±–ª–∏—Ü—å")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {e}")
    
    def _generate_cache_key(self, query: str, params: Dict = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        key_data = f"{query}_{params or {}}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É"""
        # Memory cache
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # Redis cache
        if self.use_redis and self.redis_client:
            try:
                data = await self.redis_client.get(cache_key)
                if data:
                    result = pickle.loads(data)
                    self.memory_cache[cache_key] = result
                    return result
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∑ Redis: {e}")
        
        return None
    
    async def _set_cache(self, cache_key: str, data: Any, ttl: int = None):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤ –∫–µ—à"""
        if ttl is None:
            ttl = self.cache_ttl
        
        # Memory cache
        self.memory_cache[cache_key] = data
        
        # Redis cache
        if self.use_redis and self.redis_client:
            try:
                serialized_data = pickle.dumps(data)
                await self.redis_client.setex(cache_key, ttl, serialized_data)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ Redis: {e}")
    
    async def execute_query_cached(
        self, 
        query: str, 
        params: Dict = None, 
        use_cache: bool = True,
        cache_ttl: int = None
    ) -> pd.DataFrame:
        """–í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        cache_key = self._generate_cache_key(query, params) if use_cache else None
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É
        if use_cache and cache_key:
            cached_result = await self._get_from_cache(cache_key)
            if cached_result is not None:
                logger.debug(f"üéØ Cache hit: {query[:50]}...")
                return cached_result
        
        # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É
        async with self.async_session_factory() as session:
            try:
                result = await session.execute(text(query), params or {})
                df = pd.DataFrame(result.fetchall(), columns=result.keys())
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à
                if use_cache and cache_key:
                    await self._set_cache(cache_key, df, cache_ttl)
                    logger.debug(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à")
                
                return df
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É: {e}")
                await session.rollback()
                raise
    
    async def get_or_create_symbol_id(self, symbol: str) -> int:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ symbol_id"""
        async with self.async_session_factory() as session:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è
            result = await session.execute(
                text("SELECT symbol_id FROM symbols WHERE symbol = :symbol"),
                {"symbol": symbol}
            )
            row = result.fetchone()
            
            if row:
                return row[0]
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ–≥–æ
            result = await session.execute(
                text("INSERT INTO symbols (symbol) VALUES (:symbol) RETURNING symbol_id"),
                {"symbol": symbol}
            )
            new_id = result.fetchone()[0]
            await session.commit()
            
            logger.debug(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ symbol_id={new_id} –¥–ª—è {symbol}")
            return new_id
    
    async def get_or_create_interval_id(self, interval: str) -> int:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ interval_id"""
        async with self.async_session_factory() as session:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è
            result = await session.execute(
                text("SELECT interval_id FROM intervals WHERE interval = :interval"),
                {"interval": interval}
            )
            row = result.fetchone()
            
            if row:
                return row[0]
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ–≥–æ
            result = await session.execute(
                text("INSERT INTO intervals (interval) VALUES (:interval) RETURNING interval_id"),
                {"interval": interval}
            )
            new_id = result.fetchone()[0]
            await session.commit()
            
            logger.debug(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ interval_id={new_id} –¥–ª—è {interval}")
            return new_id
    
    async def batch_insert(
        self, 
        table_name: str, 
        data: List[Dict], 
        batch_size: int = 1000
    ) -> bool:
        """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–∏—Ö"""
        if not data:
            return True
        
        if not self._metadata_loaded:
            self._load_table_metadata()
        
        table = self.metadata.tables.get(table_name)
        if table is None:
            logger.error(f"‚ùå –¢–∞–±–ª–∏—Ü—è {table_name} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        async with self.async_session_factory() as session:
            try:
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await session.execute(insert(table), batch)
                
                await session.commit()
                logger.info(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤ –≤ {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ—ó –≤—Å—Ç–∞–≤–∫–∏ –≤ {table_name}: {e}")
                await session.rollback()
                return False
    
    async def batch_upsert(
        self, 
        table_name: str, 
        data: List[Dict], 
        conflict_columns: List[str], 
        batch_size: int = 1000
    ) -> bool:
        """–ü–∞–∫–µ—Ç–Ω–∏–π upsert (INSERT ... ON CONFLICT UPDATE)"""
        if not data:
            return True
        
        async with self.async_session_factory() as session:
            try:
                columns = list(data[0].keys())
                values_placeholder = ", ".join([f":{col}" for col in columns])
                update_set = ", ".join([
                    f"{col} = EXCLUDED.{col}" 
                    for col in columns 
                    if col not in conflict_columns
                ])
                conflict_cols = ", ".join(conflict_columns)
                
                query = f"""
                INSERT INTO {table_name} ({', '.join(columns)})
                VALUES ({values_placeholder})
                ON CONFLICT ({conflict_cols})
                DO UPDATE SET {update_set}
                """
                
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    for record in batch:
                        await session.execute(text(query), record)
                
                await session.commit()
                logger.info(f"‚úÖ Upsert {len(data)} –∑–∞–ø–∏—Å—ñ–≤ –≤ {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ batch upsert –≤ {table_name}: {e}")
                await session.rollback()
                return False
    
    async def get_historical_data_optimized(
        self, 
        symbol_id: int, 
        interval_id: int, 
        days_back: int,
        use_cache: bool = True
    ) -> pd.DataFrame:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        start_time = datetime.now() - timedelta(days=days_back)
        
        query = """
        SELECT 
            hd.data_id,
            hd.timestamp,
            hd.open,
            hd.high,
            hd.low,
            hd.close,
            hd.volume,
            hd.quote_av,
            hd.trades,
            hd.tb_base_av,
            hd.tb_quote_av
        FROM historical_data hd
        WHERE hd.symbol_id = :symbol_id 
        AND hd.interval_id = :interval_id
        AND hd.timestamp >= :start_time
        ORDER BY hd.timestamp ASC
        """
        
        params = {
            'symbol_id': symbol_id,
            'interval_id': interval_id,
            'start_time': start_time
        }
        
        df = await self.execute_query_cached(query, params, use_cache)
        
        if not df.empty:
            logger.info(
                f"üîç –û—Ç—Ä–∏–º–∞–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤ –¥–ª—è "
                f"symbol_id={symbol_id}, interval_id={interval_id}"
            )
        else:
            logger.warning(
                f"‚ö†Ô∏è –ü–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è "
                f"symbol_id={symbol_id}, interval_id={interval_id}"
            )
        
        return df
    
    async def invalidate_cache_pattern(self, pattern: str):
        """–Ü–Ω–≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–µ—à—É –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º"""
        if pattern == "*":
            self.memory_cache.clear()
            logger.info("üóëÔ∏è Memory cache –æ—á–∏—â–µ–Ω–æ")
        
        if self.use_redis and self.redis_client:
            try:
                cursor = 0
                keys_deleted = 0
                
                while True:
                    cursor, keys = await self.redis_client.scan(
                        cursor=cursor, 
                        match=pattern, 
                        count=100
                    )
                    
                    if keys:
                        await self.redis_client.delete(*keys)
                        keys_deleted += len(keys)
                    
                    if cursor == 0:
                        break
                
                if keys_deleted > 0:
                    logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ {keys_deleted} –∫–ª—é—á—ñ–≤ –∑ Redis")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è Redis: {e}")
    
    async def get_cache_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        stats = {
            'memory_cache_size': len(self.memory_cache),
            'memory_cache_maxsize': self.memory_cache.maxsize,
            'redis_available': self.use_redis
        }
        
        if self.use_redis and self.redis_client:
            try:
                info = await self.redis_client.info('memory')
                stats['redis_memory_used'] = info.get('used_memory_human', 'N/A')
                stats['redis_keys'] = await self.redis_client.dbsize()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ Redis: {e}")
        
        return stats
    
    async def close(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –∑'—î–¥–Ω–∞–Ω—å"""
        try:
            await self.async_engine.dispose()
            self.sync_engine.dispose()
            
            if self.redis_client:
                await self.redis_client.close()
            
            logger.info("‚úÖ –ë–î –∑'—î–¥–Ω–∞–Ω–Ω—è –∑–∞–∫—Ä–∏—Ç–æ")
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–∫—Ä–∏—Ç—Ç—è –ë–î: {e}")


# ============================================================================
# –¢–û–†–ì–û–í–Ü –§–£–ù–ö–¶–Ü–á
# ============================================================================

async def save_trading_signal(db_manager, signal_data: Dict[str, Any]) -> int:
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª—É"""
    try:
        signal_record = {
            'symbol': signal_data['symbol'],
            'action': signal_data['action'],
            'confidence': float(signal_data['confidence']),
            'entry_price': float(signal_data['entry_price']),
            'stop_loss': float(signal_data['stop_loss']) if signal_data.get('stop_loss') else None,
            'take_profit': float(signal_data['take_profit']) if signal_data.get('take_profit') else None,
            'quantity': float(signal_data['quantity']),
            'strategy': signal_data.get('strategy', 'unknown'),
            'prediction_source': signal_data.get('prediction_source', 'technical'),
            'status': signal_data.get('status', 'generated'),
            'notes': signal_data.get('notes', '')
        }
        
        async with db_manager.async_session_factory() as session:
            result = await session.execute(
                text("""
                    INSERT INTO trading_signals 
                    (symbol, action, confidence, entry_price, stop_loss, take_profit, 
                     quantity, strategy, prediction_source, status, notes) 
                    VALUES 
                    (:symbol, :action, :confidence, :entry_price, :stop_loss, :take_profit, 
                     :quantity, :strategy, :prediction_source, :status, :notes) 
                    RETURNING id
                """),
                signal_record
            )
            signal_id = result.fetchone()[0]
            await session.commit()
            
        logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —Å–∏–≥–Ω–∞–ª: {signal_data['symbol']} {signal_data['action']}")
        return signal_id
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∏–≥–Ω–∞–ª—É: {e}")
        return None


async def save_position(db_manager, position_data: Dict[str, Any]) -> int:
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—ñ–¥–∫—Ä–∏—Ç–æ—ó –ø–æ–∑–∏—Ü—ñ—ó"""
    try:
        logger.info(f"üîÑ –°–ø—Ä–æ–±–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ–∑–∏—Ü—ñ—ó: {position_data.get('symbol')} {position_data.get('side')}")
        
        position_record = {
            'symbol': position_data['symbol'],
            'side': position_data['side'],
            'entry_price': float(position_data['entry_price']),
            'quantity': float(position_data['quantity']),
            'stop_loss': float(position_data['stop_loss']) if position_data.get('stop_loss') else None,
            'take_profit': float(position_data['take_profit']) if position_data.get('take_profit') else None,
            'strategy': position_data.get('strategy', 'unknown'),
            'status': position_data.get('status', 'open'),
            'signal_id': position_data.get('signal_id'),
            'metadata': json.dumps(position_data.get('metadata', {}))
        }
        
        logger.info(f"üìù Position record: entry_price={position_record['entry_price']}, quantity={position_record['quantity']}")
        
        async with db_manager.async_session_factory() as session:
            result = await session.execute(
                text("""
                    INSERT INTO positions 
                    (symbol, side, entry_price, quantity, stop_loss, take_profit, 
                     strategy, status, signal_id, metadata) 
                    VALUES 
                    (:symbol, :side, :entry_price, :quantity, :stop_loss, :take_profit, 
                     :strategy, :status, :signal_id, :metadata) 
                    RETURNING id
                """),
                position_record
            )
            position_id = result.fetchone()[0]
            await session.commit()
            
        logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –ø–æ–∑–∏—Ü—ñ—é: {position_data['symbol']} {position_data['side']} (ID: {position_id})")
        return position_id
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ–∑–∏—Ü—ñ—ó: {e}", exc_info=True)
        return None


async def save_trade(db_manager, trade_data: Dict[str, Any]) -> int:
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞–∫—Ä–∏—Ç–æ—ó —É–≥–æ–¥–∏"""
    try:
        entry_price = float(trade_data['entry_price'])
        exit_price = float(trade_data['exit_price'])
        quantity = float(trade_data['quantity'])
        fees = float(trade_data.get('fees', 0))
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ P&L
        if entry_price == 0:
            logger.warning(f"‚ö†Ô∏è Entry price = 0 –¥–ª—è {trade_data['symbol']}")
            pnl = -fees
            pnl_percentage = 0
        elif trade_data['side'] == 'LONG':
            gross_pnl = (exit_price - entry_price) * quantity
            pnl = gross_pnl - fees
            pnl_percentage = ((exit_price - entry_price) / entry_price) * 100
        else:  # SHORT
            gross_pnl = (entry_price - exit_price) * quantity
            pnl = gross_pnl - fees
            pnl_percentage = ((entry_price - exit_price) / entry_price) * 100
        
        trade_record = {
            'symbol': trade_data['symbol'],
            'side': trade_data['side'],
            'entry_price': entry_price,
            'exit_price': exit_price,
            'quantity': quantity,
            'entry_time': trade_data['entry_time'],
            'exit_time': trade_data.get('exit_time', datetime.now()),
            'pnl': pnl,
            'pnl_percentage': pnl_percentage,
            'strategy': trade_data.get('strategy', 'unknown'),
            'exit_reason': trade_data.get('exit_reason', 'manual'),
            'position_id': trade_data.get('position_id'),
            'signal_id': trade_data.get('signal_id'),
            'fees': fees,
            'metadata': json.dumps(trade_data.get('metadata', {}))
        }
        
        async with db_manager.async_session_factory() as session:
            result = await session.execute(
                text("""
                    INSERT INTO trades 
                    (symbol, side, entry_price, exit_price, quantity, entry_time, exit_time, 
                     pnl, pnl_percentage, strategy, exit_reason, position_id, signal_id, fees, metadata) 
                    VALUES 
                    (:symbol, :side, :entry_price, :exit_price, :quantity, :entry_time, :exit_time, 
                     :pnl, :pnl_percentage, :strategy, :exit_reason, :position_id, :signal_id, :fees, :metadata) 
                    RETURNING id
                """),
                trade_record
            )
            trade_id = result.fetchone()[0]
            await session.commit()
            
        logger.info(
            f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —É–≥–æ–¥—É: {trade_data['symbol']} {trade_data['side']} "
            f"P&L: ${pnl:.2f} ({pnl_percentage:.2f}%)"
        )
        return trade_id
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É–≥–æ–¥–∏: {e}")
        return None


# ============================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–ò–ô –ú–ï–ù–ï–î–ñ–ï–†
# ============================================================================

db_manager = OptimizedDatabaseManager()


# ============================================================================
# –§–£–ù–ö–¶–Ü–á –î–õ–Ø –ó–í–û–†–û–¢–ù–û–á –°–£–ú–Ü–°–ù–û–°–¢–Ü
# ============================================================================

async def get_historical_data_from_db_async(
    symbol: str, 
    interval: str, 
    days_back: int, 
    api_key: str = None, 
    api_secret: str = None, 
    skip_append: bool = False
) -> pd.DataFrame:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    symbol_id = await db_manager.get_or_create_symbol_id(symbol)
    interval_id = await db_manager.get_or_create_interval_id(interval)
    
    return await db_manager.get_historical_data_optimized(
        symbol_id, interval_id, days_back
    )


def get_historical_data_from_db(
    symbol: str, 
    interval: str, 
    days_back: int, 
    api_key: str = None, 
    api_secret: str = None, 
    skip_append: bool = False
) -> pd.DataFrame:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞"""
    return asyncio.run(
        get_historical_data_from_db_async(
            symbol, interval, days_back, api_key, api_secret, skip_append
        )
    )