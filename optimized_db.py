# -*- coding: utf-8 -*-
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö
–í–∫–ª—é—á–∞—î –ø—É–ª –∑'—î–¥–Ω–∞–Ω—å, –∫–µ—à—É–≤–∞–Ω–Ω—è, batch –æ–ø–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ—Å—Ç—å
"""
import asyncio
import logging
import os
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from contextlib import asynccontextmanager

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text, MetaData, Table, select, insert, update
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
import redis
from cachetools import TTLCache
import redis.asyncio as redis
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class OptimizedDatabaseManager:
    async def get_or_create_interval_id(self, interval: str) -> int:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ interval_id –¥–ª—è —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É —É —Ç–∞–±–ª–∏—Ü—ñ intervals
        """
        async with self.async_session_factory() as session:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î —ñ–Ω—Ç–µ—Ä–≤–∞–ª
            result = await session.execute(text("SELECT interval_id FROM intervals WHERE interval = :interval"), {"interval": interval})
            row = result.fetchone()
            if row:
                return row[0]
            # –Ø–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î ‚Äî —Å—Ç–≤–æ—Ä—é—î–º–æ
            result = await session.execute(text("INSERT INTO intervals (interval) VALUES (:interval) RETURNING interval_id"), {"interval": interval})
            new_id = result.fetchone()[0]
            await session.commit()
            return new_id
    async def get_or_create_symbol_id(self, symbol: str) -> int:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ symbol_id —Å–∏–º–≤–æ–ª—É —É —Ç–∞–±–ª–∏—Ü—ñ symbols
        """
        async with self.async_session_factory() as session:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î —Å–∏–º–≤–æ–ª
            result = await session.execute(text("SELECT symbol_id FROM symbols WHERE symbol = :symbol"), {"symbol": symbol})
            row = result.fetchone()
            if row:
                return row[0]
            # –Ø–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î ‚Äî —Å—Ç–≤–æ—Ä—é—î–º–æ
            result = await session.execute(text("INSERT INTO symbols (symbol) VALUES (:symbol) RETURNING symbol_id"), {"symbol": symbol})
            new_id = result.fetchone()[0]
            await session.commit()
            return new_id
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º —Ç–∞ –ø—É–ª–æ–º –∑'—î–¥–Ω–∞–Ω—å"""
    
    def __init__(self, 
                 db_url: str = None,
                 redis_url: str = "redis://localhost:6379",
                 use_redis: bool = True,
                 pool_size: int = 20,
                 max_overflow: int = 30,
                 cache_ttl: int = 3600):
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        if db_url is None:
            env_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT', 'DB_NAME']
            missing = [var for var in env_vars if not os.getenv(var)]
            if missing:
                raise ValueError(f"‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –¥–ª—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î: {missing}. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–¥–∞–π—Ç–µ —ó—Ö —É .env –∞–±–æ —á–µ—Ä–µ–∑ export.")
            db_url = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π engine
        self.sync_engine = create_engine(
            db_url,
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π engine
        async_db_url = db_url.replace("postgresql://", "postgresql+asyncpg://")
        self.async_engine = create_async_engine(
            async_db_url,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        self.async_session_factory = async_sessionmaker(
            self.async_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        # –ö–µ—à—É–≤–∞–Ω–Ω—è
        self.use_redis = use_redis
        self.memory_cache = TTLCache(maxsize=1000, ttl=cache_ttl)
        self.redis_pool = None
        
        if use_redis:
            try:
                self.redis_client = redis.from_url(redis_url, max_connections=20, decode_responses=False)
                # –¢–µ—Å—Ç—É—î–º–æ –∑'—î–¥–Ω–∞–Ω–Ω—è (—á–µ—Ä–µ–∑ event loop)
                try:
                    import asyncio
                    loop = asyncio.get_event_loop()
                    loop.run_until_complete(self.redis_client.ping())
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Redis ping –Ω–µ –≤–¥–∞–ª–æ—Å—è: {e}")
                logger.info("‚úÖ Redis –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ memory cache: {e}")
                self.use_redis = False
        
        # –ú–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ —Ç–∞–±–ª–∏—Ü—å
        self.metadata = MetaData()
        self._load_table_metadata()
        
    def _load_table_metadata(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —Ç–∞–±–ª–∏—Ü—å"""
        try:
            self.metadata.reflect(bind=self.sync_engine)
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è {len(self.metadata.tables)} —Ç–∞–±–ª–∏—Ü—å")
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {e}")
    
    def _generate_cache_key(self, query: str, params: Dict = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        key_data = f"{query}_{params or {}}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É"""
        # –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ memory cache
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # –ü–æ—Ç—ñ–º Redis
        if self.use_redis:
            try:
                data = await self.redis_client.get(cache_key)
                if data:
                    result = pickle.loads(data)
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ memory cache –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É
                    self.memory_cache[cache_key] = result
                    return result
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∑ Redis: {e}")
        
        return None
    
    async def _set_cache(self, cache_key: str, data: Any, ttl: int = 3600):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤ –∫–µ—à"""
        # Memory cache
        self.memory_cache[cache_key] = data
        
        # Redis cache
        if self.use_redis:
            try:
                serialized_data = pickle.dumps(data)
                await self.redis_client.setex(cache_key, ttl, serialized_data)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ Redis: {e}")
    
    async def execute_query_cached(self, 
                                  query: str, 
                                  params: Dict = None, 
                                  use_cache: bool = True,
                                  cache_ttl: int = 3600) -> pd.DataFrame:
        """–í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        cache_key = self._generate_cache_key(query, params) if use_cache else None
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
        if use_cache and cache_key:
            cached_result = await self._get_from_cache(cache_key)
            if cached_result is not None:
                logger.debug(f"üéØ Cache hit –¥–ª—è –∑–∞–ø–∏—Ç—É: {query[:50]}...")
                return cached_result
        
        # –í–∏–∫–æ–Ω—É—î–º–æ –∑–∞–ø–∏—Ç
        async with self.async_session_factory() as session:
            try:
                result = await session.execute(text(query), params or {})
                df = pd.DataFrame(result.fetchall(), columns=result.keys())
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
                if use_cache and cache_key:
                    await self._set_cache(cache_key, df, cache_ttl)
                    logger.debug(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à")
                
                return df
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É: {e}")
                await session.rollback()
                raise
    
    async def batch_insert(self, table_name: str, data: List[Dict], batch_size: int = 1000) -> bool:
        """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–∏—Ö"""
        if not data:
            return True
            
        table = self.metadata.tables.get(table_name)
        if table is None:
            logger.error(f"‚ùå –¢–∞–±–ª–∏—Ü—è {table_name} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        async with self.async_session_factory() as session:
            try:
                # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ –±–∞—Ç—á—ñ
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await session.execute(insert(table), batch)
                    
                await session.commit()
                logger.info(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤ –≤ {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ—ó –≤—Å—Ç–∞–≤–∫–∏ –≤ {table_name}: {e}")
                await session.rollback()
                return False
    
    async def batch_upsert(self, table_name: str, data: List[Dict], conflict_columns: List[str], batch_size: int = 1000) -> bool:
        """–ü–∞–∫–µ—Ç–Ω–∏–π upsert (INSERT ... ON CONFLICT UPDATE)"""
        if not data:
            return True
            
        async with self.async_session_factory() as session:
            try:
                # –§–æ—Ä–º—É—î–º–æ –∑–∞–ø–∏—Ç –∑ ON CONFLICT
                columns = list(data[0].keys())
                values_placeholder = ", ".join([f":{col}" for col in columns])
                update_set = ", ".join([f"{col} = EXCLUDED.{col}" for col in columns if col not in conflict_columns])
                conflict_cols = ", ".join(conflict_columns)
                
                query = f"""
                INSERT INTO {table_name} ({', '.join(columns)})
                VALUES ({values_placeholder})
                ON CONFLICT ({conflict_cols})
                DO UPDATE SET {update_set}
                """
                
                # –í–∏–∫–æ–Ω—É—î–º–æ –±–∞—Ç—á–∞–º–∏
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await session.execute(text(query), batch)
                    
                await session.commit()
                logger.info(f"‚úÖ Upsert {len(data)} –∑–∞–ø–∏—Å—ñ–≤ –≤ {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ batch upsert –≤ {table_name}: {e}")
                await session.rollback()
                return False
    
    async def get_historical_data_optimized(self, 
                                          symbol_id: int, 
                                          interval_id: int, 
                                          days_back: int,
                                          use_cache: bool = True) -> pd.DataFrame:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        
        # –§–æ—Ä–º—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∑–∞–ø–∏—Ç –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏
        query = """
        SELECT 
            hd.data_id,
            hd.timestamp,
            hd.open,
            hd.high,
            hd.low,
            hd.close,
            hd.volume,
            hd.quote_av,
            hd.trades,
            hd.tb_base_av,
            hd.tb_quote_av
        FROM historical_data hd
        WHERE hd.symbol_id = :symbol_id 
        AND hd.interval_id = :interval_id
        AND hd.timestamp >= :start_time
        ORDER BY hd.timestamp ASC
        """
        
        start_time = datetime.now() - timedelta(days=days_back)
        params = {
            'symbol_id': symbol_id,
            'interval_id': interval_id,
            'start_time': start_time
        }
        
        return await self.execute_query_cached(query, params, use_cache)
    
    async def get_technical_indicators_batch(self, data_ids: List[int]) -> pd.DataFrame:
        """–ü–∞–∫–µ—Ç–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤"""
        if not data_ids:
            return pd.DataFrame()
        
        # –§–æ—Ä–º—É—î–º–æ –∑–∞–ø–∏—Ç –∑ IN clause –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è
        placeholders = ", ".join([":id" + str(i) for i in range(len(data_ids))])
        params = {f"id{i}": data_id for i, data_id in enumerate(data_ids)}
        
        query = f"""
        SELECT * FROM technical_indicators 
        WHERE data_id IN ({placeholders})
        ORDER BY data_id
        """
        
        return await self.execute_query_cached(query, params, use_cache=False)
    
    async def invalidate_cache_pattern(self, pattern: str):
        """–Ü–Ω–≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–µ—à—É –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º"""
        # –û—á–∏—â—É—î–º–æ memory cache (–≤—Å—ñ –∫–ª—é—á—ñ)
        if pattern == "*":
            self.memory_cache.clear()
        
        # –û—á–∏—â—É—î–º–æ Redis –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
        if self.use_redis:
            try:
                keys = self.redis_client.keys(pattern)
                if keys:
                    self.redis_client.delete(*keys)
                    logger.info(f"üóëÔ∏è –û—á–∏—â–µ–Ω–æ {len(keys)} –∫–ª—é—á—ñ–≤ –∑ Redis")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è Redis –∫–µ—à—É: {e}")
    
    async def get_cache_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        stats = {
            'memory_cache_size': len(self.memory_cache),
            'memory_cache_hits': getattr(self.memory_cache, 'hits', 0),
            'memory_cache_misses': getattr(self.memory_cache, 'misses', 0)
        }
        
        if self.use_redis:
            try:
                redis_info = self.redis_client.info('memory')
                stats['redis_memory_used'] = redis_info.get('used_memory_human', 'N/A')
                stats['redis_keys'] = self.redis_client.dbsize()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ Redis: {e}")
        
        return stats
    
    async def close(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –∑'—î–¥–Ω–∞–Ω—å"""
        await self.async_engine.dispose()
        self.sync_engine.dispose()
        
        if self.redis_pool:
            self.redis_pool.disconnect()

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –ë–î
db_manager = OptimizedDatabaseManager()

# –®–≤–∏–¥–∫—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
async def get_historical_data_from_db_async(symbol: str, interval: str, days_back: int, api_key: str = None, api_secret: str = None, skip_append: bool = False) -> pd.DataFrame:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    # –û—Ç—Ä–∏–º—É—î–º–æ ID —Å–∏–º–≤–æ–ª—É —Ç–∞ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
    symbol_id = await get_or_create_symbol_id(symbol)
    interval_id = await get_or_create_interval_id(interval)
    
    return await db_manager.get_historical_data_optimized(symbol_id, interval_id, days_back)

async def get_or_create_symbol_id(symbol: str) -> int:
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è ID —Å–∏–º–≤–æ–ª—É"""
    query = "INSERT INTO symbols (symbol) VALUES (:symbol) ON CONFLICT (symbol) DO NOTHING RETURNING symbol_id"
    result = await db_manager.execute_query_cached(query, {'symbol': symbol}, use_cache=True, cache_ttl=86400)
    
    if result.empty:
        # –°–∏–º–≤–æ–ª –≤–∂–µ —ñ—Å–Ω—É—î, –æ—Ç—Ä–∏–º—É—î–º–æ –π–æ–≥–æ ID
        query = "SELECT symbol_id FROM symbols WHERE symbol = :symbol"
        result = await db_manager.execute_query_cached(query, {'symbol': symbol}, use_cache=True, cache_ttl=86400)
    
    return int(result.iloc[0]['symbol_id'])

async def get_or_create_interval_id(interval: str) -> int:
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è ID —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É"""
    query = "INSERT INTO intervals (interval_name) VALUES (:interval) ON CONFLICT (interval_name) DO NOTHING RETURNING interval_id"
    result = await db_manager.execute_query_cached(query, {'interval': interval}, use_cache=True, cache_ttl=86400)
    
    if result.empty:
        # –Ü–Ω—Ç–µ—Ä–≤–∞–ª –≤–∂–µ —ñ—Å–Ω—É—î, –æ—Ç—Ä–∏–º—É—î–º–æ –π–æ–≥–æ ID
        query = "SELECT interval_id FROM intervals WHERE interval_name = :interval"
        result = await db_manager.execute_query_cached(query, {'interval': interval}, use_cache=True, cache_ttl=86400)
    
    return int(result.iloc[0]['interval_id'])

async def batch_insert_technical_indicators(indicators_data: List[Dict]) -> bool:
    """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤"""
    return await db_manager.batch_insert('technical_indicators', indicators_data)

async def batch_insert_historical_data(historical_data: List[Dict]) -> bool:
    """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    return await db_manager.batch_upsert(
        'historical_data', 
        historical_data, 
        ['symbol_id', 'interval_id', 'timestamp']
    )

# –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ –æ–±–≥–æ—Ä—Ç–∫–∏ –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
def get_historical_data_from_db(symbol: str, interval: str, days_back: int, api_key: str = None, api_secret: str = None, skip_append: bool = False) -> pd.DataFrame:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    return asyncio.run(get_historical_data_from_db_async(symbol, interval, days_back, api_key, api_secret, skip_append))

def insert_symbol(symbol: str) -> int:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ —Å–∏–º–≤–æ–ª—É"""
    return asyncio.run(get_or_create_symbol_id(symbol))

def insert_interval(interval: str) -> int:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É"""
    return asyncio.run(get_or_create_interval_id(interval))