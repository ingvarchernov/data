# -*- coding: utf-8 -*-
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö
–í–∫–ª—é—á–∞—î –ø—É–ª –∑'—î–¥–Ω–∞–Ω—å, –∫–µ—à—É–≤–∞–Ω–Ω—è, batch –æ–ø–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ—Å—Ç—å
"""
import asyncio
import logging
import os
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from contextlib import asynccontextmanager

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text, MetaData, Table, select, insert, update
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
import redis
from cachetools import TTLCache
import redis.asyncio as redis
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class OptimizedDatabaseManager:
    async def get_or_create_interval_id(self, interval: str) -> int:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ interval_id –¥–ª—è —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É —É —Ç–∞–±–ª–∏—Ü—ñ intervals
        """
        async with self.async_session_factory() as session:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î —ñ–Ω—Ç–µ—Ä–≤–∞–ª
            result = await session.execute(text("SELECT interval_id FROM intervals WHERE interval = :interval"), {"interval": interval})
            row = result.fetchone()
            if row:
                return row[0]
            # –Ø–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î ‚Äî —Å—Ç–≤–æ—Ä—é—î–º–æ
            result = await session.execute(text("INSERT INTO intervals (interval) VALUES (:interval) RETURNING interval_id"), {"interval": interval})
            new_id = result.fetchone()[0]
            await session.commit()
            return new_id
    async def get_or_create_symbol_id(self, symbol: str) -> int:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ symbol_id —Å–∏–º–≤–æ–ª—É —É —Ç–∞–±–ª–∏—Ü—ñ symbols
        """
        async with self.async_session_factory() as session:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î —Å–∏–º–≤–æ–ª
            result = await session.execute(text("SELECT symbol_id FROM symbols WHERE symbol = :symbol"), {"symbol": symbol})
            row = result.fetchone()
            if row:
                return row[0]
            # –Ø–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î ‚Äî —Å—Ç–≤–æ—Ä—é—î–º–æ
            result = await session.execute(text("INSERT INTO symbols (symbol) VALUES (:symbol) RETURNING symbol_id"), {"symbol": symbol})
            new_id = result.fetchone()[0]
            await session.commit()
            return new_id
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º —Ç–∞ –ø—É–ª–æ–º –∑'—î–¥–Ω–∞–Ω—å"""
    
    def __init__(self, 
                 db_url: str = None,
                 redis_url: str = "redis://localhost:6379",
                 use_redis: bool = True,
                 pool_size: int = 20,
                 max_overflow: int = 30,
                 cache_ttl: int = 3600):
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        if db_url is None:
            env_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT', 'DB_NAME']
            missing = [var for var in env_vars if not os.getenv(var)]
            if missing:
                raise ValueError(f"‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –¥–ª—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î: {missing}. –ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–¥–∞–π—Ç–µ —ó—Ö —É .env –∞–±–æ —á–µ—Ä–µ–∑ export.")
            db_url = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ db_url –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ —ñ–Ω—à–∏—Ö –º–µ—Ç–æ–¥–∞—Ö
        self.db_url = db_url
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π engine
        self.sync_engine = create_engine(
            db_url,
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π engine
        async_db_url = db_url.replace("postgresql://", "postgresql+asyncpg://")
        self.async_engine = create_async_engine(
            async_db_url,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        self.async_session_factory = async_sessionmaker(
            self.async_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        # –ö–µ—à—É–≤–∞–Ω–Ω—è
        self.use_redis = use_redis
        self.memory_cache = TTLCache(maxsize=1000, ttl=cache_ttl)
        self.redis_pool = None
        
        if use_redis:
            try:
                self.redis_client = redis.from_url(redis_url, max_connections=20, decode_responses=False)
                # –ù–µ —Ç–µ—Å—Ç—É—î–º–æ –∑'—î–¥–Ω–∞–Ω–Ω—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—ñ–¥ —á–∞—Å —ñ–º–ø–æ—Ä—Ç—É - —Ü–µ –º–æ–∂–µ —Å–ø—Ä–∏—á–∏–Ω–∏—Ç–∏ –ø—Ä–æ–±–ª–µ–º–∏
                logger.info("‚úÖ Redis –∫–ª—ñ—î–Ω—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ memory cache: {e}")
                self.use_redis = False
        
        # –ú–µ—Ç–∞–¥–∞–Ω—ñ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ —Ç–∞–±–ª–∏—Ü—å
        self.metadata = MetaData()
        self._load_table_metadata()
        
    def _load_table_metadata(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —Ç–∞–±–ª–∏—Ü—å (lazy loading)"""
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ –≤—Å—ñ—Ö —Ç–∞–±–ª–∏—Ü—å –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
            from sqlalchemy import create_engine
            sync_engine = create_engine(self.db_url.replace("postgresql+asyncpg://", "postgresql://"))
            with sync_engine.connect() as conn:
                self.metadata.reflect(bind=conn)
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω—ñ {len(self.metadata.tables)} —Ç–∞–±–ª–∏—Ü—å")
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {e}")
    
    def _generate_cache_key(self, query: str, params: Dict = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        key_data = f"{query}_{params or {}}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –∫–µ—à—É"""
        # –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ memory cache
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # –ü–æ—Ç—ñ–º Redis
        if self.use_redis:
            try:
                data = await self.redis_client.get(cache_key)
                if data:
                    result = pickle.loads(data)
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ memory cache –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É
                    self.memory_cache[cache_key] = result
                    return result
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∑ Redis: {e}")
        
        return None
    
    async def _set_cache(self, cache_key: str, data: Any, ttl: int = 3600):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤ –∫–µ—à"""
        # Memory cache
        self.memory_cache[cache_key] = data
        
        # Redis cache
        if self.use_redis:
            try:
                serialized_data = pickle.dumps(data)
                await self.redis_client.setex(cache_key, ttl, serialized_data)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ Redis: {e}")
    
    async def execute_query_cached(self, 
                                  query: str, 
                                  params: Dict = None, 
                                  use_cache: bool = True,
                                  cache_ttl: int = 3600) -> pd.DataFrame:
        """–í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        cache_key = self._generate_cache_key(query, params) if use_cache else None
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
        if use_cache and cache_key:
            cached_result = await self._get_from_cache(cache_key)
            if cached_result is not None:
                logger.debug(f"üéØ Cache hit –¥–ª—è –∑–∞–ø–∏—Ç—É: {query[:50]}...")
                return cached_result
        
        # –í–∏–∫–æ–Ω—É—î–º–æ –∑–∞–ø–∏—Ç
        async with self.async_session_factory() as session:
            try:
                result = await session.execute(text(query), params or {})
                df = pd.DataFrame(result.fetchall(), columns=result.keys())
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
                if use_cache and cache_key:
                    await self._set_cache(cache_key, df, cache_ttl)
                    logger.debug(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à")
                
                return df
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É: {e}")
                await session.rollback()
                raise
    
    async def batch_insert(self, table_name: str, data: List[Dict], batch_size: int = 1000) -> bool:
        """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–∏—Ö"""
        if not data:
            return True
            
        table = self.metadata.tables.get(table_name)
        if table is None:
            logger.error(f"‚ùå –¢–∞–±–ª–∏—Ü—è {table_name} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        async with self.async_session_factory() as session:
            try:
                # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ –±–∞—Ç—á—ñ
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await session.execute(insert(table), batch)
                    
                await session.commit()
                logger.info(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤ –≤ {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ—ó –≤—Å—Ç–∞–≤–∫–∏ –≤ {table_name}: {e}")
                await session.rollback()
                return False
    
    async def batch_upsert(self, table_name: str, data: List[Dict], conflict_columns: List[str], batch_size: int = 1000) -> bool:
        """–ü–∞–∫–µ—Ç–Ω–∏–π upsert (INSERT ... ON CONFLICT UPDATE)"""
        if not data:
            return True
            
        async with self.async_session_factory() as session:
            try:
                # –§–æ—Ä–º—É—î–º–æ –∑–∞–ø–∏—Ç –∑ ON CONFLICT
                columns = list(data[0].keys())
                values_placeholder = ", ".join([f":{col}" for col in columns])
                update_set = ", ".join([f"{col} = EXCLUDED.{col}" for col in columns if col not in conflict_columns])
                conflict_cols = ", ".join(conflict_columns)
                
                query = f"""
                INSERT INTO {table_name} ({', '.join(columns)})
                VALUES ({values_placeholder})
                ON CONFLICT ({conflict_cols})
                DO UPDATE SET {update_set}
                """
                
                # –í–∏–∫–æ–Ω—É—î–º–æ –±–∞—Ç—á–∞–º–∏
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await session.execute(text(query), batch)
                    
                await session.commit()
                logger.info(f"‚úÖ Upsert {len(data)} –∑–∞–ø–∏—Å—ñ–≤ –≤ {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ batch upsert –≤ {table_name}: {e}")
                await session.rollback()
                return False
    
    async def get_historical_data_optimized(self, 
                                          symbol_id: int, 
                                          interval_id: int, 
                                          days_back: int,
                                          use_cache: bool = True) -> pd.DataFrame:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        
        # –§–æ—Ä–º—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∑–∞–ø–∏—Ç –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏
        query = """
        SELECT 
            hd.data_id,
            hd.timestamp,
            hd.open,
            hd.high,
            hd.low,
            hd.close,
            hd.volume,
            hd.quote_av,
            hd.trades,
            hd.tb_base_av,
            hd.tb_quote_av
        FROM historical_data hd
        WHERE hd.symbol_id = :symbol_id 
        AND hd.interval_id = :interval_id
        AND hd.timestamp >= :start_time
        ORDER BY hd.timestamp ASC
        """
        
        start_time = datetime.now() - timedelta(days=days_back)
        params = {
            'symbol_id': symbol_id,
            'interval_id': interval_id,
            'start_time': start_time
        }
        
        df = await self.execute_query_cached(query, params, use_cache)
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∞–Ω–æ–º–∞–ª—ñ–π, —è–∫ —É —Å—Ç–∞—Ä–æ–º—É –ø—Ä–æ–µ–∫—Ç—ñ
        if not df.empty and 'close' in df.columns:
            logger.debug(f"–ü–æ—á–∞—Ç–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤: {len(df)}")
            df = df[(df['close'] > 0) & (df['close'].pct_change().abs() < 0.1)].copy()
            logger.debug(f"–ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π: {len(df)}")
        
        return df
    
    async def get_technical_indicators_batch(self, data_ids: List[int]) -> pd.DataFrame:
        """–ü–∞–∫–µ—Ç–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤"""
        if not data_ids:
            return pd.DataFrame()
        
        # –§–æ—Ä–º—É—î–º–æ –∑–∞–ø–∏—Ç –∑ IN clause –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è
        placeholders = ", ".join([":id" + str(i) for i in range(len(data_ids))])
        params = {f"id{i}": data_id for i, data_id in enumerate(data_ids)}
        
        query = f"""
        SELECT * FROM technical_indicators 
        WHERE data_id IN ({placeholders})
        ORDER BY data_id
        """
        
        return await self.execute_query_cached(query, params, use_cache=False)
    
    async def invalidate_cache_pattern(self, pattern: str):
        """–Ü–Ω–≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–µ—à—É –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º"""
        # –û—á–∏—â—É—î–º–æ memory cache (–≤—Å—ñ –∫–ª—é—á—ñ)
        if pattern == "*":
            self.memory_cache.clear()
        
        # –û—á–∏—â—É—î–º–æ Redis –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
        if self.use_redis:
            try:
                keys = self.redis_client.keys(pattern)
                if keys:
                    self.redis_client.delete(*keys)
                    logger.info(f"üóëÔ∏è –û—á–∏—â–µ–Ω–æ {len(keys)} –∫–ª—é—á—ñ–≤ –∑ Redis")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è Redis –∫–µ—à—É: {e}")
    
    async def get_cache_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        stats = {
            'memory_cache_size': len(self.memory_cache),
            'memory_cache_hits': getattr(self.memory_cache, 'hits', 0),
            'memory_cache_misses': getattr(self.memory_cache, 'misses', 0)
        }
        
        if self.use_redis:
            try:
                redis_info = self.redis_client.info('memory')
                stats['redis_memory_used'] = redis_info.get('used_memory_human', 'N/A')
                stats['redis_keys'] = self.redis_client.dbsize()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ Redis: {e}")
        
        return stats
    
    async def close(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –∑'—î–¥–Ω–∞–Ω—å"""
        await self.async_engine.dispose()
        self.sync_engine.dispose()
        
        if self.redis_pool:
            self.redis_pool.disconnect()

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –ë–î
db_manager = OptimizedDatabaseManager()

# –®–≤–∏–¥–∫—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
async def get_historical_data_from_db_async(symbol: str, interval: str, days_back: int, api_key: str = None, api_secret: str = None, skip_append: bool = False) -> pd.DataFrame:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    # –û—Ç—Ä–∏–º—É—î–º–æ ID —Å–∏–º–≤–æ–ª—É —Ç–∞ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
    symbol_id = await get_or_create_symbol_id(symbol)
    interval_id = await get_or_create_interval_id(interval)
    
    return await db_manager.get_historical_data_optimized(symbol_id, interval_id, days_back)

async def get_or_create_symbol_id(symbol: str) -> int:
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è ID —Å–∏–º–≤–æ–ª—É"""
    query = "INSERT INTO symbols (symbol) VALUES (:symbol) ON CONFLICT (symbol) DO NOTHING RETURNING symbol_id"
    result = await db_manager.execute_query_cached(query, {'symbol': symbol}, use_cache=True, cache_ttl=86400)
    
    if result.empty:
        # –°–∏–º–≤–æ–ª –≤–∂–µ —ñ—Å–Ω—É—î, –æ—Ç—Ä–∏–º—É—î–º–æ –π–æ–≥–æ ID
        query = "SELECT symbol_id FROM symbols WHERE symbol = :symbol"
        result = await db_manager.execute_query_cached(query, {'symbol': symbol}, use_cache=True, cache_ttl=86400)
    
    return int(result.iloc[0]['symbol_id'])

async def get_or_create_interval_id(interval: str) -> int:
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è ID —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É"""
    query = "INSERT INTO intervals (interval_name) VALUES (:interval) ON CONFLICT (interval_name) DO NOTHING RETURNING interval_id"
    result = await db_manager.execute_query_cached(query, {'interval': interval}, use_cache=True, cache_ttl=86400)
    
    if result.empty:
        # –Ü–Ω—Ç–µ—Ä–≤–∞–ª –≤–∂–µ —ñ—Å–Ω—É—î, –æ—Ç—Ä–∏–º—É—î–º–æ –π–æ–≥–æ ID
        query = "SELECT interval_id FROM intervals WHERE interval_name = :interval"
        result = await db_manager.execute_query_cached(query, {'interval': interval}, use_cache=True, cache_ttl=86400)
    
    return int(result.iloc[0]['interval_id'])

async def batch_insert_technical_indicators(indicators_data: List[Dict]) -> bool:
    """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤"""
    return await db_manager.batch_insert('technical_indicators', indicators_data)

async def batch_insert_historical_data(historical_data: List[Dict]) -> bool:
    """–ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    return await db_manager.batch_upsert(
        'historical_data', 
        historical_data, 
        ['symbol_id', 'interval_id', 'timestamp']
    )

# –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ –æ–±–≥–æ—Ä—Ç–∫–∏ –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
def get_historical_data_from_db(symbol: str, interval: str, days_back: int, api_key: str = None, api_secret: str = None, skip_append: bool = False) -> pd.DataFrame:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    return asyncio.run(get_historical_data_from_db_async(symbol, interval, days_back, api_key, api_secret, skip_append))

def insert_symbol(symbol: str) -> int:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ —Å–∏–º–≤–æ–ª—É"""
    return asyncio.run(get_or_create_symbol_id(symbol))

def insert_interval(interval: str) -> int:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É"""
    return asyncio.run(get_or_create_interval_id(interval))

async def save_technical_indicators_batch(db_manager, symbol: str, interval: str, df_with_indicators: pd.DataFrame):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö"""
    if df_with_indicators.empty:
        return
    
    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ ID —Å–∏–º–≤–æ–ª—É —Ç–∞ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
        symbol_id = await db_manager.get_or_create_symbol_id(symbol)
        interval_id = await db_manager.get_or_create_interval_id(interval)
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        indicators_data = []
        for idx, row in df_with_indicators.iterrows():
            # –ü—Ä–∏–ø—É—Å–∫–∞—î–º–æ, —â–æ df –º–∞—î –∫–æ–ª–æ–Ω–∫—É data_id –∞–±–æ timestamp
            # –Ø–∫—â–æ –Ω–µ–º–∞—î data_id, –∑–Ω–∞—Ö–æ–¥–∏–º–æ –π–æ–≥–æ –ø–æ timestamp
            if 'data_id' not in row:
                # –®—É–∫–∞—î–º–æ data_id –ø–æ timestamp
                query = """
                SELECT data_id FROM historical_data 
                WHERE symbol_id = :symbol_id AND interval_id = :interval_id 
                AND timestamp = :timestamp
                """
                result = await db_manager.execute_query_cached(
                    query, 
                    {'symbol_id': symbol_id, 'interval_id': interval_id, 'timestamp': row['timestamp']}, 
                    use_cache=False
                )
                if result.empty:
                    continue
                data_id = int(result.iloc[0]['data_id'])
            else:
                data_id = int(row['data_id'])
            
            # –§–æ—Ä–º—É—î–º–æ –∑–∞–ø–∏—Å –¥–ª—è technical_indicators
            indicator_record = {
                'data_id': data_id,
                'rsi': float(row.get('rsi', None)) if pd.notna(row.get('rsi')) else None,
                'macd': float(row.get('macd', None)) if pd.notna(row.get('macd')) else None,
                'macd_signal': float(row.get('macd_signal', None)) if pd.notna(row.get('macd_signal')) else None,
                'upper_band': float(row.get('upper_band', None)) if pd.notna(row.get('upper_band')) else None,
                'lower_band': float(row.get('lower_band', None)) if pd.notna(row.get('lower_band')) else None,
                'stoch': float(row.get('stoch', None)) if pd.notna(row.get('stoch')) else None,
                'stoch_signal': float(row.get('stoch_signal', None)) if pd.notna(row.get('stoch_signal')) else None,
                'ema': float(row.get('ema', None)) if pd.notna(row.get('ema')) else None,
                'atr': float(row.get('atr', None)) if pd.notna(row.get('atr')) else None,
                'cci': float(row.get('cci', None)) if pd.notna(row.get('cci')) else None,
                'obv': float(row.get('obv', None)) if pd.notna(row.get('obv')) else None,
                'volatility': float(row.get('volatility', None)) if pd.notna(row.get('volatility')) else None,
                'volume_pct': float(row.get('volume_pct', None)) if pd.notna(row.get('volume_pct')) else None,
                'close_lag1': float(row.get('close_lag1', None)) if pd.notna(row.get('close_lag1')) else None,
                'close_lag2': float(row.get('close_lag2', None)) if pd.notna(row.get('close_lag2')) else None,
                'close_diff': float(row.get('close_diff', None)) if pd.notna(row.get('close_diff')) else None,
                'log_return': float(row.get('log_return', None)) if pd.notna(row.get('log_return')) else None,
                'hour_norm': float(row.get('hour_norm', None)) if pd.notna(row.get('hour_norm')) else None,
                'day_norm': float(row.get('day_norm', None)) if pd.notna(row.get('day_norm')) else None,
                'adx': float(row.get('adx', None)) if pd.notna(row.get('adx')) else None,
                'vwap': float(row.get('vwap', None)) if pd.notna(row.get('vwap')) else None
            }
            indicators_data.append(indicator_record)
        
        # –ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞ –∑ upsert
        if indicators_data:
            await db_manager.batch_upsert('technical_indicators', indicators_data, ['data_id'])
            logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(indicators_data)} —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –¥–ª—è {symbol} {interval}")
    
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤: {e}")

async def save_normalized_data_batch(db_manager, symbol: str, interval: str, normalized_df: pd.DataFrame):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö"""
    if normalized_df.empty:
        logger.warning("‚ö†Ô∏è normalized_df –ø–æ—Ä–æ–∂–Ω—ñ–π")
        return
    
    logger.info(f"üî¢ –°–ø—Ä–æ–±–∞ –∑–±–µ—Ä–µ–≥—Ç–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ: {len(normalized_df)} —Ä—è–¥–∫—ñ–≤, –∫–æ–ª–æ–Ω–∫–∏: {list(normalized_df.columns)}")
    
    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ ID —Å–∏–º–≤–æ–ª—É —Ç–∞ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
        symbol_id = await db_manager.get_or_create_symbol_id(symbol)
        interval_id = await db_manager.get_or_create_interval_id(interval)
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        normalized_data = []
        saved_count = 0
        
        for idx, row in normalized_df.iterrows():
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ data_id –ø–æ timestamp
            timestamp = row.get('timestamp')
            if timestamp is None or pd.isna(timestamp):
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä—è–¥–æ–∫ {idx}: –≤—ñ–¥—Å—É—Ç–Ω—ñ–π timestamp")
                continue
                
            query = """
            SELECT data_id FROM historical_data 
            WHERE symbol_id = :symbol_id AND interval_id = :interval_id 
            AND timestamp = :timestamp
            """
            result = await db_manager.execute_query_cached(
                query, 
                {'symbol_id': symbol_id, 'interval_id': interval_id, 'timestamp': timestamp}, 
                use_cache=False
            )
            if result.empty:
                logger.warning(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ data_id –¥–ª—è timestamp {timestamp}")
                continue
            data_id = int(result.iloc[0]['data_id'])
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤—Å—ñ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ —è–∫ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ
            for col in normalized_df.columns:
                if col != 'timestamp' and col.endswith('_normalized') and pd.notna(row[col]):
                    normalized_data.append({
                        'data_id': data_id,
                        'feature': col.replace('_normalized', ''),  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–∑–≤—É –±–µ–∑ _normalized
                        'normalized_value': float(row[col])
                    })
                    saved_count += 1
        
        # –ü–∞–∫–µ—Ç–Ω–∞ –≤—Å—Ç–∞–≤–∫–∞
        if normalized_data:
            await db_manager.batch_upsert('normalized_data', normalized_data, ['data_id', 'feature'])
            logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(normalized_data)} –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –¥–ª—è {symbol} {interval} (–∑ {saved_count} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤)")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ normalized_data")
    
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
        import traceback
        logger.error(traceback.format_exc())

async def save_predictions(db_manager, symbol: str, interval: str, predictions: List[float], last_price: float):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö"""
    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ ID —Å–∏–º–≤–æ–ª—É —Ç–∞ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É
        symbol_id = await db_manager.get_or_create_symbol_id(symbol)
        interval_id = await db_manager.get_or_create_interval_id(interval)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–ø–∏—Å –ø—Ä–æ–≥–Ω–æ–∑—É
        prediction_record = {
            'symbol_id': symbol_id,
            'interval_id': interval_id,
            'timestamp': datetime.now(),
            'last_price': float(last_price),
            'predicted_price': float(predictions[0]) if predictions else None,
            'fold_1_prediction': float(predictions[0]) if len(predictions) > 0 else None,
            'fold_2_prediction': float(predictions[1]) if len(predictions) > 1 else None,
            'fold_3_prediction': float(predictions[2]) if len(predictions) > 2 else None,
            'fold_4_prediction': float(predictions[3]) if len(predictions) > 3 else None,
            'fold_5_prediction': float(predictions[4]) if len(predictions) > 4 else None
        }
        
        # –í—Å—Ç–∞–≤–∫–∞ –∑ upsert (—è–∫—â–æ –≤–∂–µ —î –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ü—é –º–∏—Ç—å, –æ–Ω–æ–≤–ª—é—î–º–æ)
        await db_manager.batch_upsert('predictions', [prediction_record], ['symbol_id', 'interval_id', 'timestamp'])
        logger.info(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {symbol} {interval}: {predictions[0]:.2f}")
    
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤: {e}")