# -*- coding: utf-8 -*-
"""
ÐžÐ¿Ñ‚Ð¸Ð¼Ñ–Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸ Ð· Ð±Ð°Ð·Ð¾ÑŽ Ð´Ð°Ð½Ð¸Ñ…
Ð’ÐºÐ»ÑŽÑ‡Ð°Ñ” Ð¿ÑƒÐ» Ð·'Ñ”Ð´Ð½Ð°Ð½ÑŒ, ÐºÐµÑˆÑƒÐ²Ð°Ð½Ð½Ñ, batch Ð¾Ð¿ÐµÑ€Ð°Ñ†Ñ–Ñ— Ñ‚Ð° Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ–ÑÑ‚ÑŒ
"""
import asyncio
import logging
import os
import pickle
import hashlib
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text, MetaData, Table, select, insert, update
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.pool import QueuePool
import redis.asyncio as redis
from cachetools import TTLCache
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)


class OptimizedDatabaseManager:
    """ÐžÐ¿Ñ‚Ð¸Ð¼Ñ–Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ… Ð· ÐºÐµÑˆÑƒÐ²Ð°Ð½Ð½ÑÐ¼ Ñ‚Ð° Ð¿ÑƒÐ»Ð¾Ð¼ Ð·'Ñ”Ð´Ð½Ð°Ð½ÑŒ"""
    
    def __init__(
        self, 
        db_url: str = None,
        redis_url: str = "redis://localhost:6379",
        use_redis: bool = True,
        pool_size: int = 20,
        max_overflow: int = 30,
        cache_ttl: int = 3600
    ):
        """
        Ð†Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° Ð‘Ð”
        
        Args:
            db_url: URL Ð¿Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ Ð´Ð¾ PostgreSQL
            redis_url: URL Ð¿Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ Ð´Ð¾ Redis
            use_redis: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸ Redis Ð´Ð»Ñ ÐºÐµÑˆÑƒÐ²Ð°Ð½Ð½Ñ
            pool_size: Ð Ð¾Ð·Ð¼Ñ–Ñ€ Ð¿ÑƒÐ»Ñƒ Ð·'Ñ”Ð´Ð½Ð°Ð½ÑŒ
            max_overflow: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð° ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ð´Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ð¸Ñ… Ð·'Ñ”Ð´Ð½Ð°Ð½ÑŒ
            cache_ttl: Ð§Ð°Ñ Ð¶Ð¸Ñ‚Ñ‚Ñ ÐºÐµÑˆÑƒ (ÑÐµÐºÑƒÐ½Ð´Ð¸)
        """
        # ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ…
        if db_url is None:
            env_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT', 'DB_NAME']
            missing = [var for var in env_vars if not os.getenv(var)]
            if missing:
                raise ValueError(
                    f"âŒ Ð’Ñ–Ð´ÑÑƒÑ‚Ð½Ñ– Ð·Ð¼Ñ–Ð½Ð½Ñ– ÑÐµÑ€ÐµÐ´Ð¾Ð²Ð¸Ñ‰Ð° Ð´Ð»Ñ Ð¿Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ Ð´Ð¾ Ð‘Ð”: {missing}. "
                    f"Ð‘ÑƒÐ´ÑŒ Ð»Ð°ÑÐºÐ°, Ð·Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ñ—Ñ… Ñƒ .env Ð°Ð±Ð¾ Ñ‡ÐµÑ€ÐµÐ· export."
                )
            db_url = (
                f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
                f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
            )
        
        self.db_url = db_url
        
        # Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¸Ð¹ engine
        self.sync_engine = create_engine(
            db_url,
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        # ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¸Ð¹ engine
        async_db_url = db_url.replace("postgresql://", "postgresql+asyncpg://")
        self.async_engine = create_async_engine(
            async_db_url,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
        
        self.async_session_factory = async_sessionmaker(
            self.async_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        # ÐšÐµÑˆÑƒÐ²Ð°Ð½Ð½Ñ
        self.use_redis = use_redis
        self.memory_cache = TTLCache(maxsize=1000, ttl=cache_ttl)
        self.redis_client = None
        self.cache_ttl = cache_ttl
        
        if use_redis:
            try:
                self.redis_client = redis.from_url(
                    redis_url, 
                    max_connections=20, 
                    decode_responses=False
                )
                logger.info("âœ… Redis ÐºÐ»Ñ–Ñ”Ð½Ñ‚ ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¾")
            except Exception as e:
                logger.warning(f"âš ï¸ Redis Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ñ‚ÑŒÑÑ Ñ‚Ñ–Ð»ÑŒÐºÐ¸ memory cache: {e}")
                self.use_redis = False
        
        # ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ñ– Ð´Ð»Ñ ÑˆÐ²Ð¸Ð´ÐºÐ¾Ð³Ð¾ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ñƒ Ð´Ð¾ Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÑŒ
        self.metadata = MetaData()
        self._metadata_loaded = False
        
        logger.info("âœ… OptimizedDatabaseManager Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ð¾")
    
    async def initialize(self):
        """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð° Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ (Ñ‚ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Ð·'Ñ”Ð´Ð½Ð°Ð½ÑŒ)"""
        try:
            # Ð¢ÐµÑÑ‚ PostgreSQL
            async with self.async_session_factory() as session:
                await session.execute(text("SELECT 1"))
            logger.info("âœ… PostgreSQL Ð·'Ñ”Ð´Ð½Ð°Ð½Ð½Ñ ÑƒÑÐ¿Ñ–ÑˆÐ½Ðµ")
            
            # Ð¢ÐµÑÑ‚ Redis
            if self.use_redis and self.redis_client:
                try:
                    await self.redis_client.ping()
                    logger.info("âœ… Redis Ð·'Ñ”Ð´Ð½Ð°Ð½Ð½Ñ ÑƒÑÐ¿Ñ–ÑˆÐ½Ðµ")
                except Exception as e:
                    logger.warning(f"âš ï¸ Redis ping failed: {e}")
                    self.use_redis = False
            
            # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð¸Ñ…
            self._load_table_metadata()
            
        except Exception as e:
            logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ— Ð‘Ð”: {e}")
            raise
    
    def _load_table_metadata(self):
        """Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð¸Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÑŒ (ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾ Ð¿Ñ€Ð¸ Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ—)"""
        if self._metadata_loaded:
            return
        
        try:
            with self.sync_engine.connect() as conn:
                self.metadata.reflect(bind=conn)
            self._metadata_loaded = True
            logger.info(f"âœ… Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¾ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ñ– {len(self.metadata.tables)} Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÑŒ")
        except Exception as e:
            logger.warning(f"âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð¸Ñ…: {e}")
    
    def _generate_cache_key(self, query: str, params: Dict = None) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ ÐºÐ»ÑŽÑ‡Ð° Ð´Ð»Ñ ÐºÐµÑˆÑƒÐ²Ð°Ð½Ð½Ñ"""
        key_data = f"{query}_{params or {}}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð· ÐºÐµÑˆÑƒ"""
        # Memory cache
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # Redis cache
        if self.use_redis and self.redis_client:
            try:
                data = await self.redis_client.get(cache_key)
                if data:
                    result = pickle.loads(data)
                    self.memory_cache[cache_key] = result
                    return result
            except Exception as e:
                logger.warning(f"âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ñ‡Ð¸Ñ‚Ð°Ð½Ð½Ñ Ð· Redis: {e}")
        
        return None
    
    async def _set_cache(self, cache_key: str, data: Any, ttl: int = None):
        """Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð² ÐºÐµÑˆ"""
        if ttl is None:
            ttl = self.cache_ttl
        
        # Memory cache
        self.memory_cache[cache_key] = data
        
        # Redis cache
        if self.use_redis and self.redis_client:
            try:
                serialized_data = pickle.dumps(data)
                await self.redis_client.setex(cache_key, ttl, serialized_data)
            except Exception as e:
                logger.warning(f"âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÑƒ Ð² Redis: {e}")
    
    async def execute_query_cached(
        self, 
        query: str, 
        params: Dict = None, 
        use_cache: bool = True,
        cache_ttl: int = None
    ) -> pd.DataFrame:
        """Ð’Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ Ð· ÐºÐµÑˆÑƒÐ²Ð°Ð½Ð½ÑÐ¼"""
        cache_key = self._generate_cache_key(query, params) if use_cache else None
        
        # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° ÐºÐµÑˆÑƒ
        if use_cache and cache_key:
            cached_result = await self._get_from_cache(cache_key)
            if cached_result is not None:
                logger.debug(f"ðŸŽ¯ Cache hit: {query[:50]}...")
                return cached_result
        
        # Ð’Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ
        async with self.async_session_factory() as session:
            try:
                result = await session.execute(text(query), params or {})
                df = pd.DataFrame(result.fetchall(), columns=result.keys())
                
                # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð² ÐºÐµÑˆ
                if use_cache and cache_key:
                    await self._set_cache(cache_key, df, cache_ttl)
                    logger.debug(f"ðŸ’¾ Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾ Ð² ÐºÐµÑˆ")
                
                return df
                
            except Exception as e:
                logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ: {e}")
                await session.rollback()
                raise
    
    async def get_or_create_symbol_id(self, symbol: str) -> int:
        """ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ Ð°Ð±Ð¾ ÑÑ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ symbol_id"""
        async with self.async_session_factory() as session:
            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ñ–ÑÐ½ÑƒÐ²Ð°Ð½Ð½Ñ
            result = await session.execute(
                text("SELECT symbol_id FROM symbols WHERE symbol = :symbol"),
                {"symbol": symbol}
            )
            row = result.fetchone()
            
            if row:
                return row[0]
            
            # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾
            result = await session.execute(
                text("INSERT INTO symbols (symbol) VALUES (:symbol) RETURNING symbol_id"),
                {"symbol": symbol}
            )
            new_id = result.fetchone()[0]
            await session.commit()
            
            logger.debug(f"âœ… Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð¾ symbol_id={new_id} Ð´Ð»Ñ {symbol}")
            return new_id
    
    async def get_or_create_interval_id(self, interval: str) -> int:
        """ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ Ð°Ð±Ð¾ ÑÑ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ interval_id"""
        async with self.async_session_factory() as session:
            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ñ–ÑÐ½ÑƒÐ²Ð°Ð½Ð½Ñ
            result = await session.execute(
                text("SELECT interval_id FROM intervals WHERE interval = :interval"),
                {"interval": interval}
            )
            row = result.fetchone()
            
            if row:
                return row[0]
            
            # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾
            result = await session.execute(
                text("INSERT INTO intervals (interval) VALUES (:interval) RETURNING interval_id"),
                {"interval": interval}
            )
            new_id = result.fetchone()[0]
            await session.commit()
            
            logger.debug(f"âœ… Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð¾ interval_id={new_id} Ð´Ð»Ñ {interval}")
            return new_id
    
    async def batch_insert(
        self, 
        table_name: str, 
        data: List[Dict], 
        batch_size: int = 1000
    ) -> bool:
        """ÐŸÐ°ÐºÐµÑ‚Ð½Ð° Ð²ÑÑ‚Ð°Ð²ÐºÐ° Ð´Ð°Ð½Ð¸Ñ…"""
        if not data:
            return True
        
        if not self._metadata_loaded:
            self._load_table_metadata()
        
        table = self.metadata.tables.get(table_name)
        if table is None:
            logger.error(f"âŒ Ð¢Ð°Ð±Ð»Ð¸Ñ†Ñ {table_name} Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
            return False
        
        async with self.async_session_factory() as session:
            try:
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await session.execute(insert(table), batch)
                
                await session.commit()
                logger.info(f"âœ… Ð’ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ {len(data)} Ð·Ð°Ð¿Ð¸ÑÑ–Ð² Ð² {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ð°ÐºÐµÑ‚Ð½Ð¾Ñ— Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ð² {table_name}: {e}")
                await session.rollback()
                return False
    
    async def batch_upsert(
        self, 
        table_name: str, 
        data: List[Dict], 
        conflict_columns: List[str], 
        batch_size: int = 1000
    ) -> bool:
        """ÐŸÐ°ÐºÐµÑ‚Ð½Ð¸Ð¹ upsert (INSERT ... ON CONFLICT UPDATE)"""
        if not data:
            return True
        
        async with self.async_session_factory() as session:
            try:
                columns = list(data[0].keys())
                values_placeholder = ", ".join([f":{col}" for col in columns])
                update_set = ", ".join([
                    f"{col} = EXCLUDED.{col}" 
                    for col in columns 
                    if col not in conflict_columns
                ])
                conflict_cols = ", ".join(conflict_columns)
                
                query = f"""
                INSERT INTO {table_name} ({', '.join(columns)})
                VALUES ({values_placeholder})
                ON CONFLICT ({conflict_cols})
                DO UPDATE SET {update_set}
                """
                
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    for record in batch:
                        await session.execute(text(query), record)
                
                await session.commit()
                logger.info(f"âœ… Upsert {len(data)} Ð·Ð°Ð¿Ð¸ÑÑ–Ð² Ð² {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° batch upsert Ð² {table_name}: {e}")
                await session.rollback()
                return False
    
    async def get_historical_data_optimized(
        self, 
        symbol_id: int, 
        interval_id: int, 
        days_back: int,
        use_cache: bool = True
    ) -> pd.DataFrame:
        """ÐžÐ¿Ñ‚Ð¸Ð¼Ñ–Ð·Ð¾Ð²Ð°Ð½Ðµ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ñ–ÑÑ‚Ð¾Ñ€Ð¸Ñ‡Ð½Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ…"""
        start_time = datetime.now() - timedelta(days=days_back)
        
        query = """
        SELECT 
            hd.data_id,
            hd.timestamp,
            hd.open,
            hd.high,
            hd.low,
            hd.close,
            hd.volume,
            hd.quote_av,
            hd.trades,
            hd.tb_base_av,
            hd.tb_quote_av
        FROM historical_data hd
        WHERE hd.symbol_id = :symbol_id 
        AND hd.interval_id = :interval_id
        AND hd.timestamp >= :start_time
        ORDER BY hd.timestamp ASC
        """
        
        params = {
            'symbol_id': symbol_id,
            'interval_id': interval_id,
            'start_time': start_time
        }
        
        df = await self.execute_query_cached(query, params, use_cache)
        
        if not df.empty:
            logger.info(
                f"ðŸ” ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð¾ {len(df)} Ñ€ÑÐ´ÐºÑ–Ð² Ð´Ð»Ñ "
                f"symbol_id={symbol_id}, interval_id={interval_id}"
            )
        else:
            logger.warning(
                f"âš ï¸ ÐŸÐ¾Ñ€Ð¾Ð¶Ð½Ñ–Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð´Ð»Ñ "
                f"symbol_id={symbol_id}, interval_id={interval_id}"
            )
        
        return df
    
    async def invalidate_cache_pattern(self, pattern: str):
        """Ð†Ð½Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ ÐºÐµÑˆÑƒ Ð·Ð° Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð¼"""
        if pattern == "*":
            self.memory_cache.clear()
            logger.info("ðŸ—‘ï¸ Memory cache Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð¾")
        
        if self.use_redis and self.redis_client:
            try:
                cursor = 0
                keys_deleted = 0
                
                while True:
                    cursor, keys = await self.redis_client.scan(
                        cursor=cursor, 
                        match=pattern, 
                        count=100
                    )
                    
                    if keys:
                        await self.redis_client.delete(*keys)
                        keys_deleted += len(keys)
                    
                    if cursor == 0:
                        break
                
                if keys_deleted > 0:
                    logger.info(f"ðŸ—‘ï¸ Ð’Ð¸Ð´Ð°Ð»ÐµÐ½Ð¾ {keys_deleted} ÐºÐ»ÑŽÑ‡Ñ–Ð² Ð· Redis")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ Redis: {e}")
    
    async def get_cache_stats(self) -> Dict:
        """Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐµÑˆÑƒÐ²Ð°Ð½Ð½Ñ"""
        stats = {
            'memory_cache_size': len(self.memory_cache),
            'memory_cache_maxsize': self.memory_cache.maxsize,
            'redis_available': self.use_redis
        }
        
        if self.use_redis and self.redis_client:
            try:
                info = await self.redis_client.info('memory')
                stats['redis_memory_used'] = info.get('used_memory_human', 'N/A')
                stats['redis_keys'] = await self.redis_client.dbsize()
            except Exception as e:
                logger.warning(f"âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Redis: {e}")
        
        return stats
    
    async def close(self):
        """Ð—Ð°ÐºÑ€Ð¸Ñ‚Ñ‚Ñ Ð·'Ñ”Ð´Ð½Ð°Ð½ÑŒ"""
        try:
            await self.async_engine.dispose()
            self.sync_engine.dispose()
            
            if self.redis_client:
                await self.redis_client.close()
            
            logger.info("âœ… Ð‘Ð” Ð·'Ñ”Ð´Ð½Ð°Ð½Ð½Ñ Ð·Ð°ÐºÑ€Ð¸Ñ‚Ð¾")
        except Exception as e:
            logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð°ÐºÑ€Ð¸Ñ‚Ñ‚Ñ Ð‘Ð”: {e}")


# ============================================================================
# Ð¢ÐžÐ Ð“ÐžÐ’Ð† Ð¤Ð£ÐÐšÐ¦Ð†Ð‡
# ============================================================================

async def save_trading_signal(db_manager, signal_data: Dict[str, Any]) -> int:
    """Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ‚Ð¾Ñ€Ð³Ð¾Ð²Ð¾Ð³Ð¾ ÑÐ¸Ð³Ð½Ð°Ð»Ñƒ"""
    try:
        signal_record = {
            'symbol': signal_data['symbol'],
            'action': signal_data['action'],
            'confidence': float(signal_data['confidence']),
            'entry_price': float(signal_data['entry_price']),
            'stop_loss': float(signal_data['stop_loss']) if signal_data.get('stop_loss') else None,
            'take_profit': float(signal_data['take_profit']) if signal_data.get('take_profit') else None,
            'quantity': float(signal_data['quantity']),
            'strategy': signal_data.get('strategy', 'unknown'),
            'prediction_source': signal_data.get('prediction_source', 'technical'),
            'status': signal_data.get('status', 'generated'),
            'notes': signal_data.get('notes', '')
        }
        
        async with db_manager.async_session_factory() as session:
            result = await session.execute(
                text("""
                    INSERT INTO trading_signals 
                    (symbol, action, confidence, entry_price, stop_loss, take_profit, 
                     quantity, strategy, prediction_source, status, notes) 
                    VALUES 
                    (:symbol, :action, :confidence, :entry_price, :stop_loss, :take_profit, 
                     :quantity, :strategy, :prediction_source, :status, :notes) 
                    RETURNING id
                """),
                signal_record
            )
            signal_id = result.fetchone()[0]
            await session.commit()
            
        logger.info(f"âœ… Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾ ÑÐ¸Ð³Ð½Ð°Ð»: {signal_data['symbol']} {signal_data['action']}")
        return signal_id
        
    except Exception as e:
        logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ ÑÐ¸Ð³Ð½Ð°Ð»Ñƒ: {e}")
        return None


async def save_position(db_manager, position_data: Dict[str, Any]) -> int:
    """Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð²Ñ–Ð´ÐºÑ€Ð¸Ñ‚Ð¾Ñ— Ð¿Ð¾Ð·Ð¸Ñ†Ñ–Ñ—"""
    try:
        logger.info(f"ðŸ”„ Ð¡Ð¿Ñ€Ð¾Ð±Ð° Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð¿Ð¾Ð·Ð¸Ñ†Ñ–Ñ—: {position_data.get('symbol')} {position_data.get('side')}")
        
        position_record = {
            'symbol': position_data['symbol'],
            'side': position_data['side'],
            'entry_price': float(position_data['entry_price']),
            'quantity': float(position_data['quantity']),
            'stop_loss': float(position_data['stop_loss']) if position_data.get('stop_loss') else None,
            'take_profit': float(position_data['take_profit']) if position_data.get('take_profit') else None,
            'strategy': position_data.get('strategy', 'unknown'),
            'status': position_data.get('status', 'open'),
            'signal_id': position_data.get('signal_id'),
            'metadata': json.dumps(position_data.get('metadata', {}))
        }
        
        logger.info(f"ðŸ“ Position record: entry_price={position_record['entry_price']}, quantity={position_record['quantity']}")
        
        async with db_manager.async_session_factory() as session:
            result = await session.execute(
                text("""
                    INSERT INTO positions 
                    (symbol, side, entry_price, quantity, stop_loss, take_profit, 
                     strategy, status, signal_id, metadata) 
                    VALUES 
                    (:symbol, :side, :entry_price, :quantity, :stop_loss, :take_profit, 
                     :strategy, :status, :signal_id, :metadata) 
                    RETURNING id
                """),
                position_record
            )
            position_id = result.fetchone()[0]
            await session.commit()
            
        logger.info(f"âœ… Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾ Ð¿Ð¾Ð·Ð¸Ñ†Ñ–ÑŽ: {position_data['symbol']} {position_data['side']} (ID: {position_id})")
        return position_id
        
    except Exception as e:
        logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð¿Ð¾Ð·Ð¸Ñ†Ñ–Ñ—: {e}", exc_info=True)
        return None


async def save_trade(db_manager, trade_data: Dict[str, Any]) -> int:
    """Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð·Ð°ÐºÑ€Ð¸Ñ‚Ð¾Ñ— ÑƒÐ³Ð¾Ð´Ð¸"""
    try:
        entry_price = float(trade_data['entry_price'])
        exit_price = float(trade_data['exit_price'])
        quantity = float(trade_data['quantity'])
        fees = float(trade_data.get('fees', 0))
        
        # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº P&L
        if entry_price == 0:
            logger.warning(f"âš ï¸ Entry price = 0 Ð´Ð»Ñ {trade_data['symbol']}")
            pnl = -fees
            pnl_percentage = 0
        elif trade_data['side'] == 'LONG':
            gross_pnl = (exit_price - entry_price) * quantity
            pnl = gross_pnl - fees
            pnl_percentage = ((exit_price - entry_price) / entry_price) * 100
        else:  # SHORT
            gross_pnl = (entry_price - exit_price) * quantity
            pnl = gross_pnl - fees
            pnl_percentage = ((entry_price - exit_price) / entry_price) * 100
        
        trade_record = {
            'symbol': trade_data['symbol'],
            'side': trade_data['side'],
            'entry_price': entry_price,
            'exit_price': exit_price,
            'quantity': quantity,
            'entry_time': trade_data['entry_time'],
            'exit_time': trade_data.get('exit_time', datetime.now()),
            'pnl': pnl,
            'pnl_percentage': pnl_percentage,
            'strategy': trade_data.get('strategy', 'unknown'),
            'exit_reason': trade_data.get('exit_reason', 'manual'),
            'position_id': trade_data.get('position_id'),
            'signal_id': trade_data.get('signal_id'),
            'fees': fees,
            'metadata': json.dumps(trade_data.get('metadata', {}))
        }
        
        async with db_manager.async_session_factory() as session:
            result = await session.execute(
                text("""
                    INSERT INTO trades 
                    (symbol, side, entry_price, exit_price, quantity, entry_time, exit_time, 
                     pnl, pnl_percentage, strategy, exit_reason, position_id, signal_id, fees, metadata) 
                    VALUES 
                    (:symbol, :side, :entry_price, :exit_price, :quantity, :entry_time, :exit_time, 
                     :pnl, :pnl_percentage, :strategy, :exit_reason, :position_id, :signal_id, :fees, :metadata) 
                    RETURNING id
                """),
                trade_record
            )
            trade_id = result.fetchone()[0]
            await session.commit()
            
        logger.info(
            f"âœ… Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾ ÑƒÐ³Ð¾Ð´Ñƒ: {trade_data['symbol']} {trade_data['side']} "
            f"P&L: ${pnl:.2f} ({pnl_percentage:.2f}%)"
        )
        return trade_id
        
    except Exception as e:
        logger.error(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ ÑƒÐ³Ð¾Ð´Ð¸: {e}")
        return None


# ============================================================================
# Ð“Ð›ÐžÐ‘ÐÐ›Ð¬ÐÐ˜Ð™ ÐœÐ•ÐÐ•Ð”Ð–Ð•Ð 
# ============================================================================

db_manager = OptimizedDatabaseManager()


# ============================================================================
# Ð¤Ð£ÐÐšÐ¦Ð†Ð‡ Ð”Ð›Ð¯ Ð—Ð’ÐžÐ ÐžÐ¢ÐÐžÐ‡ Ð¡Ð£ÐœÐ†Ð¡ÐÐžÐ¡Ð¢Ð†
# ============================================================================

async def get_historical_data_from_db_async(
    symbol: str, 
    interval: str, 
    days_back: int, 
    api_key: str = None, 
    api_secret: str = None, 
    skip_append: bool = False
) -> pd.DataFrame:
    """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ðµ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ñ–ÑÑ‚Ð¾Ñ€Ð¸Ñ‡Ð½Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ…"""
    symbol_id = await db_manager.get_or_create_symbol_id(symbol)
    interval_id = await db_manager.get_or_create_interval_id(interval)
    
    return await db_manager.get_historical_data_optimized(
        symbol_id, interval_id, days_back
    )


def get_historical_data_from_db(
    symbol: str, 
    interval: str, 
    days_back: int, 
    api_key: str = None, 
    api_secret: str = None, 
    skip_append: bool = False
) -> pd.DataFrame:
    """Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð° Ð¾Ð±Ð³Ð¾Ñ€Ñ‚ÐºÐ°"""
    return asyncio.run(
        get_historical_data_from_db_async(
            symbol, interval, days_back, api_key, api_secret, skip_append
        )
    )