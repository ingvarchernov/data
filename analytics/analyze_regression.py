#!/usr/bin/env python3
"""
–ê–Ω–∞–ª—ñ–∑ —ñ—Å–Ω—É—é—á–æ—ó –º–æ–¥–µ–ª—ñ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–æ—Ä–≥–æ–≤–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤
"""
import sys
import os
import asyncio
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import tensorflow as tf

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from training.rust_features import RustFeatureEngineer
from selected_features import SELECTED_FEATURES
from gpu_config import configure_gpu
from unified_binance_loader import UnifiedBinanceLoader
from dotenv import load_dotenv
import joblib

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è GPU
configure_gpu()
load_dotenv()


class ModelAnalyzer:
    """–ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª—ñ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–æ—Ä–≥–æ–≤–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤"""
    
    def __init__(self, symbol: str = 'BTCUSDT', model_dir: str = 'models/optimized_BTC'):
        self.symbol = symbol
        self.model_dir = model_dir
        self.model = None
        self.scaler = None
        self.feature_engineer = RustFeatureEngineer(use_rust=True)
        
        # Binance loader
        api_key = os.getenv('FUTURES_API_KEY')
        api_secret = os.getenv('FUTURES_API_SECRET')
        use_testnet = os.getenv('USE_TESTNET', 'false').lower() in ('true', '1', 'yes')
        
        self.data_loader = UnifiedBinanceLoader(
            api_key=api_key,
            api_secret=api_secret,
            testnet=use_testnet,
            use_public_data=True  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—É–±–ª—ñ—á–Ω—ñ –¥–∞–Ω—ñ —è–∫—â–æ API –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–µ
        )
        
    async def load_model(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç–∞ scaler"""
        logger.info(f"üì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ {self.model_dir}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        model_path = os.path.join(self.model_dir, 'best_model.h5')
        if os.path.exists(model_path):
            self.model = tf.keras.models.load_model(model_path)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {self.model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")
        else:
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è scaler
        scaler_path = os.path.join(self.model_dir, 'scaler.pkl')
        if os.path.exists(scaler_path):
            self.scaler = joblib.load(scaler_path)
            logger.info(f"‚úÖ Scaler –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        else:
            logger.warning("‚ö†Ô∏è Scaler –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    async def load_recent_data(self, days: int = 7):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –¥–∞–Ω–∏—Ö"""
        logger.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ {days} –¥–Ω—ñ–≤ –¥–ª—è {self.symbol}")
        
        df = await self.data_loader.get_historical_data(
            symbol=self.symbol,
            interval='1h',
            days_back=days
        )
        
        if df is None or df.empty:
            raise ValueError("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ")
        
        logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
        return df
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features"""
        logger.info("üîß –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ features...")
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        df = self.feature_engineer.calculate_all(
            df,
            sma_periods=[10, 20, 50],
            ema_periods=[12, 20, 26, 50],
            rsi_periods=[7, 14, 28],
            atr_periods=[7, 14, 21]
        )
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —Å–∫—ñ–ª—å–∫–∏ features –æ—á—ñ–∫—É—î scaler
        expected_features = self.scaler.n_features_in_ if self.scaler else len(SELECTED_FEATURES)
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à—ñ N features —è–∫—ñ –æ—á—ñ–∫—É—î –º–æ–¥–µ–ª—å
        features_to_use = SELECTED_FEATURES[:expected_features]
        
        # –í—ñ–¥–±—ñ—Ä features
        available_features = [f for f in features_to_use if f in df.columns]
        missing_features = [f for f in features_to_use if f not in df.columns]
        
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ features: {missing_features[:5]}...")
        
        logger.info(f"‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è {len(available_features)}/{expected_features} features")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ close –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        df_features = df[available_features].copy()
        df_features = df_features.dropna()
        
        return df_features, df['close'].iloc[-len(df_features):]
    
    def create_sequences(self, X: np.ndarray, sequence_length: int = 60):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM"""
        if len(X) < sequence_length:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: {len(X)} < {sequence_length}")
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        if self.scaler:
            X_scaled = self.scaler.transform(X)
        else:
            X_scaled = X
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
        sequences = []
        for i in range(len(X_scaled) - sequence_length + 1):
            sequences.append(X_scaled[i:i + sequence_length])
        
        return np.array(sequences)
    
    async def predict(self, X_sequences: np.ndarray) -> np.ndarray:
        """–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è"""
        logger.info(f"üîÆ –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–ª—è {len(X_sequences)} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π...")
        
        predictions = self.model.predict(X_sequences, verbose=0)
        
        logger.info(f"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑–∏ –æ—Ç—Ä–∏–º–∞–Ω—ñ")
        return predictions.flatten()
    
    def generate_signals(self, predictions: np.ndarray, prices: pd.Series, threshold: float = 0.5):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–æ—Ä–≥–æ–≤–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤"""
        logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–æ—Ä–≥–æ–≤–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤...")
        
        signals = []
        sequence_length = 60
        
        # predictions –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –æ—Å—Ç–∞–Ω–Ω—ñ–º –¥–∞–Ω–∏–º
        actual_prices = prices.iloc[-len(predictions):].values
        
        for i, (pred, price) in enumerate(zip(predictions, actual_prices)):
            # –ü—Ä–æ–≥–Ω–æ–∑ –≤—ñ–¥–Ω–æ—Å–Ω–æ—ó –∑–º—ñ–Ω–∏ —Ü—ñ–Ω–∏
            predicted_change = pred  # –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑—É—î –≤—ñ–¥–Ω–æ—Å–Ω—É –∑–º—ñ–Ω—É
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–∏–≥–Ω–∞–ª—É
            if predicted_change > threshold:
                signal = "BUY"
                strength = min(predicted_change / threshold, 3.0)
            elif predicted_change < -threshold:
                signal = "SELL"
                strength = min(abs(predicted_change) / threshold, 3.0)
            else:
                signal = "HOLD"
                strength = 0.0
            
            signals.append({
                'index': i,
                'current_price': price,
                'predicted_change': predicted_change,
                'predicted_change_pct': predicted_change * 100,
                'signal': signal,
                'strength': strength,
                'confidence': min(abs(predicted_change), 1.0)
            })
        
        return pd.DataFrame(signals)
    
    def print_analysis(self, signals_df: pd.DataFrame, recent_n: int = 20):
        """–í–∏–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª—ñ–∑"""
        logger.info("\n" + "="*80)
        logger.info("üìà –ê–ù–ê–õ–Ü–ó –ú–û–î–ï–õ–Ü –¢–ê –¢–û–†–ì–û–í–Ü –°–ò–ì–ù–ê–õ–ò")
        logger.info("="*80)
        
        logger.info(f"\nüéØ –û—Å—Ç–∞–Ω–Ω—ñ {recent_n} –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤:")
        logger.info("-" * 80)
        
        recent = signals_df.tail(recent_n)
        
        for _, row in recent.iterrows():
            signal_emoji = {
                'BUY': 'üü¢',
                'SELL': 'üî¥',
                'HOLD': '‚ö™'
            }[row['signal']]
            
            logger.info(
                f"{signal_emoji} {row['signal']:4s} | "
                f"–¶—ñ–Ω–∞: ${row['current_price']:.2f} | "
                f"–ü—Ä–æ–≥–Ω–æ–∑: {row['predicted_change_pct']:+.2f}% | "
                f"–°–∏–ª–∞: {'‚ñà' * int(row['strength'])} {row['strength']:.2f}"
            )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("\n" + "="*80)
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("="*80)
        
        buy_signals = len(signals_df[signals_df['signal'] == 'BUY'])
        sell_signals = len(signals_df[signals_df['signal'] == 'SELL'])
        hold_signals = len(signals_df[signals_df['signal'] == 'HOLD'])
        
        logger.info(f"üü¢ BUY —Å–∏–≥–Ω–∞–ª—ñ–≤:  {buy_signals:4d} ({buy_signals/len(signals_df)*100:.1f}%)")
        logger.info(f"üî¥ SELL —Å–∏–≥–Ω–∞–ª—ñ–≤: {sell_signals:4d} ({sell_signals/len(signals_df)*100:.1f}%)")
        logger.info(f"‚ö™ HOLD —Å–∏–≥–Ω–∞–ª—ñ–≤: {hold_signals:4d} ({hold_signals/len(signals_df)*100:.1f}%)")
        
        avg_prediction = signals_df['predicted_change_pct'].mean()
        logger.info(f"\nüìà –°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–≥–Ω–æ–∑: {avg_prediction:+.2f}%")
        logger.info(f"üìä –ú—ñ–Ω –ø—Ä–æ–≥–Ω–æ–∑: {signals_df['predicted_change_pct'].min():+.2f}%")
        logger.info(f"üìä –ú–∞–∫—Å –ø—Ä–æ–≥–Ω–æ–∑: {signals_df['predicted_change_pct'].max():+.2f}%")
        
        # –ü–æ—Ç–æ—á–Ω–∏–π —Å–∏–≥–Ω–∞–ª
        logger.info("\n" + "="*80)
        logger.info("üéØ –ü–û–¢–û–ß–ù–ò–ô –°–ò–ì–ù–ê–õ")
        logger.info("="*80)
        
        current = signals_df.iloc[-1]
        signal_emoji = {
            'BUY': 'üü¢',
            'SELL': 'üî¥',
            'HOLD': '‚ö™'
        }[current['signal']]
        
        logger.info(f"{signal_emoji} {current['signal']}")
        logger.info(f"–ü–æ—Ç–æ—á–Ω–∞ —Ü—ñ–Ω–∞: ${current['current_price']:.2f}")
        logger.info(f"–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∞ –∑–º—ñ–Ω–∞: {current['predicted_change_pct']:+.2f}%")
        logger.info(f"–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å: {current['confidence']*100:.1f}%")
        logger.info(f"–°–∏–ª–∞ —Å–∏–≥–Ω–∞–ª—É: {current['strength']:.2f}/3.0")
        
        logger.info("\n" + "="*80)


async def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª—ñ–∑ –º–æ–¥–µ–ª—ñ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–≥–Ω–∞–ª—ñ–≤')
    parser.add_argument('--symbol', type=str, default='BTCUSDT', help='–¢–æ—Ä–≥–æ–≤–∏–π —Å–∏–º–≤–æ–ª')
    parser.add_argument('--model-dir', type=str, default='models/optimized_BTC', help='–ü–∞–ø–∫–∞ –∑ –º–æ–¥–µ–ª–ª—é')
    parser.add_argument('--days', type=int, default=7, help='–°–∫—ñ–ª—å–∫–∏ –¥–Ω—ñ–≤ –¥–∞–Ω–∏—Ö –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏')
    parser.add_argument('--threshold', type=float, default=0.005, help='–ü–æ—Ä—ñ–≥ –¥–ª—è —Å–∏–≥–Ω–∞–ª—ñ–≤ (0.005 = 0.5%)')
    parser.add_argument('--recent', type=int, default=20, help='–°–∫—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤ –ø–æ–∫–∞–∑–∞—Ç–∏')
    
    args = parser.parse_args()
    
    try:
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞
        analyzer = ModelAnalyzer(symbol=args.symbol, model_dir=args.model_dir)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        await analyzer.load_model()
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
        df = await analyzer.load_recent_data(days=args.days)
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ features
        df_features, prices = analyzer.prepare_features(df)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
        X_sequences = analyzer.create_sequences(df_features.values)
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        predictions = await analyzer.predict(X_sequences)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–≥–Ω–∞–ª—ñ–≤
        signals_df = analyzer.generate_signals(predictions, prices, threshold=args.threshold)
        
        # –í–∏–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª—ñ–∑
        analyzer.print_analysis(signals_df, recent_n=args.recent)
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        output_file = f'analysis_{args.symbol}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        signals_df.to_csv(output_file, index=False)
        logger.info(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_file}")
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}", exc_info=True)
        return 1
    
    return 0


if __name__ == '__main__':
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
